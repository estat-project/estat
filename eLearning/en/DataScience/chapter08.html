<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <title>Chapter 8</title>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <link href="/estat/eStat/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="/estat/eStat/css/dashboard.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <style type="text/css">code{white-space: pre;}</style>
       <style type="text/css">.sidebar ul{padding-left: 10px;}</style>
       <link rel="stylesheet" href="/estat/eStat/css/prism.css">
       <link rel="stylesheet" href="/estat/eStat/css/pandoc.css">
       <script src="/estat/eStat/lib/d3/d3.v4.min.js"></script>   
       <script src="/estat/eStat/lib/jquery/jquery.min.js"></script>    
       <script src="/estat/eStat/lib/DistributionsUtil.js" ></script>
       <script src="/estat/eStat/lib/FileSaver.min.js" ></script>
       <script src="/estat/eStat/lib/convertSVG.js"></script>
       <script src="/estat/eStat/js/prism.js"></script>
       <script src="/estat/eStat/js/eBook.js"></script>
       <script src="/estat/eStat/js/eStatU.js"></script>
       <script src="/estat/eStat/js/language.js" ></script>
       <script> setLanguage('en'); </script>
       <script type="text/javascript" id="MathJax-script" async
	       src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
       </script>

       <script>
	 $(document).ready(function() {
  	     toc = $("#sidebar > ul > li > ul");
  	     sections = toc.children();   // <li>
  	     for(var i=0; i<sections.length; i++) {
  		 if ($(sections[i]).children().length == 1) { continue; }
  		 var first = sections[i].firstElementChild;  // <a>
  		 var last = sections[i].lastElementChild;
  		 var li = $("<li>");
  		 var details = $("<details>");
  		 var summary = $("<summary>");
  		 $(summary).append(first)
  		 $(details).append(summary);
  		 $(details).append(last);
  		 $(li).append(details);	
  		 $(sections[i]).replaceWith(li);
  	     }
	 });
       </script>
      
</head>

<body>
  
<!-- begin of left sidebar -->
<div class="container-fluid">
<div class="row">
<div id="sidebar" class="col-sm-2 col-md1 sidebar">
  <ul>
    <li> <a href="index.html"><b>Table of chapters</b> </a> </li>
    <li> <a href="chapter01.html"> 1 Data science and artificial intelligence</a> </li>
    <li> <a href="chapter02.html"> 2 Data visualization</a> </li>
    <li> <a href="chapter03.html"> 3 Data summary and transformation</a> </li>
    <li> <a href="chapter04.html"> 4 Probability and distribution</a> </li>
    <li> <a href="chapter05.html"> 5 Estimation, testing hypothesis, and regression analysis</a> </li>
    <li> <a href="chapter06.html"> 6 Supervised machine learning for categorical data</a> </li>
    <li> <a href="chapter07.html"> 7 Supervised machine learning for continuous data</a> </li>
    <li> <a href="chapter08.html"> 8 Unsupervised machine learning</a> </li>
          <ul>
                <li class="listNone"><a href="#0801">8.1 Basic concepts of unsupervised learning and clustering</a></li>
                <li class="listNone"><a href="#0802">8.2 Hierarchical clustering model</a></li>
                <li class="listNone"><a href="#0803">8.3 \(\small K\)-Means clustering model</a></li>
                <li class="listNone"><a href="#0804">8.4 Exercise</a></li>
          </ul>
    <li> <a href="chapter09.html"> 9 Artificial intelligence and other applications</span></a> </li>
  </ul>
</div>
</div>
</div>
<!-- end of left sidebar -->

  
<div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">

  <h2>Chapter 8. Unsupervised Learning: Clustering Analysis</h2> 
  <h6>
      <a href="./pdf/ppt8.pdf" target="_blank"><u>[presentation]</u></a>&nbsp;&nbsp;&nbsp;
      <a href="./pdf/book8.pdf" target="_blank"><u>[book]</u></a>
  </h6>
	      <ul>
                <li class="listNone"><a href="#0801">&nbsp;&nbsp; 8.1 Basic concepts of unsupervised learning and clustering</a></li>
                <li class="listNone"><a href="#0802">&nbsp;&nbsp; 8.2 Hierarchical clustering model</a></li>
                  <li class="listNoneIndent"><a href="#080201">8.2.1  Method of linkage</a></li>
                  <li class="listNoneIndent"><a href="#080202">8.2.2  R practice - Hierarchical clustering</a></li>
                <li class="listNone"><a href="#0803">&nbsp;&nbsp; 8.3 \(\small K\)-means clustering model</a></li>
                  <li class="listNoneIndent"><a href="#080301">8.3.1  R practice - \(\small K\)-Means clustering</a></li>
                <li class="listNone"><a href="#0804">&nbsp;&nbsp; 8.4 Exercise</a></li>
	      </ul>
  <p>
  <h5>CHAPTER OBJECTIVES</h5> 
  <p>
  <div class="mainTable">
       The clustering analysis is a technique for finding clusters of data with similar properties when data 
       from multiple groups are mixed, and the group to which each data belongs is unknown. 
       We introduce the followings in this chapter.
    <p>
      <div class="textL30M20">
        • Basic concepts of clustering analysis and the evaluation methods of clustering models.
      </div>
      <div class="textL30M20">
        • Hierarchical clustering model, which has a long history, in section 8.2.
      </div>
      <div class="textL30M20">
        • \(\small K\)-means clustering model, which is frequently used in real practice, in section 8.3.
      </div>
  </div>
  <p>

  <!------------------------------------------------------------> 
  <h3 id="0801">8.1 Basic concepts of clustering analysis</h3>
  <p>

  <div class="mainTable">
       Classification analysis or supervised learning studied in Chapters 6 and 7 used data whose group affiliation 
       is known to obtain a classification function and then decided the data whose group affiliation
       is unknown would be classified into which group using the classification function. However, when analyzing real data, 
       there is a need to classify data whose group affiliation is unknown into homogeneous groups, and 
       it is called <b>clustering analysis</b> or <b>unsupervised learning</b>. Clustering analysis can help understand 
       the structure of data (clustering for understanding) in situations where the content of the data is not well known,
       or it can be a helpful starting point (clustering for utility) for other analyses by identifying the characteristics
       of the formed clusters and the relationships between clusters. Clustering analysis is used in various fields, 
       such as psychology, biology, business administration, and information science. 
    <p>
       Clustering analysis is a method of forming clusters based on the similarity or relationship between each data, such that 
       the data in a cluster are similar and the data in other clusters are different. At this time, 
       the higher the similarity within a cluster, the better, and the differences between clusters 
       should be as different as possible. However, it is generally difficult to define a cluster, 
       and it is unclear how many clusters to divide into. Many types of clustering analysis models have been developed 
       to apply to various types of data, but there is hardly a single clustering analysis model that is satisfactory 
       for all types of applications. Each clustering analysis model can show different performance when the data dimension 
       is low or high, when the data size is small or large, when the data density is small or high when there are 
       few or many outliers or extreme points, and when the data properties are discrete or continuous. 
       In general, after applying several clustering analysis models, an appropriate model is selected 
       based on the analyst's judgment. 
    <p>
       In this chapter, we first introduce the hierarchical clustering model, which has been used for a long time. 
       Then, we introduce the \(\small K\)-means clustering, which is widely used. 
  </div>
  <p>

  <h5>Classification of clustering analysis models</h5>
  <p>
  <div class="mainTable">
       Clustering analysis models are divided into hierarchical clustering models and partitional clustering models.
       The <b>hierarchical clustering model</b> allows subclusters within a cluster, and it groups the entire data into one cluster, 
       divides it into subclusters, and then divides each subcluster again. It can display the types of whole clusters
       in a tree shape. Section 8.2 introduces hierarchical clustering models. The <b>partitional clustering model</b>
       is a method that divides the entire data without overlapping each other,
       and Section 8.3 introduces the \(\small K\)-means clustering model. 
    <p>
       Clustering analysis models can also be divided into <b>exclusive clustering analysis</b>, where one data must belong to one cluster,
       and <b>inclusive clustering analysis</b>, where one data can belong to multiple clusters. The \(\small K\)-means clustering model
       is an exclusive clustering analysis, and the fuzzy clustering model and the mixed distribution clustering model
       are inclusive clustering analyses. The fuzzy cluster and mixed distribution clustering models indicate 
       the weight or probability that each data belongs to each cluster as a number between 0 and 1. However, 
       since the inclusive clustering analysis model generally classifies one data into a group with a higher probability, 
       the final data cluster can be an exclusive clustering analysis. 
    <p>
       In addition, clustering analysis models are also classified into prototype-based models, density-based models, 
       and graph-based models. 
       The <b>prototype-based model</b> determines the form of the cluster based on how close the data is to the prototype 
       of each cluster, which has been determined in advance. The \(\small K\)-means clustering model is a prototype-based model. 
       In the case of continuous data, the cluster average is usually set as the prototype, and in the case of 
       discrete data, the mode of the cluster is used as the prototype. The <b>density-based model</b> is a method that 
       considers an area where data is distributed as a cluster when the density is very high. The graph-based model 
       is a method that considers each data as a node, connects the nodes based on a set distance, and then determines 
       the data corresponding to the connected nodes as a cluster. The Kohonen clustering model belongs to this.
  </div>
  <p>

  <h5>Evaluation of clustering analysis models</h5>
  <div class="mainTable">
       The classification analysis model was created using training data, and then its accuracy was evaluated using test data. 
       However, evaluating which clustering analysis model is good is difficult, and the following factors are considered.
    <p>
       <div class="textL30M10">• Clustering tendency for a specific data set</div>
       <div class="textL30M10">• Number of accurate clusters</div>
       <div class="textL30M10">• Comparison of characteristics of formed clusters</div>
    <p>
       Various measures can be considered for evaluating these factors, such as the response within a cluster,
       <b>cohesion</b> and <b>separation</b> between clusters. In the case of clustering models 
       that utilize distance or similarity between data, the cohesion of cluster \(\small G_{i}\) and the separation 
       between two clusters \(\small G_{i}\) and \(\small G_{j}\) are defined as follows. Here, 
       \(\small d(\boldsymbol x, \boldsymbol y)\) is the distance between data \(\boldsymbol x\) and data \(\boldsymbol y\). 
       $$ \small
        \begin{align}
        \text{Cohesion}   (G_{i}) &= \sum_{\boldsymbol x , \; \boldsymbol y \; \in \; G_{i}} \; d(\boldsymbol x, \boldsymbol y)   \\ 
        \text{Separation} (G_{i} , G_{j}) &= \sum_{\boldsymbol x \; \in \; G_{i}} \; \sum_{\boldsymbol x \; \in \;  G_{j}} \; d(\boldsymbol x, \boldsymbol y)  
        \end{align}
       $$
    <p>
       In the clustering model based on the prototype, the cohesion and separation are defined as follows when 
       the centroids of clusters \(\small G_{i}\) and clusters \(\small G_{j}\) are \(\small \boldsymbol c_{i}\) and 
       \(\small \boldsymbol c_{j}\), respectively. 
       $$ \small
        \begin{align}
        \text{Cohesion}   (G_{i}) &= \sum_{\boldsymbol x  \; \in \; G_{i}} \; d(\boldsymbol x, \boldsymbol c_{i} )   \\ 
        \text{Separation} (G_{i} , G_{j}) &= d(\boldsymbol c_{i}, \boldsymbol c_{j})  
        \end{align}
       $$
       In the case of cohesion, if the distance \(d(\boldsymbol x, \boldsymbol c_{i} )\) between the data 
       \(\boldsymbol x \) of cluster and the center \(\boldsymbol c_{i} \) is defined as the squared Euclidean distance, 
       the cohesion of cluster \(\small G_{i}\) becomes the sum of squared error (SSE). 
    <p>
       When there are \(\small K\) clusters and the number of data in cluster \(\small G_{i}\) is \(\small n_{i}\), 
       the cohesion of the entire clustering model is calculated as the weighted sum of the cohesion of each cluster,
       and the weight value \(\small w_{i}\) can be \(\small n_{i}\), \(\small \frac{1}{n_{i}}\), or other various 
       measures ​​depending on the situation.
       $$ \small
        \text{Cohesion of total model} = \sum_{i=1}^{K} \; w_{i} \times (\text{Cohesion of cluster} \; G_{i} )
       $$
  </div>
  <p>

  <!---------------------------------------------------------------->
  <h3 id="0802">8.2 Hierachical clustering model</h3>
  <p>

  <div class="mainTable">
       The <b>hierarchical clustering</b> model is a widely used method with a long history, and there are two main approaches
       to forming clusters. The <b>agglomerative method</b> starts from one data and groups the closest clusters
       in order. There are several variations depending on how the distance between clusters is defined. 
       The second is the <b>divisive</b> method, which considers all data as one cluster and divides them in order 
       so that the final cluster becomes one data. There are several variations depending on which cluster is first divided and how it is divided. In this section, only the agglomerative hierarchical clustering model is introduced. 
    <p>
       The result of hierarchical clustering is often displayed in a <b>dendrogram</b> similar to a tree, as shown 
       on &lt;Figure 8.2.1&gt;, which shows the relationship between clusters and subclusters and the order 
       in which clusters are formed. The <b>subset plot</b> displays the entire data as one set, as shown in 
       &lt;Figure 8.2.2&gt;, and displays each hierarchical cluster as a subset plot within this set.
    <p>
       <img class="figure50" src="./figure/Fig080201.png">
       <div class="figText">&lt;Figure 8.2.1&gt; Dendrogram for the results of hierarchical clustering</div>
    <p>
       <img class="figure50" src="./figure/Fig080202.png">
       <div class="figText">&lt;Figure 8.2.2&gt; Subset plot for the results of hierarchical clustering</div>
  </div>
  <p>

  <div class="mainTable">
       Let \(n\) number of data observed for the \(m\) variables, \(\small \boldsymbol x =(x_{1}, x_{2}, ... x_{m})\),
       be denoted as \(\small \boldsymbol x_{1}, \boldsymbol x_{2}, ... \boldsymbol x_{n}\). 
       The agglomerative hierarchical clustering algorithm first calculates the \(n\) × \(n\) distance matrix
       or similarity matrix, \(D = \{ d_{ij} \}\),  between all data where \( d_{ij} \) means the distance 
       between data \(\small \boldsymbol x_{i}\) and \(\small \boldsymbol x_{j}\).
       $$
         D = \left [ \matrix { d_{11} & d_{12} & ... & d_{1n} \cr d_{21} & d_{22} & ... & d_{2n} \cr ... & ... & ... & ... \cr d_{n1} & d_{n2} & ... & d_{nn}  } \right ]
       $$
       After considering each data as a cluster, the two closest clusters are grouped into one cluster, 
       and the similarity matrix between this cluster and the remaining clusters is modified. At this time,
       the similarity between clusters must be defined. The same method is repeated until the number of clusters 
       becomes one, which can be summarized as the following algorithm.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Agglomerative hierarchical clustering algorithm</b>
    <p>
        <table>
          <tr>
            <td>Step 1</td>
            <td>Consider each data as one cluster and calculate the similarity matrix of all data.<br>
                
            </td>
          </tr>
          <tr>
            <td>Step 2</td>
            <td><b>repeat</b></td>
          </tr>
          <tr>
            <td>Step 3</td>
            <td>\(\qquad\)Group the two closest clusters into one cluster.</td>
          </tr>
          <tr>
            <td>Step 4</td>
            <td>\(\qquad\)Obtain the similarity matrix between all clusters including the newly formed cluster.</td>
          </tr>
          <tr>
            <td>Step 5</td>
            <td><b>until</b> &nbsp;(the number of clusters becomes one)</td>
          </tr>
         </table>
  </div>
  <p>

  <!------------------------------------------------------------>
  <h4 id="080201">8.2.1 Method of linkage</h4>
  <p>

  <div class="mainTable">
       The hierarchical clustering algorithm has several variations depending on how the distance between clusters is defined.
       There are several methods for defining the distance between a cluster and other clusters: single linkage, 
       complete linkage, average linkage, median linkage, centroid linkage, and Ward method.
  </div>
  <p>

  <h5>A. Single linkage</h5>
  <div class="mainTable">
       In the <b>single linkage</b> or <b>shortest distance</b> method, if the data with the closest distance in the 
       \(n\) × \(n\) distance matrix \(\small D = \{ d_{ij} \}\) are \(\small U\) and \(\small V\), the two data 
       are first grouped to form a cluster \(\small (UV)\). 
       The next step calculates the distance between cluster \(\small (UV)\) and the remaining \(n-2\) other data or clusters. 
       The single linkage distance between cluster \(\small (UV)\) and cluster \(\small W\) is calculated as follows:
       $$
         d_{(UV)W } = min ( d_{UW}, d_{VW} )
       $$
       In the modified distance matrix, the two data or clusters with the closest distance are combined into a new cluster. 
       Repeat this process until a single cluster includes all data.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Example 8.2.1</b>
       The five observed data for two variables \(\small x_{1}\) and \(\small x_{2}\) and the matrix of squared Euclid
       distances between these data are as follows. Create a hierarchical cluster using the single linkage method.
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="7">Table 8.2.1  Five observed data and the matrix of squared Euclid distances</th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th style="width:100px"></th>
          <th colspan="5">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Data</th>
          <th style="width:100px">\(\small (x_{1}, \small x_{2})\)</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small D\)</th>
          <th style="width:100px">\(\small E\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>  <td class="tdCenter">(1, 5)</td> <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>  <td class="tdCenter">(2, 4)</td> <td class="tdCenter">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>  <td class="tdCenter">(4, 6)</td> <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small D\)</td>  <td class="tdCenter">(4, 3)</td> <td class="tdCenter">13</td> <td class="tdCenter">5</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
        <tr><td class="tdCenter">\(\small E\)</td>  <td class="tdCenter">(5, 3)</td> <td class="tdCenter">20</td> <td class="tdCenter">10</td> <td class="tdCenter">10</td> <td class="tdCenter" style="background:yellow">1</td> <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       Since the distance between data \(\small D\) and \(\small E\) is 1, which is the minimum, \(\small (DE)\) is the first cluster, 
       and the distance between cluster \(\small (DE)\) and the remaining data is calculated using the single linkage method, 
       and the distance matrix is ​​modified as follows.
    <p>
       \( \small \qquad d((DE), A) = min(d(D,A), d(E,A)) = min(13, 20) = 13 \) <br>
       \( \small \qquad d((DE), B) = min(d(D,B), d(E,B)) = min(5, 10) = 5 \) <br>
       \( \small \qquad d((DE), C) = min(d(D,C), d(E,C)) = min(9, 10) = 9 \) <br>
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 8.2.2  Modified distance matrix with cluster \(\small (DE)\) using the single linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="4">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>    <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>    <td class="tdCenter" style="background:yellow">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>    <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td> <td class="tdCenter">13</td> <td class="tdCenter">5</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small d(A,B)\) = 2, so \(\small (AB)\) becomes the next cluster. 
       If we calculate the distance between clusters \(\small (AB)\) and \(\small C\) , \(\small (DE)\) using the 
       single linkage method and modify the distance matrix, we get the following.
    <p>
       \( \small \qquad d((AB), C) = min(d(A,C), d(B,C)) = min(10, 8) = 8 \) <br>
       \( \small \qquad d((AB), (DE)) = min(d(A, (DE)), d(B,(DE)) = min(13, 5) = 5 \) <br>
    <p>
      <table style="width:500px"> 
        <tr>
          <th colspan="5">Table 8.2.3  Modified distance matrix with cluster \(\small (AB)\) using the single linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="3">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small (AB)\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small (AB)\)</td>  <td class="tdCenter">0</td>  <td class="tdCenter"></td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>     <td class="tdCenter">8</td> <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td>  <td class="tdCenter" style="background:yellow">5</td> <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small (d(AB), (DE))\) = 5, so \(\small (AB)(DE)\) becomes the next cluster. 
       If we calculate the distance between clusters \(\small (AB)(DE)\) and \(\small C\) using the 
       single linkage method, we get the following.
    <p>
       \( \small \qquad d((AB)(DE), C) = min(d((AB), C), d((DE), C)) = min(8, 9) = 8 \) 
    <p>
       If the above single linkage method is displayed as a dendrogram, it is as shown in &lt;Figure 8.2.3&gt;. 
    <p>
       <img class="figure30" src="./figure/Fig080203.png">
       <div class="figText">&lt;Figure 8.2.3&gt; Hierarchical clustering dendrogram using the single linkage</div>
  </div>
  <p>

  <h5>B. Complete linkage</h5>
  <div class="mainTable">
       In the <b>complete linkage</b> or <b>maximum distance</b> method, if the data with the closest distance in the 
       \(n\) × \(n\) distance matrix \(\small D = \{ d_{ij} \}\) are \(\small U\) and \(\small V\), the two data 
       are first grouped to form a cluster \(\small (UV)\). 
       The next step calculates the distance between cluster \(\small (UV)\) and the remaining \(n-2\) other data or clusters. 
       The complete linkage distance between cluster \(\small (UV)\) and cluster \(\small W\) is calculated as follows:
       $$
         d_{(UV)W } = max ( d_{UW}, d_{VW} )
       $$
       In the modified distance matrix, the two data or clusters with the closest distance are combined into a new cluster. 
       Repeat this process until a single cluster includes all data.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Example 8.2.2</b>
       The five observed data for two variables \(\small x_{1}\) and \(\small x_{2}\) and the matrix of squared Euclid
       distances between these data are as follows. Create a hierarchical cluster using the complete linkage method.
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="7">Table 8.2.4  Five observed data and the matrix of squared Euclid distances</th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th style="width:100px"></th>
          <th colspan="5">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Data</th>
          <th style="width:100px">\(\small (x_{1}, \small x_{2})\)</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small D\)</th>
          <th style="width:100px">\(\small E\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>  <td class="tdCenter">(1, 5)</td> <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>  <td class="tdCenter">(2, 4)</td> <td class="tdCenter">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>  <td class="tdCenter">(4, 6)</td> <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small D\)</td>  <td class="tdCenter">(4, 3)</td> <td class="tdCenter">13</td> <td class="tdCenter">5</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
        <tr><td class="tdCenter">\(\small E\)</td>  <td class="tdCenter">(5, 3)</td> <td class="tdCenter">20</td> <td class="tdCenter">10</td> <td class="tdCenter">10</td> <td class="tdCenter" style="background:yellow">1</td> <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       Since the distance between data \(\small D\) and \(\small E\) is 1, which is the minimum, \(\small (DE)\) is the first cluster, 
       and the distance between cluster \(\small (DE)\) and the remaining data is calculated using the complete linkage method, 
       and the distance matrix is ​​modified as follows.
    <p>
       \( \small \qquad d((DE), A) = max(d(D,A), d(E,A)) = max(13, 20) = 20 \) <br>
       \( \small \qquad d((DE), B) = max(d(D,B), d(E,B)) = max(5, 10) = 10 \) <br>
       \( \small \qquad d((DE), C) = max(d(D,C), d(E,C)) = max(9, 10) = 10 \) <br>
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 8.2.5  Modified distance matrix with cluster \(\small (DE)\) using the complete linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="4">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>    <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>    <td class="tdCenter" style="background:yellow">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>    <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td> <td class="tdCenter">20</td> <td class="tdCenter">10</td>  <td class="tdCenter">10</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small d(A,B)\) = 2, so \(\small (AB)\) becomes the next cluster. 
       If we calculate the distance between clusters \(\small (AB)\) and \(\small C\) , \(\small (DE)\) using the 
       complete linkage method and modify the distance matrix, we get the following.
    <p>
       \( \small \qquad d((AB), C) = max(d(A,C), d(B,C)) = max(10, 8) = 10 \) <br>
       \( \small \qquad d((AB), (DE)) = max(d(A, (DE)), d(B,(DE)) = max(20, 10) = 20 \) <br>
    <p>
      <table style="width:500px"> 
        <tr>
          <th colspan="5">Table 8.2.6  Modified distance matrix with cluster \(\small (AB)\) using the complete linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="3">Distance</th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small (AB)\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small (AB)\)</td>  <td class="tdCenter">0</td>  <td class="tdCenter"></td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>     <td class="tdCenter">10</td> <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td>  <td class="tdCenter">20</td> <td class="tdCenter" style="background:yellow">10</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small d((AB), C)\) = \(\small d(C, (DE))\) = 10, so 
       \(\small (AB)C\) or \(\small C(DE)\) becomes the next cluster. Let's select \(\small C(DE)\) is the next cluster.
       If we calculate the distance between clusters \(\small (AB)\) using the 
       complete linkage method, we get the following.
    <p>
       \( \small \qquad d((AB), C(DE)) = max(d((AB), C), d((AB), (DE))) = max(10, 20) = 20 \) 
    <p>
       If the above complete linkage method is displayed as a dendrogram, it is as shown in &lt;Figure 8.2.4&gt;. 
    <p>
       <img class="figure30" src="./figure/Fig080204.png">
       <div class="figText">&lt;Figure 8.2.4&gt; Hierarchical clustering dendrogram using the complete linkage</div>
  </div>
  <p>

  <h5>C. Average linkage</h5>
  <div class="mainTable">
       In the <b>average linkage</b> method, if the data with the closest distance in the 
       \(n\) × \(n\) distance matrix \(\small D = \{ d_{ij} \}\) are \(\small U\) and \(\small V\), the two data 
       are first grouped to form a cluster \(\small (UV)\). 
       The next step calculates the average distance between cluster \(\small (UV)\) and the other cluster
       \(\small W\) as follows.
       $$
         d_{(UV)W } = \frac{\sum_{\boldsymbol x_{i} \in (UV)} \sum_{\boldsymbol x_{j} \in W} \; d(\boldsymbol x_{i}, \boldsymbol x_{j}) }{n_{(UV)} \times n_{W}} 
       $$
       Here \( \small d(\boldsymbol x_{i}, \boldsymbol x_{j}) \) is the distance between the data 
       \( \small \boldsymbol x_{i} \) in the cluster \(\small (UV)\) and the data \( \small \boldsymbol x_{j} \)
       in the cluster \( \small W\), and \( n_{(UV)}\) and \( n_{W}\) are the number of data  
       in the cluster \(\small (UV)\) and \( \small W\) respectively.
       In the modified distance matrix, the two data or clusters with the closest distance are combined into a new cluster. 
       Repeat this process until a single cluster includes all data.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Example 8.2.3</b>
       The five observed data for two variables \(\small x_{1}\) and \(\small x_{2}\) and the matrix of squared Euclid
       distances between these data are as follows. Create a hierarchical cluster using the average linkage method.
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="7">Table 8.2.7  Five observed data and the matrix of squared Euclid distances</th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th style="width:100px"></th>
          <th colspan="5">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Data</th>
          <th style="width:100px">\(\small (x_{1}, \small x_{2})\)</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small D\)</th>
          <th style="width:100px">\(\small E\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>  <td class="tdCenter">(1, 5)</td> <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>  <td class="tdCenter">(2, 4)</td> <td class="tdCenter">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>  <td class="tdCenter">(4, 6)</td> <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small D\)</td>  <td class="tdCenter">(4, 3)</td> <td class="tdCenter">13</td> <td class="tdCenter">5</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
        <tr><td class="tdCenter">\(\small E\)</td>  <td class="tdCenter">(5, 3)</td> <td class="tdCenter">20</td> <td class="tdCenter">10</td> <td class="tdCenter">10</td> <td class="tdCenter" style="background:yellow">1</td> <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       Since the distance between data \(\small D\) and \(\small E\) is 1, which is the minimum, \(\small (DE)\) is the first cluster, 
       and the distance between cluster \(\small (DE)\) and the remaining data is calculated using the average linkage method, 
       and the distance matrix is ​​modified as follows.
    <p>
       \( \small \qquad d((DE), A) = \frac{d(D,A) + d(E,A)}{2 \times 1} = \frac {13  + 20}{2} = 16.5 \) <br>
       \( \small \qquad d((DE), B) = \frac{d(D,B) + d(E,B)}{2 \times 1} = \frac {5 + 10}{2} = 7.5 \) <br>
       \( \small \qquad d((DE), C) = \frac{d(D,C) + d(E,C)}{2 \times 1} = \frac {9 + 10}{2} = 9.5 \) <br>
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 8.2.8  Modified distance matrix with cluster \(\small (DE)\) using the single linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="4">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>    <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>    <td class="tdCenter" style="background:yellow">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>    <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td> <td class="tdCenter">16.5</td> <td class="tdCenter">7.5</td>  <td class="tdCenter">9.5</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small d(A,B)\) = 2, so \(\small (AB)\) becomes the next cluster. 
       If we calculate the distance between clusters \(\small (AB)\) and \(\small C\) , \(\small (DE)\) using the 
       average linkage method and modify the distance matrix, we get the following.
    <p>
       \( \small \qquad d((AB), C)    = \frac{d(A,C) + d(B,C)}{2 \times 1} = \frac{10 + 8}{2} = 9 \) <br>
       \( \small \qquad d((AB), (DE)) = \frac{d(A,D) + d(A,E) + d(B,D) + d(B,E)}{2 \times 2} = \frac{13+20+5+10}{4} = 12 \) <br>
    <p>
      <table style="width:500px"> 
        <tr>
          <th colspan="5">Table 8.2.9  Modified distance matrix with cluster \(\small (AB)\) using the average linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="3">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small (AB)\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small (AB)\)</td>  <td class="tdCenter">0</td>  <td class="tdCenter"></td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>     <td class="tdCenter" style="background:yellow">9</td> <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td>  <td class="tdCenter">12</td> <td class="tdCenter">9.5</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small (d(AB), C)\) = 9, so \(\small (AB)C\) becomes the next cluster. 
       If we calculate the distance between clusters \(\small (AB)C\) and \(\small (DE)\) using the 
       average linkage method, we get the following.
    <p>
       \( \small \qquad d((AB)C, (DE)) = \frac{d(A,D)+d(A,E)+d(B,D)+d(B,E)+d(C,D)+d(C,E)}{3 \times 2} = \frac{12+20+5+10+9+10}{6} = 11.1 \) 
    <p>
       If the above average linkage method is displayed as a dendrogram in &lt;Figure 8.2.5&gt. 
    <p>
       <img class="figure30" src="./figure/Fig080205.png">
       <div class="figText">&lt;Figure 8.2.5&gt; Hierarchical clustering dendrogram using the average linkage</div>
  </div>
  <p>

  <h5>D. Centroid linkage</h5>
  <div class="mainTable">
       In the <b>centroid linkage</b> method, the distance between two clusters is calculated as the distance
       between the centroids of the two clusters. If the number of data belonging to cluster \(\small G_{i}\) is 
       \(n_{i}\) and the centroid of the cluster is \(\boldsymbol c_{i}\), 
       and the number of data belonging to cluster \(\small G_{j}\) is 
       \(n_{j}\) and the centroid of the cluster is \(\boldsymbol c_{j}\), then the distance between two clusters,
       \(\small d(G{i}, G_{j}) \), is defined as the squared Euclid distance between the two centroids as follows.
       $$ \small
         d(G{i}, G_{j}) = ||\boldsymbol c_{i} - \boldsymbol c_{j}  ||^{2}
       $$
       If two clusters are combined, the center of the new cluster, \(\boldsymbol c\), is calculated
       using the weighted average as follows.
       $$ \small
         \boldsymbol c = \frac{n_{i} \boldsymbol c_{i} + n_{j} \boldsymbol c_{j}}{n_{i} + n_{j}} 
       $$
       After calculating the distance between each cluster, a new cluster is formed with the data with 
       the closest centroid distance. 
       Repeat this process until a single cluster includes all data.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Example 8.2.4</b>
       The five observed data for two variables \(\small x_{1}\) and \(\small x_{2}\) and the matrix of squared Euclid
       distances between these data are as follows. Create a hierarchical cluster using the single linkage method.
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="7">Table 8.2.10  Five observed data and the matrix of squared Euclid distances</th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th style="width:100px"></th>
          <th colspan="5">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Data</th>
          <th style="width:100px">\(\small (x_{1}, \small x_{2})\)</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small D\)</th>
          <th style="width:100px">\(\small E\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>  <td class="tdCenter">(1, 5)</td> <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>  <td class="tdCenter">(2, 4)</td> <td class="tdCenter">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>  <td class="tdCenter">(4, 6)</td> <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small D\)</td>  <td class="tdCenter">(4, 3)</td> <td class="tdCenter">13</td> <td class="tdCenter">5</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
        <tr><td class="tdCenter">\(\small E\)</td>  <td class="tdCenter">(5, 3)</td> <td class="tdCenter">20</td> <td class="tdCenter">10</td> <td class="tdCenter">10</td> <td class="tdCenter" style="background:yellow">1</td> <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       Since the distance between data \(\small D\) and \(\small E\) is 1, which is the minimum, \(\small (DE)\) is the first cluster, 
       and the distance between cluster \(\small (DE)\) and the remaining data is calculated using the centroid linkage method, 
       and the distance matrix is ​​modified as follows.
    <p>
       \( \small \qquad d((DE), A) = (4.5 - 1)^2 + (3-5)^2 = 16.25 \) <br>
       \( \small \qquad d((DE), B) = (4.5 - 2)^2 + (3-4)^2 = 7.25 \) <br>
       \( \small \qquad d((DE), C) = (4.5 - 4)^2 + (3-6)^2 = 9.25 \) <br>
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 8.2.11  Modified distance matrix with cluster \(\small (DE)\) using the centroid linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="4">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>    <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>    <td class="tdCenter" style="background:yellow">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>    <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td> <td class="tdCenter">16.25</td> <td class="tdCenter">7.25</td>  <td class="tdCenter">9.25</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small d(A,B)\) = 2, so \(\small (AB)\) becomes the next cluster and
       the center of the cluster is \(\small \frac{(4,3) + (5,3)}{2} = (4.5, 3)\). 
       If we calculate the distance between clusters \(\small (AB)\) and \(\small C\) , \(\small (DE)\) using the 
       centroid linkage method and modify the distance matrix, we get the following.
    <p>
       \( \small \qquad d((AB), C)    = (1.5 - 4)^2 + (4.5 - 6)^2 = 8.5 \) <br>
       \( \small \qquad d((AB), (DE)) = (1.5 - 4.5)^2 + (4.5 - 3)^2 = 11.25 \) <br>
    <p>
      <table style="width:500px"> 
        <tr>
          <th colspan="5">Table 8.2.12  Modified distance matrix with cluster \(\small (AB)\) using the centroid linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="3">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small (AB)\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small (AB)\)</td>  <td class="tdCenter">0</td>  <td class="tdCenter"></td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>     <td class="tdCenter" style="background:yellow">8.5</td> <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td>  <td class="tdCenter">11.25</td> <td class="tdCenter">9.5</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small (d(AB), C))\) = 8.5, so \(\small (AB)C\) becomes the next cluster
       and the center of the cluster is as follows. 
    <p>
       \( \qquad \frac{2 \times (1.5, \; 4.5) + 1 \times (4, \; 6)}{2 + 1}  \) = (2.3, 5)
    <p>
       If we calculate the distance between clusters \(\small (AB)C\) and \(\small (DE)\) using the 
       centroid linkage method, we get the following.
    <p>
       \( \small \qquad d((AB)C, (DE)) = (2.3 - 4.5)^2 + (5-3)^2  \) = 8.84
    <p>
       If the above centroid linkage method is displayed as a dendrogram as shown in &lt;Figure 8.2.6&gt;. 
    <p>
       <img class="figure30" src="./figure/Fig080206.png">
       <div class="figText">&lt;Figure 8.2.6&gt; Hierarchical clustering dendrogram using the centroid linkage</div>
  </div>
  <p>

  <h5>E. Ward linkage</h5>
  <div class="mainTable">
       The <b>Ward linkage</b> is a method of merging clusters based on the within-group sum of squares rather than 
       linking data based on the distance between clusters. Ward linkage measures the information loss caused by grouping data 
       into a single cluster at each stage of clustering analysis by the cluster mean and the error sum of squares (\(\small ESS\)) 
       between the data. If there are \(\small K\) clusters at the current stage in data with \(m\) variables 
       and \(n_{i}\) data in each cluster, the error sum of squares \(\small ESS_{i}\) of each cluster and 
       \(\small ESS\) of the entire cluster are as follows. Here, \(\small x_{ijk}\) is the measurement value 
       for the \(i\)-th variable of the \(j\)-th data of cluster \(\small G_{i}\), and 
        \( \small {\overline x}_{ik} = \frac {\sum_{j=1}^{n_{i}} \; x_{ijk} }{n_{i}} \) 
       means the mean value of variable \(k\) in cluster \(\small G_{i}\).
    <p>
       \( \small \qquad   ESS_{i} = \sum_{j=1}^{n_{i}} \; \sum_{k=1}^{m} \; (x_{ijk} - \overline x_{ik} )^2  \)
    <p>
       \( \small \qquad   ESS = \sum_{i=1}^{K} \; ESS_{i} = \sum_{i=1}^{K} \; \sum_{j=1}^{n_{i}} \; \sum_{k=1}^{m} \; (x_{ijk} - \overline x_{ik} )^2  \)
    <p>
       First, each data itself forms a cluster, then, since \(ESS_{i}\) for all i, \(ESS\) = 0. 
       At each stage of creating a cluster, the merging of all possible pairs of clusters is considered, and  
       the clusters are merged to create a new cluster so that the increment of \(ESS\) (information loss) due to 
       the merging of two clusters is minimized. The increment of \(ESS\) that occurs when grouping two clusters 
       \(\small G_{i}\) and \(\small G_{j}\), whose sizes are \(n_{i}\) and \(n_{j}\) respectively, is as follows, 
       and the Ward linkage method defines this increment as the distance between the two clusters \(\small G_{i}\) 
       and \(\small G_{j}\).
       $$ \small
          \qquad d(G_{i}, G_{j}) = \frac{|| \boldsymbol c_{i} - \boldsymbol c_{j} ||^2 }{\frac{1}{n_{i}} + \frac{1}{n_{j}} }
       $$
       Here, \( \boldsymbol c_{i}\) and \( \boldsymbol c_{j} \) are the averages of two clusters \(\small G_{i}\)
       and \(\small G_{j}\) respectively. This result differs from the centroid linkage method because
       the Ward linkage weights the distance between clusters means when calculating the distance between clusters. 
       The Ward linkage method tends to merge clusters of similar size.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Example 8.2.5</b>
       The five observed data for two variables \(\small x_{1}\) and \(\small x_{2}\) and the matrix of squared Euclid
       distances between these data are as follows. Create a hierarchical cluster using the Ward linkage method.
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="7">Table 8.2.13  Five observed data and the matrix of squared Euclid distances</th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th style="width:100px"></th>
          <th colspan="5">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Data</th>
          <th style="width:100px">\(\small (x_{1}, \small x_{2})\)</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small D\)</th>
          <th style="width:100px">\(\small E\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>  <td class="tdCenter">(1, 5)</td> <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>  <td class="tdCenter">(2, 4)</td> <td class="tdCenter">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>  <td class="tdCenter">(4, 6)</td> <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small D\)</td>  <td class="tdCenter">(4, 3)</td> <td class="tdCenter">13</td> <td class="tdCenter">5</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> </tr>
        <tr><td class="tdCenter">\(\small E\)</td>  <td class="tdCenter">(5, 3)</td> <td class="tdCenter">20</td> <td class="tdCenter">10</td> <td class="tdCenter">10</td> <td class="tdCenter" style="background:yellow">1</td> <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       When each data is considered as a cluster, the increment of ESS is the squared Euclid distance. Since the distance 
       between data D and E is 1, which is the minimum, (DE) becomes the first cluster. 
       The center of the cluster (DE) is \( \frac{(4,3) + (5,3)}{2} \) = (4.5, 3), so the distance is calculated using 
       the Ward linkage method for the remaining data, and the distance matrix is ​​modified as follows.
    <p>
       \( \small \qquad d((DE), A) = \frac{(4.5 - 1)^2 + (3 - 5)^2)}{\frac{1}{2} +\frac{1}{1}} = 11.17 \) <br>
       \( \small \qquad d((DE), B) = \frac{(4.5 - 2)^2 + (3 - 4)^2)}{\frac{1}{2} +\frac{1}{1}} = 4.83 \) <br>
       \( \small \qquad d((DE), C) = \frac{(4.5 - 4)^2 + (3 - 6)^2)}{\frac{1}{2} +\frac{1}{1}} = 6.17 \) <br>
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 8.2.14  Modified distance matrix with cluster \(\small (DE)\) using the Ward linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="4">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small A\)</th>
          <th style="width:100px">\(\small B\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small A\)</td>    <td class="tdCenter">0</td>  <td class="tdCenter"></td>   </tr>
        <tr><td class="tdCenter">\(\small B\)</td>    <td class="tdCenter" style="background:yellow">2</td>  <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>    <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td>   </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td> <td class="tdCenter">11.17</td> <td class="tdCenter">4.83</td>  <td class="tdCenter">6.17</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small d(A,B)\) = 2, so \(\small (AB)\) becomes the next cluster and the center
       of the cluster becomes \( \frac{(1,5) + (2,4)}{2} \) = (1.5, 4.5). 
       If we calculate the distance between clusters \(\small (AB)\) and \(\small C\) , \(\small (DE)\) using the 
       Ward linkage method and modify the distance matrix, we get the following.
    <p>
       \( \small \qquad d((AB), C) = \frac{(1.5 - 4)^2 + (4.5 - 6)^2)}{\frac{1}{2} +\frac{1}{1}} = 5.67 \) <br>
       \( \small \qquad d((AB), (DE)) = \frac{(1.5 - 4.5)^2 + (4.5 - 3)^2)}{\frac{1}{2} +\frac{1}{1}} = 11.25 \) <br>
    <p>
      <table style="width:500px"> 
        <tr>
          <th colspan="5">Table 8.2.15  Modified distance matrix with cluster \(\small (AB)\) using the Ward linkage </th>
        </tr>
        <tr> 
          <th style="width:100px"></th>
          <th colspan="3">Distance/th>
        </tr>
        <tr> 
          <th style="width:100px">Cluster</th>
          <th style="width:100px">\(\small (AB)\)</th>
          <th style="width:100px">\(\small C\)</th>
          <th style="width:100px">\(\small (DE)\)</th>
        </tr>
        <tr><td class="tdCenter">\(\small (AB)\)</td>  <td class="tdCenter">0</td>  <td class="tdCenter"></td>  </tr>
        <tr><td class="tdCenter">\(\small C\)</td>     <td class="tdCenter" style="background:yellow">5.67</td> <td class="tdCenter">0</td>  </tr>
        <tr><td class="tdCenter">\(\small (DE)\)</td>  <td class="tdCenter">11.25</td> <td class="tdCenter">6.17</td>  <td class="tdCenter">0</td> </tr>
      </table>
    <p>
       Here, the minimum distance is \(\small (d(AB), C)\) = 5.67, so \(\small (AB)C\) becomes the next cluster
       and the center of the cluster becomes \( \frac{2 \times (1.5, 4.5) + 1 \times (4, 6)}{2 + 1} \) = (2.3, 5). 
       If we calculate the distance between clusters \(\small (AB)C)\) and \(\small DE\) using the 
       Ward linkage method, we get the following.
    <p>
       \( \small \qquad d((AB)C, (DE)) = \frac{(2.3 - 4.5)^2 + (5 - 3)^2)}{\frac{1}{3} +\frac{1}{2}} = 10.61 \) 
    <p>
       If the above Ward linkage method is displayed as a dendrogram, it is as shown in &lt;Figure 8.2.7&gt;. 
    <p>
       <img class="figure30" src="./figure/Fig080207.png">
       <div class="figText">&lt;Figure 8.2.7&gt; Hierarchical clustering dendrogram using the Ward linkage</div>
  </div>
  <p>

  <div class="mainTableGrey">
       Hierarchical clustering module of 『eStatU』 using the 27 iris data is as follows. You can select clustering methods
       discussed in this section, but it is limited up to 100 observations. 
    <p>
      <!--- ************ html hierarchical clustering ******* ---->
      <b>[Hierarchical clustering\</b>
      <p>
         <iframe class="example140" src="./example/080201.html"> </iframe>
         <div class="figText">&lt;Figure 8.2.8&gt; Hierarchical clustering module in 『eStatU』 </div>
  </div>
  <p>

  <h5>Characteristics of the hierarchical clustering model</h5>
  <div class="mainTable">
       The characteristics of the hierarchical clustering model are as follows.
    <p>
       <div class="textL20M20">1) The hierarchical clustering model is a method of finding a locally optimal cluster at each stage, 
                              so it cannot be considered a general method of optimizing the entire objective function.
       </div>
       <div class="textL20M20">2) Once a cluster is created, the hierarchical clustering model does not consider the dissolution 
                              of the created cluster at all in the next stage. The results of the hierarchical clustering model 
                              are used as the initial clusters of the \(\small K\)-means clustering model in the next section 
                              to test the stability of the results, etc.
       </div>
       <div class="textL20M20">3) When merging clusters in the average linkage method, centroid linkage method, and Ward linkage method, 
                              the size of each cluster is weighted so that clusters with large sizes are merged if possible.
       </div>
  </div>
  <p>

  <!--------------------------------------------------------->
  <h4 id="080202">8.2.2 R practice - Hierarchical clustering</h4>
  <p>
  <div class="mainTable">
       You must install a package called <b>stats</b> to use Hierarchical clustering using R. 
       From the main menu of R,
       select ‘Package’ => ‘Install package(s)’, and a window called ‘CRAN mirror’ will appear. Here, 
       select ‘0-Cloud [https]’ and click ‘OK’. Then, when the window called ‘Packages’ appears, select 
       ‘stats’ and click ‘OK’. dist() and hcluster() are used for the hierarchical clustering, and general usage and 
       key arguments of the functions are described in the following table. 
    <p>
       <table style="width:800px;">
         <tr>
           <th class="tdLeft" colspan="2">Distance Matrix Computation<br>
               This function computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.
           </th>
         </tr>
         <tr>
           <th class="tdLeft" colspan="2">
             dist(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)
           </th>
         </tr>
         <tr>
           <td>x</td>
           <td>a numeric matrix, data frame or "dist" object.</td>
         </tr>
         <tr>
           <td>method</td>
           <td>the distance measure to be used. This must be one of "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski". </td>
         </tr>
         <tr>
           <td>diag</td>
           <td>logical value indicating whether the diagonal of the distance matrix should be printed by print.dist.</td>
         </tr>
         <tr>
           <td>upper</td>
           <td>logical value indicating whether the upper triangle of the distance matrix should be printed by print.dist.</td>
         </tr>
         <tr>
           <td>p</td>
           <td>The power of the Minkowski distance.</td>
         </tr>
       </table>
    <p>
       <table style="width:800px;">
         <tr>
           <th>hclust {stats}</th>
           <th class="tdLeft">Hierarchical Clustering<br>
               Hierarchical cluster analysis on a set of dissimilarities and methods for analyzing it.
           </th>
         </tr>
         <tr>
           <th class="tdLeft" colspan="2">
             hclust(d, method = "complete", members = NULL)
           </th>
         </tr>
         <tr>
           <td>d</td>
           <td>a dissimilarity structure as produced by dist.</td>
         </tr>
         <tr>
           <td>members</td>
           <td>NULL or a vector with length size of d. </td>
         </tr>
       </table>
    <p>
       An example of R commands for a Hierarchical clustering with 30 iris data is as follows. 
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:650px;"><span style="color:red;">> library(stats)</span></td>
           <td class="tdCenter"><button onclick="rFunction(170)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;"><span style="color:red;">> iris <- read.csv('iris30.csv', header=T, as.is=FALSE)</span></td>
           <td class="tdCenter"><button onclick="rFunction(171)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;"><span style="color:red;">> attach(iris)</span></td>
           <td class="tdCenter"><button onclick="rFunction(172)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;">
             # select Sepal.Length, Sepal.Width, Petal.Length, Petal.Width from iris data<br>
             <span style="color:red;">> iris4 <-  iris[, c(2,3, 4, 5)]</span><br>
           </td>
           <td class="tdCenter"><button onclick="rFunction(173)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;">
             # calculate distance matrix using squared Euclid distance<br>
             <span style="color:red;">> dist(iris4, method = 'euclidean')</span><br>
           </td>
           <td class="tdCenter"><button onclick="rFunction(178)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> hclustIris4 <-hclust(distIris4, method = "ward.D")<span></td>
           <td class="tdCenter"><button onclick="rFunction(179)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> hclustIris4</span>
<pre>
Call: hclust(d = distIris4, method = "ward.D")
Cluster method   : ward.D 
Distance         : euclidean 
Number of objects: 30  
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(180)">copy r command</button></td>
         <tr>
           <td># plot hierarchical clusters<br>
               <span style="color:red;">> plot(hclustIris4)<span><br>
       <img class="figure90" src="./figure/Fig080208.png">
       <div class="figText">&lt;Figure 8.2.8&gt; Hierarchical clustering dendrogram using R</div>      
           <td class="tdCenter"><button onclick="rFunction(181)">copy r command</button></td>
         </tr>
       </table>

  </div>
  <p>

  <h3 id="0803">8.3 \(\small K\)-means clustering model</h3>
  <p>

  <div class="mainTable">
       The <b>\(\small K\)-means clustering model</b> is a prototype-based model, and if medians are used instead of means,
       it is called <b>\(\small K\)-median clustering model</b>. These models can be applied to continuous data, 
       and the mean and median of the data can be used as the centers of the clusters, respectively. 
       If there is no outlier, the \(\small K\)-means clustering model is frequently used. 
       A similar concept can be applied to discrete data by defining the centroid of the discrete data. 
    <p>
       The clustering model requires an appropriate distance measure between data and a cluster. 
       \(\small K\)-means clustering model first determines the number of clusters \(\small K\)
       and selects the initial center of each cluster. Then, each data is classified into a cluster with 
       the closest cluster center, and the center of each cluster is recalculated. 
       We can use the various distance measures studied in Chapter 2,
       and in the case of continuous data, the Euclidean distance 
       is generally used. This method is repeated until there is no change in the cluster center. The basic procedure 
       of the \(\small K\)-means clustering model is summarized as follows.
  </div>
  <p>

  <div class="mainTableGrey">
       <b>\(\small K\)-means clustering algorithm</b>
    <p>
        <table>
          <tr>
            <td>Step 1</td>
            <td>Determine the number of clusters \(\small K\) you want.</td>
          </tr>
          <tr>
            <td>Step 2</td>
            <td>Select the initial center of each cluster.</td>
          </tr>
          <tr>
            <td>Step 3</td>
            <td><b>repeat</b></td>
          </tr>
          <tr>
            <td>Step 4</td>
            <td>\(\qquad\)Classify each data into the cluster with the closest cluster center.</td>
          </tr>
          <tr>
            <td>Step 5</td>
            <td>\(\qquad\)Recalculate the center of each cluster.</td>
          </tr>
          <tr>
            <td>Step 6</td>
            <td><b>until</b> (there is little change in the cluster center</td>
          </tr>
         </table>
  </div>
  <p>

  <div class="mainTableGrey">
       <b>Example 8.3.1</b>
       For the two variables \(x_{1}\) and \(x_{2}\), four data were observed as follows. Find two clusters 
       using the 2-means clustering algorithm with the squared Euclid distance between the data. 
    <p>
       <table style="width:400px"> 
         <tr>
           <th colspan="2">Table 8.3.1  Data for the 2-means clustering algorithm</th>
         </tr>
         <tr> 
           <th style="width:100px;">Data</th>
           <th>(\(x_{1}, x_{2}\))</th>
         </tr>
         <tr>
           <td class="tdCenter">A</td>
           <td class="tdCenter">(3, 4)</td>
         </tr>
         <tr>
           <td class="tdCenter">B</td>
           <td class="tdCenter">(-1, 2)</td>
         </tr>
         <tr>
           <td class="tdCenter">C</td>
           <td class="tdCenter">(-2, -3)</td>
         </tr>
         <tr>
           <td class="tdCenter">D</td>
           <td class="tdCenter">(1, -2)</td>
         </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       Let the center of cluster 1 be data A=(3,4) and the center of cluster 2 be data C=(-2,-3). The distances 
       from each data to the centers of the two clusters are as follows.
    <p>
       <table style="width:500px"> 
         <tr>
           <th colspan="3">Table 8.3.2  Distance between data and the center of cluster</th>
         </tr>
         <tr> 
           <th style="width:100px;">Data</th>
           <th>Cluster 1<br>Distance to center (3, 4)</th>
           <th>Cluster 2<br>Distance to center (-2, -3)</th>
         </tr>
         <tr>
           <td class="tdCenter">A</td>
           <td class="tdCenter">0</td>
           <td class="tdCenter">74</td>
         </tr>
         <tr>
           <td class="tdCenter">B</td>
           <td class="tdCenter">20</td>
           <td class="tdCenter">26</td>
         </tr>
         <tr>
           <td class="tdCenter">C</td>
           <td class="tdCenter">74</td>
           <td class="tdCenter">0</td>
         </tr>
         <tr>
           <td class="tdCenter">D</td>
           <td class="tdCenter">40</td>
           <td class="tdCenter">10</td>
         </tr>
       </table>
    <p>
       Therefore, if each data is classified by the nearest cluster center, data A and B are classified into cluster 1, 
       data C and D are classified into cluster 2, and the center of the new cluster 1 by the average is (1,3), 
       and the center of cluster 2 is (-0.5,-2.5). The distances from each data to the centers of the two new clusters are as follows.
    <p>
       <table style="width:500px"> 
         <tr>
           <th colspan="3">Table 8.3.3  Modified distance between data and the center of cluster</th>
         </tr>
         <tr> 
           <th style="width:100px;">Data</th>
           <th>Cluster 1<br>Distance to center (1, 3)</th>
           <th>Cluster 2<br>Distance to center (-0.5, -2.5)</th>
         </tr>
         <tr>
           <td class="tdCenter">A</td>
           <td class="tdCenter">5</td>
           <td class="tdCenter">54.5</td>
         </tr>
         <tr>
           <td class="tdCenter">B</td>
           <td class="tdCenter">5</td>
           <td class="tdCenter">20.5</td>
         </tr>
         <tr>
           <td class="tdCenter">C</td>
           <td class="tdCenter">45</td>
           <td class="tdCenter">2.5</td>
         </tr>
         <tr>
           <td class="tdCenter">D</td>
           <td class="tdCenter">25</td>
           <td class="tdCenter">2.5</td>
         </tr>
       </table>
    <p>
       If each data is classified by the nearest cluster center, data A and B are classified again as cluster 1, 
       and data C and D are classified as cluster 2, so the center of each cluster does not change, so the algorithm is stopped. 
       Finally, data A and B are classified as cluster 1, and data C and D are classified as cluster 2.
  </div>
  <p>

  <h5>Theoretical background of the \(\small K\)-means clustering model</h5>
  <p>
  <div class="mainTable">
       The hierarchical clustering model in Section 8.2 has a disadvantage: if data is assigned to a specific cluster,
       it cannot be reassigned to another cluster. The \(\small K\)-means clustering model, on the other hand, 
       can assign the data to a different group in the next clustering stage.
       Let's look at the theoretical background of the \(\small K\)-means clustering model. 
    <p>
       Suppose there are \(m\) variables \(\small \boldsymbol x = (x_{1}, x_{2}, ... , x_{m}) \) and 
       \(n\) number of data observed for these variables. Let the \(\small K\) clusters be \(\small G_{1}, G_{2}, ... , G_{K}\), 
       the number of data observed in each cluster be \(\small n_{1}, n_{2}, ... , n_{K}\), and the mean of each cluster
       be \(\small \boldsymbol c_{1}, \boldsymbol c_{2}, ... , \boldsymbol c_{K} \) as follows. 
       $$ 
         \boldsymbol c_{i} = \frac{1}{n_{i}} \; \sum_{\boldsymbol x \in G_{i}} \; \boldsymbol x  
       $$
       If \(\small d(\boldsymbol c_{i}, \boldsymbol x)\) is the distance between the center \(\small \boldsymbol c_{i}\) of cluster 
       \(\small G_{i}\) and the data \(\small \boldsymbol x\), the measure of cluster performance can be defined as 
       the sum of distances from all data to each center. 
       $$
         \text{(Performance measure of clustering)} = \sum_{i=1}^{K} \; \sum_{\boldsymbol x \in G_{i}} \;  d( \boldsymbol c_{i} ,  \boldsymbol x)
       $$
       When using the squared Euclid distance as a distance measure, the \(\small \boldsymbol c_{i}\)
       that minimizes this performance measure of clustering can be shown to be the mean of the cluster. 
       Here, let us prove that \(\small \boldsymbol c_{i}\) is the cluster's mean when the data is only one-dimensional. 
       The performance measure of clustering is the following within sum of squares (WSS) in the case of the squared Euclid distance.
       $$
         \text{WSS} = \sum_{i=1}^{K} \; \sum_{x \in G_{i}} \;  (c_{i} -  x)^2
       $$
       In order to find \(\small c_{1}, c_{2}, ... , c_{K} \) that minimizes this WSS, we take partial differentiation 
       for each \(\small c_{i}, i=1,2, ... , K\), and set it to 0.
       $$
         \frac{\partial}{\partial c_{i}} \text{SSE} = \sum_{x \in G_{i}} \;  2 (c_{i} -  x) = 0
       $$
       The solution to these simultaneous equations is as follows. 
       $$
         c_{i} = \frac{1}{n_{i}} \; \sum_{x \in G_{i}} \;  x
       $$
       That is, \(\small c_{1}, c_{2}, ... , c_{K} \) that minimize the SSE are each mean of the clusters.
    <p>
       If the data is one-dimensional and the absolute distance (Manhattan distance) is used as a distance measure, 
       the performance measure of clustering becomes the sum of absolute error (SAE) as follows.
       $$
         \text{SAE} = \sum_{i=1}^{K} \; \sum_{x \in G_{i}} \;  | c_{i} -  x |
       $$
       It can be shown that the solution \(\small c_{1}, c_{2}, ... , c_{K} \) that minimizes this SAE
       are each median of the clusters. In general, the median is known to be less sensitive to extreme points or outliers.
  </div>
  <p>

  <h5>Determine the number of cluster</h5>
  <p>
  <div class="mainTable">
       In general, it is not easy to determine the number of clusters in \(\small K\)-means clustering model. 
       One method is first to examine the clustering results of the hierarchical clustering model in Section 8.2 
       and then decide the number of clusters. Another useful way is to analyze various \(\small K\) values ​​and then compare 
       the within sum of squares. Selection of \(\small K\), which has the minimum within sum of squares is reasonable.
       However, since the \(\small K\)-means clustering algorithm finds a solution that minimizes the within sum of squares, 
       this algorithm may find a local minimum rather than a global minimum. It is desirable to run the 
       initial center of each cluster as multiple data to prevent this problem and select a cluster with a smaller WSS. 
    <p>
       \(\small K\)-means clustering module of 『eStatU』 provides a plot of the within sum of squares for various \(\small K\)
       as follows. After selecting \(\small K\), you can do clustering analysis by checking 'fixed K'.
  </div>
  <p>
  <div class="mainTableGrey">
      <!--- ************ html K-means clustering ******* ---->
      <b>[\(\small K\)-means clustering]</b>
      <p>
         <iframe class="example140" src="./example/080301.html"> </iframe>
         <div class="figText">&lt;Figure 8.3.1&gt; \(\small K\)-means clustering in 『eStatU』</div>
  </div>
  <p>

  <div class="mainTable">
       In general, extreme points or outliers can seriously affect the \(\small K\)-means clustering model, 
       and we should try various distance measures. In other words, if there is an extreme point, the average of 
       that cluster is not suitable as a center measure for the cluster. 
       You can remove extreme points or outliers through exploratory data analysis to prevent this.
  </div>
  <p>

  <!--------------------------------------------------------->
  <h4 id="080301">8.3.1 R practice - \(\small K\)-means clustering</h4>
  <p>
  <div class="mainTable">
       To use \(\small K\)-means clustering using R, you need to install a package called <b>stats</b>. 
       From the main menu of R,
       select ‘Package’ => ‘Install package(s)’, and a window called ‘CRAN mirror’ will appear.  
       Select ‘0-Cloud [https]’ and click ‘OK’. Then, when the window called ‘Packages’ appears, select 
       ‘stats’ and click ‘OK’. The following table describes the function's general usage and key arguments. 
    <p>
       <table style="width:800px;">
         <tr>
           <th class="tdLeft" colspan="2">\(\small K\)-Means Clustering<br>
               Perform k-means clustering on a data matrix.
           </th>
         </tr>
         <tr>
           <th class="tdLeft" colspan="2">
             kmeans(x, centers, iter.max = 10, nstart = 1, algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"), trace = FALSE)
           </th>
         </tr>
         <tr>
           <td>x</td>
           <td>numeric matrix of data, or an object that can be coerced to such a matrix (such as a numeric vector or a data frame with all numeric columns).</td>
         </tr>
         <tr>
           <td>centers</td>
           <td>either the number of clusters, say k, or a set of initial (distinct) cluster centres. If a number, a random set of (distinct) rows in x is chosen as the initial centres.</td>
         </tr>
         <tr>
           <td>test</td>
           <td>The data set for which we want to obtain the k-NN classification, i.e. the test set.</td>
         </tr>
         <tr>
           <td>iter.max</td>
           <td>the maximum number of iterations allowed.</td>
         </tr>
         <tr>
           <td>nstart</td>
           <td>if centers is a number, how many random sets should be chosen?</td>
         </tr>
       </table>
    <p>
       An example of R commands for a \(\small K\)-means clustering with iris data when k = 3 is as follows. 
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:650px;"><span style="color:red;">> library(stats)</span></td>
           <td class="tdCenter"><button onclick="rFunction(170)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;"><span style="color:red;">> iris <- read.csv('iris150.csv', header=T, as.is=FALSE)</span></td>
           <td class="tdCenter"><button onclick="rFunction(171)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;"><span style="color:red;">> attach(iris)</span></td>
           <td class="tdCenter"><button onclick="rFunction(172)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:650px;">
             # select Sepal.Length, Sepal.Width, Petal.Length, Petal.Width from iris data<br>
             <span style="color:red;">> iris4 <-  iris[, c(2,3, 4, 5)]</span><br>
           </td>
           <td class="tdCenter"><button onclick="rFunction(173)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> iriskmeans <- kmeans(iris4, centers = 3, iter.max = 1000)<span></td>
           <td class="tdCenter"><button onclick="rFunction(174)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> iriskmeans</span>
<pre>
K-means clustering with 3 clusters of sizes 38, 62, 50
Cluster means:
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1     6.850000    3.073684     5.742105    2.071053
2     5.901613    2.748387     4.393548    1.433871
3     5.006000    3.428000     1.462000    0.246000
Clustering vector:
  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1
[112] 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 2 1
[149] 1 2

Within cluster sum of squares by cluster:
[1] 23.87947 39.82097 15.15100
 (between_SS / total_SS =  88.4 %)
Available components:
[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"   
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(175)">copy r command</button></td>
         <tr>
           <td># the cluster vector only can be seen as follows. <br>
               1: verginica, 2: versicolor, 3: setosa<br>
               <span style="color:red;">> iriskmeans$cluster<span>
<pre>
  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1
[112] 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 2 1
[149] 1 2
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(176)">copy r command</button></td>
         </tr>
         </tr>
       </table>
    <p>
       To make a classification cross table, you can use a vector of Species and iriskmeans$cluster which is the cluster  
       with table command as below. 'Setosa's are clustered 100% correctly, 'versicolor's are 48/50, but 'virginica's
       are 36/50. 
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:650px;">
              # 1: verginica, 2: versicolor, 3: setosa<br>
              <span style="color:red;">> classtable <- table(Species, iriskmeans$cluster)</span>
<pre>
Species       1  2  3
  setosa      0  0 50
  versicolor  2 48  0
  virginica  36 14  0
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(177)">copy r command</button></td>
         </tr>
       </table>
  </div>
  <p>

  <!--------------------------------------------->
  <h3 id="0804">8.4 Exercise</h3>
  <p>
  <div class="mainTableExercise">

    <p class="textL30M30">8.1  The distance matrix for five data (A, B, C, D, E) is as follows.
        <table style="width:300px"> 
           <tr> <th>Distance</th> <th>A</th> <th>B</th> <th>C</th> <th>D</th> <th>E</th> </tr>
           <tr> <td class="tdCenter">A</td> <td class="tdCenter">0</td> <td class="tdCenter"></td> <td class="tdCenter"></td> <td class="tdCenter"></td> <td class="tdCenter"></td> </tr>
           <tr> <td class="tdCenter">B</td> <td class="tdCenter">10</td> <td class="tdCenter">0</td> <td class="tdCenter"></td> <td class="tdCenter"></td> <td class="tdCenter"></td> </tr>
           <tr> <td class="tdCenter">C</td> <td class="tdCenter">41</td> <td class="tdCenter">64</td> <td class="tdCenter"> 0</td> <td class="tdCenter"></td> <td class="tdCenter"></td> </tr>
           <tr> <td class="tdCenter">D</td> <td class="tdCenter">55</td> <td class="tdCenter">47</td> <td class="tdCenter">44</td> <td class="tdCenter">0</td> <td class="tdCenter"></td> </tr>
           <tr> <td class="tdCenter">E</td> <td class="tdCenter">35</td> <td class="tdCenter">98</td> <td class="tdCenter">85</td> <td class="tdCenter">76</td> <td class="tdCenter">0</td> </tr>
         </table>
      <div class="textL30M20">
        1) Create a hierarchical cluster using the single linkage method.
       </div>
      <div class="textL30M20">
        2) Create a hierarchical cluster using the complete linkage method.
      </div>
      <div class="textL30M20">
        3) Create a hierarchical cluster using the average linkage method.
      </div>
      <div class="textL30M20">
        4) Create a hierarchical cluster using the centroid linkage method.
      </div>
      <div class="textL30M20">
        5) Create a hierarchical cluster using the Ward linkage method.
      </div>

    <p class="textL30M30">8.2  The following six data were observed for two variables \(X_1\) and \(X_2\). 
        <table style="width:300px"> 
           <tr> <th>Data</th> <th>\(X_1\)</th> <th>\(X_2\)</th> </tr>
           <tr> <td class="tdCenter">A</td> <td class="tdCenter"> 3</td> <td class="tdCenter"> 4</td> </tr>
           <tr> <td class="tdCenter">B</td> <td class="tdCenter">-1</td> <td class="tdCenter"> 2</td> </tr>
           <tr> <td class="tdCenter">C</td> <td class="tdCenter">-2</td> <td class="tdCenter">-3</td> </tr>
           <tr> <td class="tdCenter">D</td> <td class="tdCenter"> 1</td> <td class="tdCenter">-2</td> </tr>
           <tr> <td class="tdCenter">E</td> <td class="tdCenter"> 1</td> <td class="tdCenter"> 3</td> </tr>
           <tr> <td class="tdCenter">E</td> <td class="tdCenter">-1</td> <td class="tdCenter"> 2</td> </tr>
         </table>
      <div class="textL30M20">
        1) Calculate the distance between the data using the Euclidean square distance. 
           Create a hierarchical cluster using the single linkage method.
       </div>
      <div class="textL30M20">
        2) Create a hierarchical cluster using the complete linkage method.
      </div>
      <div class="textL30M20">
        3) Create a hierarchical cluster using the average linkage method.
      </div>
      <div class="textL30M20">
        4) Create a hierarchical cluster using the centroid linkage method.
      </div>
      <div class="textL30M20">
        5) Create a hierarchical cluster using the Ward linkage method.
      </div>

    <p class="textL30M30">8.3  Create clusters using the 2-mean clustering model. 
          Use the mean as the central measure in the data of Problem 2.

  </div>

</div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="https://maxcdn.bootstrapcdn.com/js/ie10-viewport-bug-workaround.js"></script>
    <script>
        //document.getElementById('sidebar').getElementsByTagName('ul')[0].className += "nav nav-sidebar";
        
        /* ajust the height when click the toc
           the code is from https://github.com/twbs/bootstrap/issues/1768
        */
        var shiftWindow = function() { scrollBy(0, -50) };
        window.addEventListener("hashchange", shiftWindow);
        function load() { if (window.location.hash) shiftWindow(); }
        
        /*add Bootstrap styles to tables*/
        var tables = document.getElementsByTagName("table");
        for(var i = 0; i < tables.length; ++i){
            tables[i].className += "table table-bordered table-hover";
        }
    </script>

</body>
</html>

