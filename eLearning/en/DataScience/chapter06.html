<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

  <title>Chapter 6</title>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
  <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
  <link href="/estat/eStat/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="/estat/eStat/css/dashboard.css" rel="stylesheet">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <style type="text/css">code{white-space: pre;}</style>
       <style type="text/css">.sidebar ul{padding-left: 10px;}</style>
       <link rel="stylesheet" href="/estat/eStat/css/prism.css">
       <link rel="stylesheet" href="/estat/eStat/css/pandoc.css">
       <script src="/estat/eStat/lib/d3/d3.v4.min.js"></script>   
       <script src="/estat/eStat/lib/jquery/jquery.min.js"></script>    
       <script src="/estat/eStat/lib/DistributionsUtil.js" ></script>
       <script src="/estat/eStat/lib/FileSaver.min.js" ></script>
       <script src="/estat/eStat/lib/convertSVG.js"></script>
       <script src="/estat/eStat/js/prism.js"></script>
       <script src="/estat/eStat/js/eBook.js"></script>
       <script src="/estat/eStat/js/eStatU.js"></script>
       <script src="/estat/eStat/js/language.js" ></script>
       <script> setLanguage('en'); </script>
       <script type="text/javascript" id="MathJax-script" async
	       src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
       </script>

       <script>
	 $(document).ready(function() {
  	     toc = $("#sidebar > ul > li > ul");
  	     sections = toc.children();   // <li>
  	     for(var i=0; i<sections.length; i++) {
  		 if ($(sections[i]).children().length == 1) { continue; }
  		 var first = sections[i].firstElementChild;  // <a>
  		 var last = sections[i].lastElementChild;
  		 var li = $("<li>");
  		 var details = $("<details>");
  		 var summary = $("<summary>");
  		 $(summary).append(first)
  		 $(details).append(summary);
  		 $(details).append(last);
  		 $(li).append(details);	
  		 $(sections[i]).replaceWith(li);
  	     }
	 });
       </script>

</head>

<body>
  
<!-- begin of left sidebar -->
<div class="container-fluid">
<div class="row">
<div id="sidebar" class="col-sm-2 col-md1 sidebar">
  <ul>
    <li> <a href="index.html"><b>Table of chapters</b> </a> </li>
    <li> <a href="chapter01.html"> 1 Data science and artificial intelligence</a> </li>
    <li> <a href="chapter02.html"> 2 Data visualization</a> </li>
    <li> <a href="chapter03.html"> 3 Data summary and transformation</a> </li>
    <li> <a href="chapter04.html"> 4 Probability and distribution</a> </li>
    <li> <a href="chapter05.html"> 5 Estimation, testing hypothesis, and regression analysis</a> </li>
    <li> <a href="chapter06.html"> 6 Supervised machine learning for categorical data</a> </li>
          <ul>
                <li><a href="#0601">&nbsp;&nbsp; 6.1 Basic concept of supervised machine learning and classification</a></li>
                <li><a href="#0602">&nbsp;&nbsp; 6.2 Decision tree model</a></li>
                <li><a href="#0603">&nbsp;&nbsp; 6.3 Naive Bayes classification model</a></li>
                <li><a href="#0604">&nbsp;&nbsp; 6.4 Evaluation and comparision of classification model</a></li>
                <li><a href="#0605">&nbsp;&nbsp; 6.5 Exercise</a></li>
          </ul>
    <li> <a href="chapter07.html"> 7 Supervised machine learning for continuous data</a> </li>
    <li> <a href="chapter08.html"> 8 Unsupervised machine learning</a> </li>
    <li> <a href="chapter09.html"> 9 Artificial intelligence and other applications</span></a> </li>
  </ul>
</div>
</div>
</div>
<!-- end of left sidebar -->

  
<div class="col-sm-9 col-sm-offset-3 col-md-10 col-md-offset-2 main">

  <h2>Chapter 6. Supervised machine learning for categorical data</h2> 
  <h6>
      <a href="./pdf/ppt6.pdf" target="_blank"><u>[presentation]</u></a>&nbsp;&nbsp;&nbsp;
      <a href="./pdf/book6.pdf" target="_blank"><u>[book]</u></a>
  </h6>
	    <ul>
              <li class="listNone"><a href="#0601">6.1 Basic concepts of supervised machine learning and classification</a></li>
                <li class="listNoneIndent"><a href="#060101">6.1.1  Evaluation measures of classification model</a></li>
                <li class="listNoneIndent"><a href="#060102">6.1.2  Spliting method for training and testing data</a></li>
              <li class="listNone"><a href="#0602">6.2 Decision tree model</a></li>
                <li class="listNoneIndent"><a href="#060201">6.2.1  Decision tree algorithm</a></li>
                <li class="listNoneIndent"><a href="#060202">6.2.2  Selection of a variable for branching</a></li>
                <li class="listNoneIndent"><a href="#060203">6.2.3  Categorization of a continuous variable</a></li>
                <li class="listNoneIndent"><a href="#060204">6.2.4  Overfitting and pruning decision tree</a></li>
                <li class="listNoneIndent"><a href="#060205">6.2.5  R practice - decision tree</a></li>
              <li class="listNone"><a href="#0603">6.3 Naive Bayes classification model</a></li>
                <li class="listNoneIndent"><a href="#060301">6.3.1  Bayes classification model</a></li>
                <li class="listNoneIndent"><a href="#060302">6.3.2  Naive Bayes classification model for categorical data</a></li>
                <li class="listNoneIndent"><a href="#060303">6.3.3  Stepwise variable selection</a></li>
                <li class="listNoneIndent"><a href="#060304">6.3.4  R practice - Naive Bayes classification</a></li>
              <li class="listNone"><a href="#0604">6.4 Evaluation and comparison of classification model</a></li>
                <li class="listNoneIndent"><a href="#060401">6.4.1  Evaluation of classification model</a></li>
                <li class="listNoneIndent"><a href="#060402">6.4.2  Comparison of classification models</a></li>
              <li class="listNone"><a href="#0605">6.5 Exercise</a></li>
	    </ul>
  <p>

  <h5>CHAPTER OBJECTIVES </h5> 
  <div class="mainTable">
       The classification analysis technique uses data with known group membership 
       to create a model to determine the data group with unknown group membership. It has been used in traditional Statistics. 
       Many new models similar to the classification analysis have been developed to train a computer 
       for artificial intelligence. 
       All these models for classification are called models for 'supervised machine learning.'
       We introduce the following in this chapter.
    <p>
       <div class="textL30M20">
         • Basic concepts of supervised machine learning and introduce classification analysis models in section 6.1. 
       </div>
       <div class="textL30M20">
         • Decision tree model for categorical data in section 6.2.
       </div>
       <div class="textL30M20">
         • General Bayes classification model, a basic statistical classification analysis,
           and the naive Bayes classification model for categorical data in section 6.2.
       </div>
       <div class="textL30M20">
         • Evaluation of classification model and comparison methods for several classification models.
       </div>
  </div>
  <p>

  <!------------------------------------------------------------->
  <h3 id="0601">6.1 Basic concepts of supervised machine learning and classification</h3>
  <p>
  <div class="mainTable">
       <b>Supervised machine learning</b> is a technique that uses data with known group affiliation to create 
       a model to determine the group of data whose group affiliation is unknown. It is called as 
       <b>discriminant analysis</b> or <b>classification analysis</b> in traditional statistics.
       In contrast, a technique that determines groups of similar data using data with unknown group affiliation
       is called <b>cluster analysis</b> or <b>unsupervised machine learning</b>. A supervised machine learning model 
       is applied in various fields, such as when a doctor examines a patient and classifies the patient's 
       diagnosis based on the examination record, when spam mail is filtered out using email subject lines, 
       and when a customer visiting a department store is identified as a customer who will purchase a product. 
    <p>
       A standard method to classify data whose group affiliation is unknown into a group would be 
       to classify it into the group that is the ‘closest’ to that data. 
       Many methods define 'closeness' as being from one data source to a group.
       For example, the Euclid distance from the data 
       to the mean of each group, the Minkowski distance, or the Mahalanobis distance,  
       a statistical measure that considers the variance of the data, can be used. If we consider 
       a probabilistic approach, there may be a method to estimate the distribution function of each group 
       and classify data whose group affiliation is unknown into the group it is likely to belong to. 
       There are many classification models based on various reasonable criteria. In this chapter, 
       we focus on statistical models for discrete data, such as the decision tree model and the naive Bayes classification model.
       Chapter 7 discusses other classification models for continuous data,
       such as the k-nearest neighbor model, the neural network model, the support vector machine model, and the ensemble model. 
  </div>

  <!------------------------------------------------------------->
  <h5>Classification analysis procedure</h5>
  <p>
  <div class="mainTable">
       Assume that there are \(K\) number of groups (or classes) and \(n_1 , n_2 , ... , n_K\) data
       were observed in each group. Let \(\boldsymbol x = (x_1 , x_2 , ... , x_m )\) be the observed data of
       the random variable \(\boldsymbol X = (X_1 , X_2 , ... , X_m )\). &lt;Figure 6.1.1&gt; shows 
       the general process of the classification analysis.
    <p>
       <table style="width:500px; margin-left:70px">
         <tr>
           <td class="tdLeft" style="border:2px solid black;  margin: 10px 10px 10px 10px;">
             <b>Data cleaning and division</b><br>
             Collect data from each group, clean and transform it, and divide the data into training and test data.
           </td>
         </tr>
         <tr>
           <td class="tdCenter"><span style="font-size:30px">&darr; &uarr;</span></td>
         </tr>
         <tr>
           <td class="tdLeft" style="border:2px solid black; margin: 10px 10px 10px 10px;">
             <b>Training the model</b><br>
             Establish a classification model \(y = f( \boldsymbol x ) \) using the training data.<br>
             &nbsp;
           </td>
         </tr>
         <tr>
           <td class="tdCenter"><span style="font-size:30px">&darr; &uarr;</span></td>
         </tr>
         <tr>
           <td class="tdLeft" style="border:2px solid black; margin: 10px 10px 10px 10px;">
             <b>Validation of the model</b><br>
             Validate the classification model using the testing data.<br>
             &nbsp;
           </td>
         </tr>
         <tr>
           <td class="tdCenter"><span style="font-size:30px">&darr;</span></td>
         </tr>
         <tr>
           <td class="tdLeft" style="border:2px solid black; margin: 10px 10px 10px 10px;">
             <b>Apply the model</b><br>
             Classify data whose group affiliation is unknown into one group using the classification model.
           </td>
         </tr>
       </table>
    <p>
       <div class="figText">&lt;Figure 6.1.1&gt; General process of classification analysis</div>
    <br>
       First, data is collected, refined, and transformed appropriately for analysis. Then, the observed data 
       in each group is divided into training data and testing data. Using the training data, establish
       a classification model \(y = f( \boldsymbol x ) \) where \(y\) is the target variable to be estimated, 
       which means the group. The validity of the model is examined using the testing data. 
       If this model is not satisfactory, another model is established again, and its validity is examined. 
       After comparing several classification models and selecting the most satisfactory classification model,
       this model is applied to data whose group is unknown to determine which group to classify. 
       If the variable that represents each group is considered as a dependent variable and the variable 
       used to classify each group is considered as an independent variable, classification analysis is 
       similar to regression analysis. However, in regression analysis, the dependent variable is 
       a random variable that follows a normal distribution, and the levels of the independent variables 
       are assumed to be given constants, but classification analysis does not make this assumption, 
       so there is a difference in the model.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h5>Preparation of data for classification analysis</h5>
  <p>
  <div class="mainTable">
       Data for classification analysis should be prepared in advance to improve classification accuracy, 
       efficiency, and scalability.
    <br>
       <h6>A. Data cleaning</h6>
       If there is noise in the data, it is recommended to remove it. If there are missing values, 
       it is recommended to preprocess using the corresponding variable's average or mode. 
    <br>
       <h6>B. Relevance analysis</h6>
       Among the variables in the data, there may be variables that are not related to classification, 
       or the variables may be duplicated. Relevance analysis can be used to remove irrelevant or 
       duplicated variables, improving the classification model's efficiency and accuracy.
    <br>
       <h6>C. Data transformation</h6>
       Continuous data may need to be discretized for classification. 
       The neural network model requires converting the units of variable values, 
       such as from -1.0 to 1.0 or from 0.0 to 1.0.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060101">6.1.1 Evaluation measures of classification model</h4>
  <p>
  <div class="mainTable">
       Suppose there are two groups \(\small G_1 , G_2\), and there are \(n\) number of data whose group affiliation is known.  
       If a classification model is used to classify each data, 
       the actual group of data and the group classified by the model can be compared and summarized in Table 6.1.1.
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 6.1.1  Table for the test results of the actual group and the classified group</th>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2">Classified group</th>
          <th></th>
        <tr> 
          <th></th>
          <th></th>
          <th>\(\small G_1\)</th>
          <th>\(\small G_2\)</th>
          <th>Total</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Actual group</td>
          <td class="tdCenter">\(\small G_1\)</td>
          <td class="tdCenter">\(\small f_{11}\)</td>
          <td class="tdCenter">\(\small f_{12}\)</td>
          <td class="tdCenter">\(\small f_{11} + f_{12}\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\small G_2\)</td>
          <td class="tdCenter">\(\small f_{21}\)</td>
          <td class="tdCenter">\(\small f_{22}\)</td>
          <td class="tdCenter">\(\small f_{21} + f_{22}\)</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">\(n\)</td>
        </tr>
      </table>
    <br>
       Here, \(f_{ij}\) means the number of data of the group \(G_i\) classified into the group \(G_j\).
       The number of data correctly classified out of the total data is \(f_{11} + f_{22}\), and the number of data 
       incorrectly classified is \(f_{12} + f_{21}\). The <b>accuracy</b> of the classification model
       is defined as the ratio of the number of correctly classified data out of the total number of data, 
       and the <b>error rate</b> is defined as the ratio of the number of incorrectly classified data 
       out of the total number of data.
       $$
       \begin{align}
         \text{Accuracy} &=  \frac{f_{11} + f_{22}}{n} \\
         \text{Error rate} &=  \frac{f_{12} + f_{21}}{n}
       \end{align}
       $$
    <p>
       Generally, classification models strive to find an algorithm that maximizes accuracy or 
       minimizes error rate. Accuracy and error rate are reasonable criteria assuming that each data 
       belongs to one group. However, there is a possibility that one data belongs to more than one group, 
       in which case it is reasonable to predict the probability of belonging to the group. 
       There are various measures other than accuracy and error rate to evaluate a classification model, 
       and lift charts, ROC graphs, and statistical analysis methods are used to compare and evaluate various 
       classification models, which are explained in detail in Section 6.4.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060102">6.1.2 Spliting method for training and testing data</h4>
  <p>
  <div class="mainTable">
       To objectively evaluate a classification model, the entire data set is generally divided 
       into <b>training data</b> and <b>testing data</b>. The model is established using the training data, and the accuracy 
       of the model is evaluated using the testing data. If there is sufficient data, <b>validation data</b> is set aside 
       to improve the performance of the model. This section introduces commonly used methods for dividing 
       training and testing data.
  </div>
  <p> 

  <h5>A. Holdout Method and Random Subsampling</h5>
  <div class="mainTable">
       The <b>holdout method</b> first divides the entire data set into two non-overlapping data sets 
       and holding out one as training data and the other as testing data. The holdout method is 
       a widely used method in which a classification model is established using the training data, 
       and this model is applied to the testing data to measure the accuracy (or error rate). 
       The ratio at which training and test data are divided depends on the researcher's judgment, 
       but the most commonly used method is (1/2 training: 1/2 test) or (2/3 training: 1/3 test). 
       When extracting data at a determined ratio, a simple random sampling method without replacement is used 
       to reduce bias. The following should be noted when using the reserve method.
    <p>
       <div class="textL20M20">
         1) Since a portion of the entire data for which the group is known is reserved as test data,
         there is a risk that the absolute number of training data for establishing the classification model 
         will be small. In this case, the classification model created with only the training data may not be
         as good as the model created using all the data.
       </div>
       <div class="textL20M20">
         2) The classification model may vary depending on how the training and test data are divided. 
         In general, the smaller the number of training data, the greater the variance in the accuracy of the model.
         On the other hand, as the number of training data increases, the reliability of the accuracy estimated 
         from the test data decreases.
       </div>
       <div class="textL20M20">
         3) Since the training and test data are subsets of the entire data set, they are not independent.
       </div>
    <p>
       To solve the above problems and to increase the reliability of the accuracy of the classification model, 
       the preliminary method can be repeatedly performed. Each non-replaced sample is called a subsampling, 
       and the method of repeatedly extracting them is called the random subsampling method. If the accuracy of 
       the classification model by the \(i^{th}\) subsampling is \( (Accuracy)_i\), and this experiment is repeated 
       \(r\) times, the overall accuracy of the classification model is defined as the average of each accuracy
       as follows;
       $$
         \text{Overall accuarcy} = \frac{1}{r} \Sigma_{i=1}^r (Accuracy)_i
       $$
       Since the random subsampling method does not use the entire data set, just like the holdout method, 
       it still has problems. However, since the accuracy of the model is repeatedly estimated, 
       the reliability can be increased.
  </div>
  <p>

  <h5>B. Cross Validation Method</h5>
  <div class="mainTable">
       The <b>cross-validation method</b> is a method that attempts to solve the problems of random subsampling.
       Like the holdout method, the entire data set is first divided into training data and testing data. 
       We create a model using the training data and record the number of data correctly classified by the model 
       using testing data. Then, the roles of testing data and training data are swapped, and the number of 
       correctly classified data is added up. It is called the <b>two-fold cross-validation method</b>, and the accuracy 
       of the model is calculated as the number of data correctly classified in the entire data. The experiment 
       is repeated \(r\) times in the same way to obtain the average accuracy. In this method, each data is used once 
       for training and again for testing, which can somewhat solve the problem of random subsampling.
    <p>
       If the two-fold cross-validation method is extended, the <b>\(k\)-fold cross-validation method</b> can be created.
       This method divides the entire data set into \(k\) equal-sized subsets, reserves one of the subsets as 
       testing data, and uses the remaining data as training data to obtain the classification function. 
       This method is repeated \(k\) times so that each data subset can be used for testing once. 
       The accuracy of the classification model is the average of the measured accuracies. 
    <p>
       In the cross-validation method, a special case where the total number of data is \(k\), that is, 
       when there is only one testing data, is called the <b>leave-one-out method</b>. This method has the 
       advantage of maximizing the training data and testing all data without overlapping the test data. 
       However, since each testing data contains only one data, there is a disadvantage in that the variance 
       of the estimated accuracy increases, and since the experiment must be repeated as many times as 
       the number of data, it takes a lot of time.
  </div>

  <h5>C. Bootstrap method</h5>
  <div class="mainTable">
       The methods described above extract the training data set from the entire data set without replacement, 
       so there is no identical data in the training and test data. The <b>bootstrap method</b> extracts 
       the training data with replacement. The data extracted once is not removed from the entire data set,
       and the next data is extracted. When the total number of data is \(N\), and the bootstrap method extracts data, 
       approximately 63.2% of the entire data is extracted as training data. This is because the probability of 
       each data being extracted as a bootstrap sample is \(1 - (1 - \frac{1}{N})^N\), and this probability 
       asymptotically converges to \(1 - \frac{1}{e} \)= 0.632 if \(N\) is sufficiently large. Samples not 
       extracted by the bootstrap method are used as testing data. A classification model is established 
       using the data set extracted with replacement as training, and this model is applied to the testing data
       to investigate the accuracy. The average of the accuracies measured by repeating similar experiments 
       is used as the model's overall accuracy.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h3 id="0602">6.2 Decision tree model</h3>
  <p>
  <div class="mainTable">
       <b>Decision tree</b> is a tree-shaped drawing of a classification function consisting of decision rules,
       such as &lt;Figure 6.2.1&gt; which classifies whether a customer visiting a computer store 
       will purchase a computer. Decision trees are widely used because they are easy to understand 
       in terms of classification methods and easy to explain results.
    <p>
       <img class="figure50" src="./Figure/Fig060201.png">
       <div class="figText">&lt;Figure 6.2.1&gt; A decision tree to classify a customer whether he buys or not</div>
    <br>
       In the tree diagram above, the ovals colored yellow represent <b>nodes</b> that indicate tests for variables, and the top node
       is called the <b>root node</b>. In &lt;Figure 6.2.1&gt;, Credit is the root node, and Gender and Age are nodes
       for testing variables. The <b>branches</b> from the nodes represent the values ​​of the tested variables, 
       and the <b>rectangles</b> colored blue or red represent the final classified groups, 
       which are called <b>leaves</b>. The root node, Credit, has three branches 'Bad', 'Fair', and 'Good', 
       and the classified group for 'Bad' credit is indicated by the leaf 'No' which is the Non-purchasing group.
       In order to classify the data whose group is unknown, the variable values ​​of the data are examined 
       along the path from the root node to the leaves. For example, in &lt;Figure 6.2.1&gt;, a person 
       whose 'Credit' is 'Good' and 'Age' is '30s' is classified as 'Yes', which is a Purchasing group.
       When the target variable is a finite number of groups, such as Purchasing group or Non-purchasing group 
       in the example above, the tree for classification is called a <b>decision tree</b>. In the case of 
       a continuous target variable, we can draw a similar decision tree based on a regression model, 
       which is called a regression tree (Breiman et al.). 
    <p>
       The decision tree model was first attempted by Sonquist and Morgan in 1964 and was widely used 
       by the general public in 1973 because of Morgan and Messenger's algorithm called THAID. In 1980,
       Kass introduced an algorithm called CHAID based on the chi-square goodness-of-fit test, 
       which is still widely used today. In 1982, computer scientist Quinlan introduced a decision tree algorithm 
       called ID3 and later developed it into an algorithm called C4.5. In 1984, Breiman et al. established 
       the theory of decision tree growth and pruning through CART. C4.5 and CART are nonparametric classification methods,
       but in 1988, Loh and Vanichetaku introduced FACT, a parametric approach.
       Recently, rather than classifying data as a single decision tree, an ensemble model that extracts 
       multiple samples using the bootstrap method and then integrates multiple decision tree classifications 
       based on these samples is widely used. The ensemble model is studied in Chapter 7.
  </div>
  <p>

  <!----------------------------------------------------------------------->
  <h4 id="060201">6.2.1 Decision tree algorithm</h4>
  <p>
  <div class="mainTable">
      The number of cases for making a decision tree is exponentially proportional to the number of variables
      and values ​​for each variable, so there are many cases. Some cases may have higher 
      classification accuracy than others, and some may have higher accuracy but take too much time. 
      Therefore, many people have studied to find an answer to the question, 
      ‘How can we find an algorithm that is accurate and has reasonable calculation time?’ 
      A rational algorithm partitions a data set by making locally optimal decisions at each node's decision point
      when deciding which variable to use. This method is applied sequentially to the partitioned data sets 
      to complete a decision tree to partition all data sets. In this section, we introduce an algorithm 
      as an inductive loop.
  </div>
  <p>
  <div class="mainTableGrey">
      [<b>Decision tree algorithm</b>]: Inductive loop
    <p>
      TreeGrowth (\(\small E, F\))<br>
      Step 1:	\(\;\;\)<b>if</b> stopping_condition(\(\small E, F\)) = true <b>then</b><br>
      Step 2:	\(\qquad\)leaf = creatNode().<br>
      Step 3:	\(\qquad\)leaf.label = Classify(\(\small E\))<br>
      Step 4:	\(\qquad\)return leaf<br>
      Step 5:	\(\;\;\)<b>else</b><br>
      Step 6:	\(\qquad\)root = creatNode()<br>
      Step 7:	\(\qquad\)root.test_condition = find_best_split(\(\small E, F\))<br>
      Step 8:	\(\qquad\)let \(\small V\) = {\(v: v\) is a possible outcome of root.test_condition}<br>
      Step 9:	\(\qquad\)<b>for</b> each \(\small v \in V\) <b>do</b><br>
      Step 10:	\(\qquad \quad\)\(\small E_v =\){ {root.test_condition(\(\small e\)) = } ∩ {\(\small e \in E\)} }<br>
      Step 11:	\(\qquad \quad\)child = TreeGrowth(\(\small E_v , F\))<br>
      Step 12:	\(\qquad \quad\)add child as descendent of root and label the edge(root → child) as \(v\)<br>
      Step 13:	\(\qquad\)<b>end for</b><br>
      Step 14:	\(\;\)<b>end if</b><br>
      Step 15:	\(\;\)<b>return</b> root<br>
  </div>
  <p>
  <div class="mainTableNoIndent">
       The algorithm's input is training data \(\small E\) and a variable set \(\small F\). 
       In step 7 of the algorithm, the optimal variable for splitting the data set is selected (find_best_split), 
       and in steps 11 and 12, the tree is expanded (TreeGrowth). This process is repeated until the stopping 
       condition of step 1 is satisfied. The entire algorithm process is explained using an example.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060202">6.2.2 Selection of a variable for branching</h4>
  <p>
  <div class="mainTable">
       In classification using decision trees, deciding which variable to select for each node 
       is crucial for better classification. Various measures have been studied to choose variables 
       for optimal branching. A common way to select one variable among several variables would be 
       to choose a variable that makes classification more accurate for each branch when the variable is selected 
       and branches out. Let's look at the following example.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.1</b>
       In a department store, 20 people who visited a particular store were surveyed, and 8 people (40%) were 
       in the Purchasing group (\(\small G_1\)) and 12 people (60%) were in the Non-purchasing group (\(\small G_2\)). 
       The Gender and Credit Status of these 20 people were analyzed and a crosstable was created,  
       as shown in Table 6.2.1 and Table 6.2.2. Let's find out which variable has better branching in a decision tree.
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="4">Table 6.2.1  Crosstable of Gender by Purchase and Non-purchasing group</th>
        </tr>
        <tr> 
          <th>Gender</th>
          <th>Purchasing group<br>\(G_1\)</th>
          <th>Non-purchasing group<br>\(G_2\)</th>
          <th>Total</th>
        </tr>
        <tr>
          <td class="tdCenter">Male</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr>
          <td class="tdCenter">Female</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr> 
          <th>Total</th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
      </table>
    <p>      
      <table style="width:600px"> 
        <tr>
          <th colspan="4">Table 6.2.2  Crosstable of Credit status by Purchase and Non-purchasing group</th>
        </tr>
        <tr> 
          <th>Credit Status</th>
          <th>Purchasing group<br>\(G_1\)</th>
          <th>Non-purchasing group<br>\(G_2\)</th>
          <th>Total</th>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter">7</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">9</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr> 
          <th>Total</th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       In the case of Gender, the ratios of the Purchasing group to the Non-purchasing group for Male and Female are 
       4 to 6 (40% to 60%), which is the same as the ratio of all 20 people.
       In the case of Credit Status, 90% of the customers are in the Non-purchasing group when the Credit Status is Bad,
       and 70% are in the Purchasing group when the Credit Status is Good, so there is a significant difference 
       in the Purchase or Non-purchase ratio between Bad and Good cases. In other words, if the Credit Status
       of a customer is Bad, there is a high possibility that he belongs to the Non-purchasing group, 
       and if it is Good, there is a high possibility that he belongs to the Purchasing group. 
       So, if the Credit Status is identified, we can distinguish the Purchasing group and the Non-purchasing group can be
       somewhat distinguished.
       Therefore, if one of the two variables must be selected as a variable for branching 
       in the decision tree, choosing the Credit Status variable is reasonable for a more accurate classification. 
       Generally, a variable that has a lot of information for classification is selected.
  </div>
  <p>
  <div class="mainTable">
       Many studies have been conducted on how to select a reasonable variable, such as the example above, using 
       statistical methods. Currently, the most commonly used methods for variable selection in decision trees 
       include the chi-square independence test, entropy coefficient, Gini coefficient, and classification error rate.
  </div>
  <p>
  <h5>A. Chi-square independence test</h5>
  <div class="mainTable">
       The chi-square independence test checks whether the distribution of each group for a variable is independent
       or not. In Example 6.2.1, the Gender variable and Purchase status variable can be tested for independence, and 
       the Credit Status variable and Purchase status variable can be tested for independence. 
       Suppose there are observed frequencies of a variable \(A\), which are summarized in Table 6.2.3. In that case, the expected frequencies 
       when the variable \(A\) and the Purchase status variable are independent are calculated in Table 6.2.4.
       The expected frequencies are calculated to make the ratios 
       (\( \frac{O_{\cdot 1}}{O_{\cdot \cdot}} , \frac{O_{\cdot 2}}{O_{\cdot \cdot}}\)) of 
       the Purchasing group and the Non-purchasing group remain the same in each variable value. 
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="4">Table 6.2.3  Observed frequencies of a variable \(A\) by Purchase status group</th>
        </tr>
        <tr> 
          <th style="width:100px">Variable \(A\)<br>value</th>
          <th style="width:250px">Purchasing group<br>\(G_1\)</th>
          <th style="width:250px">Non-purchasing group<br>\(G_2\)</th>
          <th style="width:100px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">\(A_1\)</td>
          <td class="tdCenter">\(O_{11}\)</td>
          <td class="tdCenter">\(O_{12}\)</td>
          <td class="tdCenter">\(O_{1 \cdot}\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(A_1\)</td>
          <td class="tdCenter">\(O_{21}\)</td>
          <td class="tdCenter">\(O_{22}\)</td>
          <td class="tdCenter">\(O_{2 \cdot}\)</td>
        </tr>
        <tr> 
          <th>Total</th>
          <td class="tdCenter">\(O_{\cdot 1}\)</td>
          <td class="tdCenter">\(O_{\cdot 2}\)</td>
          <td class="tdCenter">\(O_{\cdot \cdot}\)</td>
        </tr>
      </table>
    <p>      
      <table style="width:700px"> 
        <tr>
          <th colspan="4">Table 6.2.4  Expected frequencies of a variable by Purchase status when they are independent</th>
        </tr>
        <tr> 
          <th style="width:100px">Variable \(A\)<br>value</th>
          <th style="width:250px">Purchasing group<br>\(G_1\)</th>
          <th style="width:250px">Non-purchasing group<br>\(G_2\)</th>
          <th style="width:100px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">\(A_1\)</td>
          <td class="tdCenter">\(E_{11} = O_{1 \cdot} \times \frac{O_{\cdot 1}}{O_{\cdot \cdot}} \)</td>
          <td class="tdCenter">\(E_{12} = O_{1 \cdot} \times \frac{O_{\cdot 2}}{O_{\cdot \cdot}}\)</td>
          <td class="tdCenter">\(O_{1 \cdot}\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(A_1\)</td>
          <td class="tdCenter">\(E_{21} = O_{2 \cdot} \times \frac{O_{\cdot 1}}{O_{\cdot \cdot}}\)</td>
          <td class="tdCenter">\(E_{22} = O_{2 \cdot} \times \frac{O_{\cdot 2}}{O_{\cdot \cdot}}\)</td>
          <td class="tdCenter">\(O_{2 \cdot}\)</td>
        </tr>
        <tr> 
          <th>Total</th>
          <td class="tdCenter">\(O_{\cdot 1}\)</td>
          <td class="tdCenter">\(O_{\cdot 2}\)</td>
          <td class="tdCenter">\(O_{\cdot \cdot}\)</td>
        </tr>
      </table>
    <p>      
       The chi-square statistic of the observed frequencies in Table 6.2.3 for independence test is the sum of 
       the squares of the differences between the observed and expected frequencies in each cell divided by 
       the expected frequency.
       $$
          \chi^2 = \sum_{i=1}^{2} \sum_{j=1}^{2} \frac{ (O_{ij} - E_{ij})^2}{E_{ij}}
       $$
       This statistic follows the chi-square distribution with a degree of freedom of 1. 
       Suppose the distribution 
       of each group in each variable value is the same as the distribution of the entire group. In that case, 
       the chi-square statistic becomes 0, concluding that the variables and groups are independent. 
       Suppose the distribution of each group in each variable value is very different from the distribution of 
       the entire group. In that case, the chi-square statistic becomes very large, and the null hypothesis that the variables 
       and groups are independent is rejected. The stronger the degree of rejection, the better the variable is 
       for branching.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.2</b>
       Let's examine which variable is better for branching by using the chi-square independence test 
       for the Gender and Credit Status variable in Example 6.2.1.
    <p>
       <b>Answer</b>
    <p>
       In the crosstable of the Gender and Purchase status, the distribution of 
       (purchasing group, non-purchasing group) in the entire data is (40%, 60%). The distributions in each Male 
       and female are also the same at (40%, 60%), so the expected frequencies of Male and Female are (4, 6) 
       which are the same as observed frequencies and the chi-square statistic \(\chi_{Gender}^2\)is 0 as follows.
       $$ \small
          \chi_{Credit}^2 = \frac{ (4 - 4)^2}{4} + \frac{ (6 - 6)^2}{6} +\frac{ (4 - 4)^2}{4} +\frac{ (6 - 6)^2}{6} = 0
       $$
       Therefore, the Gender variable and Purchase status are independent.
    <p>
       In the crosstable of the Credit and Purchase status, the expected frequencies for each Credit status
       is (4, 6), so the chi-square statistic is as follows. 
       $$ \small
          \chi_{Credit}^2 = \frac{ (7 - 4)^2}{4} + \frac{ (3 - 6)^2}{6} +\frac{ (1 - 4)^2}{4} +\frac{ (9 - 6)^2}{6} = 7.5
       $$
       Therefore, since it is greater than the critical value of \(\small \chi_{1;\; 0.05}^2 \) = 3.841 at the significance level of 5% 
       in the chi-square distribution with the degree of freedom of 1, the Credit variable and Purchase status 
       are not independent. In other words, if the Credit status is known, it contains a lot of information 
       to decide the Purchasing group and the Non-purchasing group. Therefore, the branching is selected 
       by selecting the Credit variable rather than the Gender variable.
  </div>
  <p>
  <div class="mainTable">
       If a variable \(A\) has \(a\) number of values and a group variable has \(k\) number of groups, 
       the chi-square statistic of the \(a \times k\) crosstable is as follows;
       $$
          \chi^2 = \sum_{i=1}^{a} \sum_{j=1}^{k} \frac{ (O_{ij} - E_{ij})^2}{E_{ij}}  \;\;\text{where}\;\; E_{ij} = O_{i \cdot} \times \frac{O_{\cdot j}}{O_{\cdot \cdot}}
       $$
       This test statistic follows the chi-square distribution with \((a-1)(k-1)\) degree of freedom.
       Since the number of values of each variable can be different, the variable with the smaller \(p\)-value 
       of the chi-square test is used when selecting a variable to split in a decision tree.
  </div>
  <p>
  <h5>B. Entropy coefficient, Gini coefficient, and classification error rate</h5>
  <p>
  <div class="mainTable">
       The entropy coefficient, Gini coefficient, and classification error rate are similar concepts that measure 
       the uncertainty or purity of a distribution function. If there are \(k\) number of groups, 
       \(G_1 , G_2 , ... , G_k\), and \(p_1 , p_2 , ... , p_k\) are the probability distribution that 
       the data belongs to each group, each measure is defined as follows;
       $$
       \begin{align}
         \text{Entropy coefficient} &= - \sum_{i=1}^{k} p_i \times log_{2} p_i \;\; (\text{define}\;\; 0 \times log_{2} 0 = 0 )  \\
         \text{Gini coefficient} &= 1 - \sum_{i=1}^{k} p_{i}^2 \\
         \text{Classification error rate} &= 1 - max\{p_1 , p_2 , ..., p_k \}
       \end{align}
       $$
       To understand these three measures, let's look at the case where there are only two groups, that is \(k\) = 2. 
       If the probability of group 1 is \(p\), then the probability of group 2 is \(1-p\). In this case, 
       the three measures become as follows;
       $$
       \begin{align}
         \text{Entropy coefficient} &= - p \times log_{2} p - (1-p) \times log_{2} (1-p)   \\
         \text{Gini coefficient} &= 1 - p^2 - (1-p)^2 \\
         \text{Classification error rate} &= 1 - max\{p , 1-p \}
       \end{align}
       $$
       &lt;Figure 6.2.2&gt; shows graphs of three measures according to the value of \(p\). 
    <p>
       <img class="figure70" src="./Figure/Fig060202.png">
       <div class="figText">&lt;Figure 6.2.2&gt; Entropy, Gini coefficient, and classification error when there are two groups</div>
    <br>
       As we can see in the figure, all three measures have a maximum value when \(p\) = 0.5 (in this case, 
       \(1-p\) = 0.5 also, and it is a uniform distribution) and a minimum value of \(p\) = 0 when or \(p\) = 1. 
       That is, when the probability of two groups is the same (\(p\) = 0.5), uncertainty has a maximum value
       because we do not know which group to classify into. 
       On the other hand, if the probability of one group is 1, there is no uncertainty, that is, the certainty 
       of classification is 100%, so each measure has a minimum value of 0. Therefore, branching selects a variable 
       with less uncertainty.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.3</b>
       Find the entropy coefficient, Gini coefficient, and classification error rate for the distribution 
       of the Purchasing group and the Non-purchasing group (40%, 60%) in the entire data. Also, find the 
       entropy coefficient, Gini coefficient, and classification error rate for each Gender and Credit Status,
       and examine which variable is good for branching.
    <p>
       <b>Answer</b>
    <p>
       The entropy coefficient, Gini coefficient, and classification error rate for the probability distribution 
       (0.4, 0.6) of the Purchasing group and the Non-purchasing group are as in Table 6.2.5, and the measures 
       for the Gender are in Table 6.2.6, and the measures for the Credit Status are in Table 6.2.7.
    <p>
      <table style="width:750px"> 
        <tr>
          <th colspan="5">Table 6.2.5  Uncetainty measures for the distribution of (Purchase, Non-purchase) of entire data</th>
        </tr>
        <tr> 
          <th style="width:50px"></th>
          <th style="width:100px">Purchasing group<br>\(G_1\)</th>
          <th style="width:100px">Non-purchasing group<br>\(G_2\)</th>
          <th style="width:100px">Total</th>
          <th style="width:400px">Uncetainty measures</th>
        </tr>
        <tr>
          <td class="tdCenter"><br>Entire data</td>
          <td class="tdCenter"><br>6</td>
          <td class="tdCenter"><br>12</td>
          <td class="tdCenter"><br>20</td>
          <td class="tdCenter">
            $$ \small
            \begin{align}
              \text{Entropy coefficient} &= - 0.4 \times log_{2} 0.4 - (1-0.4) \times log_{2} (1-0.4) = 0.9710   \\
              \text{Gini coefficient} &= 1 - {0.4}^2 - (1-0.4)^2 = 0.4800\\
              \text{Classification error rate} &= 1 - max\{0.4 , 1-0.4 \} = 0.4000
            \end{align}
            $$
          </td>
        </tr>
      </table>
    <p>      
      <table style="width:750px"> 
        <tr>
          <th colspan="5">Table 6.2.6  Uncetainty measures for the distribution of (Purchase, Non-purchase) of Gender</th>
        </tr>
        <tr> 
          <th style="width:50px">Gender</th>
          <th style="width:100px">Purchasing group<br>\(G_1\)</th>
          <th style="width:100px">Non-purchasing group<br>\(G_2\)</th>
          <th style="width:100px">Total</th>
          <th style="width:400px">Uncetainty measures</th>
        </tr>
        <tr>
          <td class="tdCenter"><br>Male</td>
          <td class="tdCenter"><br>4</td>
          <td class="tdCenter"><br>6</td>
          <td class="tdCenter"><br>10</td>
          <td class="tdCenter">
            $$ \small
            \begin{align}
              \text{Entropy coefficient} &= - 0.4 \times log_{2} 0.4 - (1-0.4) \times log_{2} (1-0.4) = 0.9710   \\
              \text{Gini coefficient} &= 1 - {0.4}^2 - (1-0.4)^2 = 0.4800\\
              \text{Classification error rate} &= 1 - max\{0.4 , 1-0.4 \} = 0.4000
            \end{align}
            $$
          </td>
        </tr>
        <tr>
          <td class="tdCenter"><br>Female</td>
          <td class="tdCenter"><br>4</td>
          <td class="tdCenter"><br>6</td>
          <td class="tdCenter"><br>10</td>
          <td class="tdCenter">
            $$ \small
            \begin{align}
              \text{Entropy coefficient} &= - 0.4 \times log_{2} 0.4 - (1-0.4) \times log_{2} (1-0.4) = 0.9710   \\
              \text{Gini coefficient} &= 1 - {0.4}^2 - (1-0.4)^2 = 0.4800\\
              \text{Classification error rate} &= 1 - max\{0.4 , 1-0.4 \} = 0.4000
            \end{align}
            $$
          </td>
        </tr>
      </table>
    <p>
      <table style="width:750px"> 
        <tr>
          <th colspan="5">Table 6.2.7  Uncetainty measures for the distribution of (Purchase, Non-purchase) of Credit Status data</th>
        </tr>
        <tr> 
          <th style="width:50px">Credit Status</th>
          <th style="width:100px">Purchasing group<br>\(G_1\)</th>
          <th style="width:100px">Non-purchasing group<br>\(G_2\)</th>
          <th style="width:100px">Total</th>
          <th style="width:400px">Uncetainty measures</th>
        </tr>
        <tr>
          <td class="tdCenter"><br>Good</td>
          <td class="tdCenter"><br>7</td>
          <td class="tdCenter"><br>3</td>
          <td class="tdCenter"><br>10</td>
          <td class="tdCenter">
            $$
            \begin{align}
              \text{Entropy coefficient} &= - 0.7 \times log_{2} 0.7 - (1-0.7) \times log_{2} (1-0.7) = 0.8813   \\
              \text{Gini coefficient} &= 1 - {0.7}^2 - (1-0.7)^2 = 0.4200\\
              \text{Classification error rate} &= 1 - max\{0.7 , 1-0.7 \} = 0.3000
            \end{align}
            $$
          </td>
        </tr>
        <tr>
          <td class="tdCenter"><br>Bad</td>
          <td class="tdCenter"><br>1</td>
          <td class="tdCenter"><br>9</td>
          <td class="tdCenter"><br>10</td>
          <td class="tdCenter">
            $$
            \begin{align}
              \text{Entropy coefficient} &= - 0.1 \times log_{2} 0.1 - (1-0.1) \times log_{2} (1-0.1) = 0.4690   \\
              \text{Gini coefficient} &= 1 - {0.1}^2 - (1-0.1)^2 = 0.1800\\
              \text{Classification error rate} &= 1 - max\{0.1 , 1-0.1 \} = 0.1000
            \end{align}
            $$
          </td>
        </tr>
      </table>
    <p>      
       When looking at the uncertainty for the two variables, each attribute of Credit Status is relatively less than
       the Gender attribute, so branching using Credit Status is reasonable.
  </div>
  <p>
  <div class="mainTable">
       Suppose there are \(k\) groups as \(G_1 , G_2 , ... , G_k \) and there are \(a\) number of attributes​​ in 
       variable \(A\) as \(A_1 , A_2 , ... , A_a \). Let \(O_{ij}\) be the observed frequency of the 
       attribute \(A_i \) and the group \(G_j\), \(O_{i \cdot}\) be the sum of the observed frequencies for 
       the attribute \(A_i\), \(O_{\cdot j}\) be the sum of the observed frequencies for the group \(G_j\),
       \(O_{\cdot \cdot}\) be the total number of data, and \(I(A_{i})\) be the uncertainty of the attribute \(A_i\)
       as summarized in Table 6.2.8. 
    <p>
      <table style="width:750px"> 
        <tr>
          <th colspan="7">Table 6.2.8  \(a \times k\) frequency table and uncetainty measure of the variable \(A\)</th>
        </tr>
        <tr> 
          <th style="width:150px">Variable \(A\)</th>
          <th style="width:100px">Group \(G_1\)</th>
          <th style="width:100px">Group \(G_2\)</th>
          <th style="width:50px">\(\cdots\)</th>
          <th style="width:100px">Group \(G_k\)</th>
          <th style="width:100px">Total</th>
          <th style="width:150px">Uncetainty</th>
        </tr>
        <tr>
          <td class="tdCenter">\(A_1\)</td>
          <td class="tdCenter">\(O_{11}\)</td>
          <td class="tdCenter">\(O_{12}\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(O_{1k}\)</td>
          <td class="tdCenter">\(O_{1 \cdot} \)</td>
          <td class="tdCenter">\(I(A_1)\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(A_2\)</td>
          <td class="tdCenter">\(O_{21}\)</td>
          <td class="tdCenter">\(O_{22}\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(O_{2k}\)</td>
          <td class="tdCenter">\(O_{2 \cdot }\)</td>
          <td class="tdCenter">\(I(A_2)\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(\cdots\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(A_a\)</td>
          <td class="tdCenter">\(O_{a1}\)</td>
          <td class="tdCenter">\(O_{a2}\)</td>
          <td class="tdCenter">\(\cdots\)</td>
          <td class="tdCenter">\(O_{ak}\)</td>
          <td class="tdCenter">\(O_{a \cdot} \)</td>
          <td class="tdCenter">\(I(A_a)\)</td>
        </tr>
        <tr>
          <th class="tdCenter">Total</td>
          <th class="tdCenter">\(O_{\cdot 1}\)</td>
          <th class="tdCenter">\(O_{\cdot 2}\)</td>
          <th class="tdCenter">\(\cdots\)</td>
          <th class="tdCenter">\(O_{\cdot k}\)</td>
          <th class="tdCenter">\(O_{\cdot \cdot }\)</td>
          <th class="tdCenter">Uncertainty of \(A\)<br>\(I(A\))</td>
        </tr>
      </table>
    <br>      
       The uncertainty of the variable \(A\) is the expected value of each attribute \(A_{i}\) by weighting 
       the proportion of the observed frequency, \(\frac{O_{i \cdot}}{O_{\cdot \cdot}}\), as follows;
       $$
         I(A) = \frac{O_{1 \cdot}}{O_{\cdot \cdot}} \times I(A_{1}) + \frac{O_{2 \cdot}}{O_{\cdot \cdot}} \times I(A_{2}) + \cdots + \frac{O_{a \cdot}}{O_{\cdot \cdot}} \times I(A_{a})
       $$
       If we do not know the information of the variable \(A\), the uncertainty of this node \(T\), \(I(T)\), is
       the uncertainty about the distribution of each group proportion. In the case of the entropy coefficient,
       it is as follows;
       $$
         I(T) = - \sum_{j=1}^{k} \left( \frac{O_{\cdot j}}{O_{\cdot \cdot}} \right) \;\; log_{2} \left( \frac{O_{\cdot j}}{O_{\cdot \cdot}} \right)
       $$
       In the decision tree, a variable for branching is selected if it has a large difference between
       the uncertainty of the current node, \(I(T)\), and the expected uncertainty of a variable \(A\), \(I(A)\).
       This difference is called an <b>information gain</b> and is expressed as \(\Delta\). The information gain
       at the current node \(T\) by branching into the variable \(A\) is as follows;
       $$
         \Delta = I(T) - I(A)
       $$
       The greater information gain of one variable implies that by branching into this variable, more uncertainty
       is removed, and, therefore, more accurate classification is expected. The information gain obtained using 
       the entropy coefficient or Gini coefficient tends to prefer a variable with many variable values. 
       In order to overcome this problem, the <b>information gain ratio</b>, which is the information gain 
       divided by the uncertainty of the current node \(T\) is often used as the basis for branching.
       $$
         \text{Information gain ratio} = \frac{\Delta}{I(T)}
       $$
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.4</b>
       In Example 6.2.3, find the information gain for each measure of the Gender and Credit Status variables.
    <p>
       <b>Answer</b>
    <p>
       Using the uncertainty values ​​for each measure calculated in Example 6.2.3, the information gain 
       for each measure of the Gender variable is as follows;
       $$ \small
       \begin{align}
         \text{Information gain by entropy} &= 0.9710 - \left( \frac{10}{20} \times 0.9710 +  \frac{10}{20} \times 0.9710 \right) = 0.0000 \\
         \text{Information gain by Gini} &= 0.4800 - \left( \frac{10}{20} \times 0.4800 +  \frac{10}{20} \times 0.4800 \right) = 0.0000 \\
         \text{Information gain by misclassification error} &= 0.4000 - \left( \frac{10}{20} \times 0.4000 + \frac{10}{20} \times 0.4000 \right) = 0.0000 \\
       \end{align}
       $$
       That is, since the Gender variable has the same uncertainty as the current node, there is no information gain
       for classification that can be obtained by branching. The information gain for the Credit Status
       variable is as follows;
       $$ \small
       \begin{align}
         \text{Information gain by entropy} &= 0.9710 - \left( \frac{10}{20} \times 0.8813 +  \frac{10}{20} \times 0.4690 \right) = 0.2958 \\
         \text{Information gain by Gini} &= 0.4800 - \left( \frac{10}{20} \times 0.4200 +  \frac{10}{20} \times 0.1800 \right) = 0.1800 \\
         \text{Information gain by misclassification error} &= 0.4000 - \left( \frac{10}{20} \times 0.3000 + \frac{10}{20} \times 0.1000 \right) = 0.2000 \\
       \end{align}
       $$
       Therefore, regardless of which measure is used, the Credit Status variable has more information gain than Gender, 
       so it can be said that the Credit Status variable is better for branching at the current node.
  </div>
  <p>
  <div class="mainTable">
       There are many comparative studies on the question, ‘Which of the three uncertainty measures is better?’ 
       The conclusion is that since all three measures measure uncertainty similarly, it is not possible to say 
       ‘which measure is better.’
    <p>
       Let's take a closer look at the algorithm for creating a decision tree using the following example.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.5</b>
       When we surveyed 20 customers who visited a computer store, 8 customers purchased a computer (Purchasing group, \(\small G_1\)) and
       12 customers did not purchase a computer (Non-purchasing group, \(\small G_2 \). The survey included variables such as gender, age, 
       monthly income, and credit status of these 20 customers as well as Purchase status as shown in Table 6.2.9. 
       Note that all variables are surveyed as categorical variables such as (Female, Male) for gender, (20s, 30s) for age,
       (GE2000, LT2000) for income, (Bad, Fair, Good) for credit status, and (No, Yes) for Purchase status.
    <p>
       <div class="textL20M20">
         1) Find a classification model using the decision tree. Use the entropy coefficient for variable selection, and if the number of data in each leaf is 
         5 or less, no further branching is performed, and a decision is made by majority vote. If the data in each leaf are classified 
         into one group, no further branching is performed. 
       </div>
       <div class="textL20M20">
         2) Using this decision tree model, classify a customer who is a 33-year-old male with a monthly income of 2,200 (unit 10,000 won)
         and good credit status, whether he will purchase a computer or not. 
       </div>
    <p>
      <table style="width:740px"> 
        <tr>
          <th colspan="6">Table 6.2.9  Survey of customers on gender, age, income, credit status and purchase status</th>
        </tr>
        <tr> 
          <th style="width:120px">Number</th>
          <th style="width:120px">Gender</th>
          <th style="width:120px">Age</th>
          <th style="width:140px">Income<br>(unit 10,000 won)</th>
          <th style="width:120px">Credit</th>
          <th style="width:120px">Purchase</th>
        </tr>
        <tr><td class="tdCenter">1</td>  <td class="tdCenter">Male  </td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">2</td>  <td class="tdCenter">Female</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">3</td>  <td class="tdCenter">Female</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">4</td>  <td class="tdCenter">Female</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">5</td>  <td class="tdCenter">Female</td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">6</td>  <td class="tdCenter">Female</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">7</td>  <td class="tdCenter">Female</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">8</td>  <td class="tdCenter">Male  </td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">9</td>  <td class="tdCenter">Female</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">10</td> <td class="tdCenter">Male  </td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">11</td> <td class="tdCenter">Female</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">12</td> <td class="tdCenter">Female</td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">13</td> <td class="tdCenter">Male  </td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">14</td> <td class="tdCenter">Male  </td> <td class="tdCenter">30s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">15</td> <td class="tdCenter">Female</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">16</td> <td class="tdCenter">Female</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">17</td> <td class="tdCenter">Female</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">18</td> <td class="tdCenter">Male  </td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">19</td> <td class="tdCenter">Male  </td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">20</td> <td class="tdCenter">Male  </td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       In the decision tree algorithm, the data set \(\small E\) is the data in Table 6.2.9, and the variable set is 
       \(\small F\) = {Gender, Age, Income, Credit}. The target variable, Purchase, has two groups, {Yes, No}. 
       The stopping rule of the decision tree is ‘If the number of data in each leaf is 5 or less, do not divide any more’, 
       and ‘If all are classified into one group, stop with the leaf as the group’.
    <p>
       The number of data in the current data set is 20, so stopping.conditon(\(\small E, F\)) in step 1 is false, 
       so go to step 6. For the root node \(\small T\)'s creatNode(), the entropy coefficient \(\small I(T) \) for 
       the distribution of the purchasing group(\(\small G_1\)) and the non-purchasing group (\(\small G_2\)), (8/20, 12/20), is as follows;
       $$ \small
         I(T) = - 0.4 \times log_{2} 0.4 - 0.6 \times log_{2} 0.6  = 0.9710
       $$
       In order to find the optimal branch split, find_best_split(), of step 7, a cross-table is obtained for each variable
       by group, and the expected information and information gain for the variable are obtained as in Table 6.2.10
       using the entropy coefficient. Since the information gain of the credit status is the largest, the root node 
       becomes the credit status, and the set of variable values ​​of credit status in step 8 becomes 
       \(\small V\) = {Bad, Fair, Good}. The \(\small E_{Bad}, E_{Fair}, E_{Good}\) according to the credit status in step 10 
       are drawn in the form of a decision tree as in &lt;Figure 6.2.3&gt;.
    <p>
      <table style="width:780px; border: 2px solid black;"> 
        <tr>
          <th colspan="7">Table 6.2.10  Expected information and information gain for each variable </th>
        </tr>
        <tr> 
          <th colspan="2" style="width:160px; border: 2px solid black;">Variable</th>
          <th style="width:120px; border: 2px solid black;">Purchasing group \(G_1\)</th>
          <th style="width:120px; border: 2px solid black;">Non-purchasing group \(G_2\)</th>
          <th style="width:140px; border: 2px solid black;">Total</th>
          <th style="width:100px; border: 2px solid black;">Entropy</th>
          <th style="width:140px; border: 2px solid black;">Information gain<br>\(\Delta\)</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Gender</td>
          <td class="tdCenter">Female</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">12</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <td class="tdCenter">Male</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">1.0000</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9510</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0200</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Age</td>
          <td class="tdCenter">20s</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.7219</td>
        </tr>
        <tr>
          <td class="tdCenter">30s</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.9710</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.8464</th>
          <th class="tdCenter" style="border: 2px solid black;">0.1246</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Income</td>
          <td class="tdCenter">GE200</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">14</td>
          <td class="tdCenter">0.9852</td>
        </tr>
        <tr>
          <td class="tdCenter">LT200</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">0.9783</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9651</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0059</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="3">Credit</td>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">0.0000</td>
        </tr>
        <tr>
          <td class="tdCenter">Fair</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">7</td>
          <td class="tdCenter">11</td>
          <td class="tdCenter">0.9457</td>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9756</th>
          <th class="tdCenter" style="border: 2px solid black;">0.1754</th>
        </tr>
      </table>
    <p>
       <img class="figure" src="./Figure/Fig060203.png">
       <div class="figText">&lt;Figure 6.2.3&gt; Decision tree with branching according to credit status</div>
    <p>
       The TreeGrowth() algorithm is repeatedly applied to each data set until the stopping rule is satisfied (step 11). 
       Among these, the data sets of 3 people with Bad credit, \(\small E_{Bad} \), are all Non-purchasing groups, satisfying the stopping rule, 
       so they are not branching any further, and the leaves are marked as the Non-purchasing group.
    <p>
       Since the stopping rule is not satisfied for the data set of 11 people with Fair credit, \(\small E_{Fair} \), 
       this data set needs further split.  
       The entropy coefficients for the distribution (4/11, 7/11) of the Purchasing group (\(\small G_1 \)) and 
       the Non-purchasing group (\(\small G_2 \)) are as follows.
       $$ \small
         I(E_{Fair}) = - \frac{4}{11} \times log_{2} \frac{4}{11} - \frac{7}{11} \times log_{2} \frac{7}{11} = 0.9457
       $$
       In order to find the optimal split, find_best_split(), for the 11 people, a cross-table for each variable by the group 
       is obtained, and the expected information and information gain for the variables are obtained using the entropy coefficient, 
       as shown in Table 6.2.11. In the case of the data set with Fair credit, the information gain for Gender is the largest, 
       so it becomes a node for branching, and a decision tree such as &lt;Figure 6.2.4&gt; is formed.
    <p>
      <table style="width:780px; border: 2px solid black;"> 
        <tr>
          <th colspan="7">Table 6.2.11  Expected information and information gain for each variable in \(\small E_{Fair} \)</th>
        </tr>
        <tr> 
          <th colspan="2" style="width:160px; border: 2px solid black;">Variable</th>
          <th style="width:120px; border: 2px solid black;">Purchasing group \(G_1\)</th>
          <th style="width:120px; border: 2px solid black;">Non-purchasing group \(G_2\)</th>
          <th style="width:140px; border: 2px solid black;">Total</th>
          <th style="width:100px; border: 2px solid black;">Entropy</th>
          <th style="width:140px; border: 2px solid black;">Information gain<br>\(\Delta\)</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Gender</td>
          <td class="tdCenter">Female</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">0.7219</td>
        </tr>
        <tr>
          <td class="tdCenter">Male</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">1.0000</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.8736</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0721</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Age</td>
          <td class="tdCenter">20s</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <td class="tdCenter">30s</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">0.9710</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9422</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0034</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Income</td>
          <td class="tdCenter">GE200</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <td class="tdCenter">LT200</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">0.9710</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9422</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0034</th>
        </tr>
      </table>
    <p>
       <img class="figure90" src="./Figure/Fig060204.png">
       <div class="figText">&lt;Figure 6.2.4&gt; Decision tree with branching according to Gender in \(\small E_{Fair} \)</div>
    <p>
       As shown in the Figure 6.2.4, there are 5 data sets of Female with Fair credit, \(\small E_{Female} \), which satisfies 
       the stopping rule, so they are not split any further. Since there four people who did not purchase the computer and 
       only one person purchased, this node are marked as Non-purchasing group by majority vote. 
    <p> 
       As shown in Figure 6.2.4, there are 6 data sets of Male with Fair credit, \(\small E_{Male} \),  
       the stopping rule is not satisfied and this data set needs further split. 
       The entropy coefficients for the distribution (3/6, 3/6) of the Purchasing group (\(\small G_1 \)) and 
       the Non-purchasing group (\(\small G_2 \)) are as follows.
       $$ \small
         I(E_{Male}) = - \frac{3}{6} \times log_{2} \frac{3}{6} - \frac{3}{6} \times log_{2} \frac{3}{6} = 1
       $$
       In order to find the optimal branch split, find_best_split(), for the 6 people, a cross-table by the group for each variable
       is obtained, and the expected information and information gain for the variables are obtained using the entropy coefficient, 
       as shown in Table 6.2.12. In the case of the Male with Fair credit, the information gain for Age is the largest, 
       so it becomes a node for branching, and a decision tree such as &lt;Figure 6.2.5&gt; is formed.
       Here, since there are 3 people in their 20s and 30s, there is no more branching, and the 20s becomes the Non-purchasing group, 
       and the 30s becomes the Purchasing group by majority vote.
    <p>
      <table style="width:780px; border: 2px solid black;"> 
        <tr>
          <th colspan="7">Table 6.2.12  Expected information and information gain for each variable in \(\small E_{Male} \)</th>
        </tr>
        <tr> 
          <th style="width:160px; border: 2px solid black;" colspan="2">Variable</th>
          <th style="width:120px; border: 2px solid black;">Purchasing group \(G_1\)</th>
          <th style="width:120px; border: 2px solid black;">Non-purchasing group \(G_2\)</th>
          <th style="width:140px; border: 2px solid black;">Total</th>
          <th style="width:100px; border: 2px solid black;">Entropy</th>
          <th style="width:140px;border: 2px solid black;">Information gain<br>\(\Delta\)</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="1">Age</td>
          <td class="tdCenter">20s</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <td class="tdCenter">30s</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9183</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0817</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Income</td>
          <td class="tdCenter">GE200</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">1.0000</td>
        </tr>
        <tr>
          <td class="tdCenter">LT200</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">1.0000</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">1.0000</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0000</th>
        </tr>
      </table>
    <p>
       <img class="figure90" src="./Figure/Fig060205.png">
       <div class="figText">&lt;Figure 6.2.5&gt; Decision tree with branching according to Gender in \(\small E_{Male} \)</div>
    <p>
       At the root node with Credit, since the stopping rule is not satisfied for the data set of 6 people with Good credit, \(\small E_{Good} \), 
       the entropy coefficients for the distribution (4/6, 2/6) of the Purchasing group (\(\small G_1 \)) and 
       the Non-purchasing group (\(\small G_2 \)) are as follows.
       $$ \small
         I(E_{Good}) = - \frac{4}{6} \times log_{2} \frac{4}{6} - \frac{2}{6} \times log_{2} \frac{2}{6} = 0.9183
       $$
       In order to find the optimal branch split, find_best_split(), for the 6 people, a cross-table by the group for each variable
       is obtained, and the expected information and information gain for the variables are obtained using the entropy coefficient 
       as shown in Table 6.2.13. Since the information gain of Age is the largest in the data set of Good credit, 
       it becomes a node for branching and forms a decision tree as in &lt;Figure 6.2.6&gt;. As shown in the figure, 
       among the people with Good credit, there is only one person in Age 20s who did not purchase a computer, 
       so it becomes the Non-purchasing group by the stopping rule. There are 5 people in Age 30s, and 4 of them 
       purchased a computer, so they become the Purchasing group by majority vote. 
    <p>
      <table style="width:780px; border: 2px solid black;"> 
        <tr>
          <th colspan="7">Table 6.2.13  Expected information and information gain for each variable in \(\small E_{Good} \)</th>
        </tr>
        <tr> 
          <th style="width:160px; border: 2px solid black;" colspan="2">Variable</th>
          <th style="width:120px; border: 2px solid black;">Purchasing group \(G_1\)</th>
          <th style="width:120px; border: 2px solid black;">Non-purchasing group \(G_2\)</th>
          <th style="width:140px; border: 2px solid black;">Total</th>
          <th style="width:100px; border: 2px solid black;">Entropy</th>
          <th style="width:140px; border: 2px solid black;">Information gain<br>\(\Delta\)</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Gender</td>
          <td class="tdCenter">Female</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">0.9710</td>
        </tr>
        <tr>
          <td class="tdCenter">Male</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">0.0000</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.8091</th>
          <th class="tdCenter" style="border: 2px solid black;">0.1092</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Age</td>
          <td class="tdCenter">20s</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">0.0000</td>
        </tr>
        <tr>
          <td class="tdCenter">30s</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">0.7219</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.6016</th>
          <th class="tdCenter" style="border: 2px solid black;">0.3167</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Income</td>
          <td class="tdCenter">GE200</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">0.9183</td>
        </tr>
        <tr>
          <td class="tdCenter">LT200</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">0.0000</td>
        </tr>
        <tr>
          <th class="tdRight" colspan="5" style="border: 2px solid black;">Expected entropy</th>
          <th class="tdCenter" style="border: 2px solid black;">0.9183</th>
          <th class="tdCenter" style="border: 2px solid black;">0.0000</th>
        </tr>
      </table>
    <p>
       <img class="figure90" src="./Figure/Fig060206.png">
       <div class="figText">&lt;Figure 6.2.6&gt; Decision tree with branching according to Age in \(\small E_{Good} \)</div>
    <p>
       If we combine the above, the decision tree in Figure 6.2.7 is completed. Therefore, the customer
       who is a 33-year-old man, has a monthly income of 220, and has a good credit is classified as a Purchasing group.
    <p>
       <img class="figure90" src="./Figure/Fig060207.png">
       <div class="figText">&lt;Figure 6.2.7&gt; Decision tree to decide a customer will purchase a computer or not</div>
    <p>
       In 『eStatU』 menu, select [Decision Tree] to see the window as follows;  
       Check the criteria for variable selection which is 'Entropy' in this example, enter the maximum tree depth 5 and 
       the minimum number of data 5 for a decision. You can divide the original data set into 'Train' and 'Test'
       by assigning the percents. Click [Execute] button to see the bar graph matrix and decision tree. 
       Click [Classification Stat] to see the decision rules of this decision tree and the accuracy/misclassification of 
       the classification as in &lt;Figure 6.2.8&gt;.
       Click [Classification Table] to see the original data and classification result as in &lt;Figure 6.2.9&gt;.
    <p>
       <!---   ************ html for decision tree ************  ---->
       <b>[<span data-msgid="DecisionTree">Decision Tree</span>]</b>
    <p>
       <iframe class="example110" src="./example/060201.html"> </iframe>
    <p>
       <img class="figure70" src="./Figure/Fig060208.png">
       <div class="figText">&lt;Figure 6.2.8&gt; Decision rules and classifcation acuracy</div>
    <p>
       <img class="figure70" src="./Figure/Fig060209.png">
       <div class="figText">&lt;Figure 6.2.9&gt; Original data and classification</div>
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060203">6.2.3  Categorization of a continuous variable</h4>
  <p>
  <div class="mainTable">
       We can convert a continuous variable to a categorical variable, and a decision tree model can be applied. 
       For example, the monthly income variable can be divided into two groups: ‘Greater than or equal 2000’ and 
       ‘Less than 2000’. In this case, the question arises ‘What boundary value should be used to divide 
       a value of the continuous variable?’ An expert related to this research can decide this boundary value.
       However, if the determination of the boundary value is for more accurate classification, then 
       the uncertainty measures studied in the previous section can be used to determine the boundary value 
       which increases the accuracy of the classification.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.6</b>
       In a store, a survey of 10 customers was conducted on their monthly income (unit 10,000 won) and whether they purchased 
       a certain product or not, and the results are shown in Table 6.2.14. The incomes are arranged in ascending order
       and the purchase status is denoted as 'Y' if a customer purchases, and 'N' if he did not purchase. 
       In order to apply the decision tree model, we want to divide the monthly income into two categories. 
       What boundary value of the income is reasonable to divide for classification?
    <p>
      <table style="width:700px"> 
        <tr>
          <th colspan="11">Table 6.2.14  Survey of customers on income and purchase status</th>
        </tr>
        <tr><td class="tdCenter">Purchase</td>  <td class="tdCenter">N</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td>  </tr>
        <tr><td class="tdCenter">Income</td>    <td class="tdCenter">100</td> <td class="tdCenter">120</td> <td class="tdCenter">160</td> <td class="tdCenter">180</td> <td class="tdCenter">186</td> <td class="tdCenter">190</td> <td class="tdCenter">210</td> <td class="tdCenter">250</td> <td class="tdCenter">270</td> <td class="tdCenter">300</td></tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       It is reasonable to examine all middle values of two adjacent incomes as the boundary value, 
       and check how their classification result is made. Then select the boundary value with the least ‘uncertainty’ among them. 
       When a boundary value is examined, if incomes on left side of the boundary value are classified as 'N' group and incomes
       on the right-side of the boundary value are classified as 'Y' group,  a crosstable for classification is summarized 
       as in Table 6.2.15. A boundary value which is smaller than the minimum income or larger than the maximum income
       is excluded because its division is meaningless. For the first middle value 110 between income 100 and 120 in Table 6.2.11,
       we classify data using the rule as follows;
       $$ \small
         \text{If the income ≤ 110, then classify data as 'N' group, else classify data as 'Y' group}
       $$
       The data 100 is classified correctly using this rule as 'N' group, and the remaining nine data are classified 'Y' group, 
       therefore, three (180, 186, 190) out of nine data are classified correctly as 'Y' group, and six (120, 160, 210, 250, 270, 300)
       out of nine data are classified incorrectly as 'Y' group as the following crosstable;
    <p>
       Using the Gini coefficient as the uncertainty measure, the expected Gini coefficient when the middle value 
       is 110 calculated as follows; 
       $$ \small
         \frac{1}{10} \times \left\{ 1 - ( \frac{1}{1} )^2 - ( \frac{0}{1} )^2 \right\} \;+\; \frac{9}{10} \times \left\{ 1 - ( \frac{6}{9} )^2 - ( \frac{3}{9} )^2 \right\} = 0.4000
       $$
       The expected Gini coefficients for all remaining middle values can be calculated in the same way as Table 6.2.15.
       The boundary value with the least uncertainty is 200.
    <p>
      <table style="width:600px; border: 2px solid black;"> 
        <tr>
          <th colspan="6">Table 6.2.15 Expected Gini coefficient using the middle value of two adjacent incomes</th>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        </tr>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 110</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">1</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">9</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.400</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 135</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">2</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">8</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.375</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 170</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">3</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">7</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.343</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 183</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">4</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">6</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.417</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 188</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">5</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.400</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 200</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">6</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">4</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter" style="background-color:yellow;">0.300</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 230</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">7</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">3</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.343</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 260</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">8</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">2</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.375</td>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Actual group</th>
          <th></th>
        <tr> 
          <th style="border: 2px solid black;">Middle value = 285</th>
          <th style="border: 2px solid black;"></th>
          <th style="border: 2px solid black;">\(N\)</th>
          <th style="border: 2px solid black;">\(Y\)</th>
          <th style="border: 2px solid black;">Total</th>
          <th style="border: 2px solid black;">Expected Gini coefficient</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Classified group</td>
          <td class="tdCenter">\(N\)</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">9</td>
        </tr>
        <tr>
          <td class="tdCenter">\(Y\)</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">1</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">10</td>
          <td class="tdCenter">0.400</td>
        </tr>
      </table>
  </div>
  <p>
  <div class="mainTable">
       The method of setting the boundary value of the continuous variable, as the above example, requires many calculations. 
       If there are several candidates for the threshold, we can decide which of them is better in a similar way.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.2.7</b>
       In a department store, when 20 customers were surveyed, 7 (35%) made purchases and 13 (65%) did not make purchases. 
       A cross-tabulation was created to compare the methods of dividing the ages of these 20 people into those 
       under 25 and over 25, and those under 35 and over 35, as shown in Table 6.2.16. Using the entropy information gain,
       decide which interval is better.
    <p>
      <table style="width:600px; border: 2px solid black;"> 
        <tr>
          <th colspan="4">Table 6.2.16 crosstable of two interval divisions by purchase status</th>
        </tr>
        <tr> 
          <th style="border: 2px solid black;"></th>
          <th colspan="2" style="border: 2px solid black;">Purchase status</th>
          <th style="border: 2px solid black;"></th>
        </tr>
        <tr> 
          <th style="border: 2px solid black;">Age interval 1</th>
          <th style="border: 2px solid black;">Purchasing group</th>
          <th style="border: 2px solid black;">Non-purchasing group</th>
          <th style="border: 2px solid black;">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">< 25</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">6</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\ge\) 25</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">14</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>7</th>
          <th>13</th>
          <th>20</th>
        </tr>
        <tr> 
          <th style="border: 2px solid black;">Age interval 2</th>
          <th style="border: 2px solid black;">Purchasing group</th>
          <th style="border: 2px solid black;">Non-purchasing group</th>
          <th style="border: 2px solid black;">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">< 35</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">12</td>
          <td class="tdCenter">15</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\ge\) 35</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>7</th>
          <th>13</th>
          <th>20</th>
        </tr>
       </table>
    <p>
       <b>Answer</b>
    <p>
       Let us compare the two crosstables to see which interval division is better. 
       (Age interval 1) has many Non-purchasing groups in both '< 25' and '\(\ge 25\) intervals. 
       (Age unterval 2) has 12 customers in Non-purchasing group out of 15 customers who are '< 35' which is a high proportion,
       and 4 customers in Purchasing group out of 5 customers who are '\(\ge 35\) which is also a high proportion. 
       Therefore, among the two interval division methods, (Age interval 2) provides more information on purchase. 
       Let us confirm this using the entropy measure.
    <p>
       The expected entropy for the total distribution (\( \frac{7}{20} , \frac{13}{20}\)) of Purchasing group and 
       Non-purchasing group is calculated as follows.
       $$ \small
         - (\frac{7}{20}) \; log_{2} ( \frac{7}{20} ) \; - \;(\frac{13}{20} ) \; log_{2} (\frac{13}{20} ) \;=\; 0.9341
       $$
       The expected entropy and information gain for the two interval divisions are calculated as in Table 6.2.17.
    <p>
      <table style="width:780px; border: 2px solid black;"> 
        <tr>
          <th colspan="6">Table 6.2.17 Expected entropy and information gain of two interval divisions</th>
        </tr>
        <tr> 
          <th></th>
          <th colspan="2" style="border: 2px solid black;">Purchase status</th>
          <th></th>
        </tr>
        <tr> 
          <th style="width=100px; border: 2px solid black;">Age interval 1</th>
          <th style="width:100px; border: 2px solid black;">Purchasing group</th>
          <th style="width:100px; border: 2px solid black;">Non-purchasing group</th>
          <th style="width:50px ; border: 2px solid black;">Total</th>
          <th style="width:200px; border: 2px solid black;">Entropy</th>
          <th style="border: 2px solid black;"></th>
        </tr>
        <tr>
          <td class="tdCenter">< 25</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
          <td class="tdCenter">6</td>
          <td>\(\small - (\frac{1}{6}) log_{2} ( \frac{1}{6} )^2 - (\frac{5}{6} ) log_{2} (\frac{5}{6} ) = 0.6500\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\ge\) 25</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">14</td>
          <td>\(\small - (\frac{6}{14}) log_{2} ( \frac{6}{14} )^2 - (\frac{8}{14} ) log_{2} (\frac{8}{14} ) = 0.9852\)</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>7</th>
          <th>13</th>
          <th>20</th>
          <th class="tdLeft">Expected entropy<br>= \(\small \frac{6}{20} \times 0.6500 + \frac{14}{20} \times 0.9852\)<br>= 0.8847</th>
          <th class="tdLeft" style="width:130px">Information gain<br>= \(\small 0.9341 - 0.8847\)<br>= 0.0494</th>
        </tr>
        <tr> 
          <th style="width=100px; border: 2px solid black;">Age interval 2</th>
          <th style="width:100px; border: 2px solid black;">Purchasing group</th>
          <th style="width:100px; border: 2px solid black;">Non-purchasing group</th>
          <th style="width:50px ; border: 2px solid black;">Total</th>
          <th style="width:200px; border: 2px solid black;">Entropy</th>
          <th style="border: 2px solid black;"></th>
        </tr>
        <tr>
          <td class="tdCenter">< 35</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">12</td>
          <td class="tdCenter">15</td>
          <td>\(\small - (\frac{3}{15}) log_{2} ( \frac{3}{15} )^2 - (\frac{12}{15} ) log_{2} (\frac{12}{15} ) = 0.7219\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\ge\) 35</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
          <td>\(\small - (\frac{4}{5}) log_{2} ( \frac{4}{5} )^2 - (\frac{1}{5} ) log_{2} (\frac{1}{5} ) = 0.7219\)</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>7</th>
          <th>13</th>
          <th>20</th>
          <th class="tdLeft">Expected entropy<br>= \(\small \frac{15}{20} \times 0.7219 + \frac{5}{20} \times 0.7219\)<br>= 0.7219</th>
          <th class="tdLeft">Information gain<br>= \(\small 0.9341 - 0.7219\)<br>= 0.2121</th>
        </tr>
       </table>
    <p>
       (Age interval 2) that divides into '< 35' and '\(\ge\) 35' has a large information gain, so this interval division 
       is selected. 
  </div>
  <p>
  <div class="mainTable">
       The above method can also be applied if you want to reduce the number of categories when there are multiple values
       ​​for a categorical variable. For example, if there are three categorical variable values, ​​(\(A_{1}, A_{2}, A_{3}\)),
       and we want to reduce them to two, we can investigate the information gain for the three possible combinations
       (\(A_{1}, A_{2} : A_{3}\)), (\(A_{1}, A_{3} : A_{2}\)), (\(A_{2}, A_{3} : A_{1}\)) and select the combination 
       that maximizes the information gain.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060204">6.2.4  Overfitting and pruning decision tree</h4>
  <p>
  <div class="mainTable">
       Decision tree models can have an overfitting problem, classifying training data well but not good for testing data. 
       Pruning is one way to solve the problem of overfitting, and there are pre-pruning and post-pruning. Pre-pruning 
       is to examine the appropriateness of the division using chi-square tests and information gain to prevent 
       meaningless divisions from continuing. Regardless of which method is applied, a threshold value must be set, 
       which must be determined by the researcher. If the threshold value is too high, a simple tree will be formed,
       and conversely, if it is too low, a complex tree may be formed. 
    <p>
       Post-pruning is a method of removing branches from a completed tree. For example, when pruning subtrees 
       for each node, the expected error rate is calculated, and if this value is the maximum expected error rate, 
       the subtrees are maintained. Otherwise, they are pruned. Pre-pruning and post-pruning are sometimes used in combination.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h5>Characteristics of decision tree model</h5>
  <p>
  <div class="mainTable">
       The important characteristics of the decision tree classification model are summarized as follows:
    <p>
       <div class="textL20M20">
         1) The decision tree model is a nonparametric method that does not assume the distribution function of each group.
       </div>
       <div class="textL20M20">
         2) The results of the decision tree model are easy to explain to anyone. The accuracy of the model is not inferior to other classification models.
       </div>
       <div class="textL20M20">
         3) Since the method of creating a decision tree is not computationally complex, it can be created quickly, even for large amounts of data. Once a decision tree is created, the task of classifying data whose group affiliation is unknown into one group is very fast.
       </div>
       <div class="textL20M20">
         4) The decision tree algorithm can classify abnormal noise data without much sensitivity.
       </div>
       <div class="textL20M20">
         5) Even if there are other unnecessary variables that are highly correlated with one variable, the decision tree 
            is not greatly affected. However, if there are many unnecessary variables, there is a risk that the decision tree 
            will become too large. It is necessary to remove unnecessary variables before classification to prevent the risk.
       </div>
       <div class="textL20M20">
         6) Since the number of decision trees that can be created from one data is very large, it is not easy to find 
         the optimal decision tree among them. Most decision tree algorithms use heuristic search to find the optimal tree, 
         and the Algorithm we discussed also expands the decision tree using a top-down iterative partitioning strategy.
       </div>
       <div class="textL20M20">
         7) Since most decision tree algorithms are top-down iterative partitioning algorithms, they continuously partition 
            the entire data set into smaller data sets. If this process is repeated, there may be too few data in some leaves
            to make a statistically meaningful classification decision. It is necessary to create a stopping rule that 
            prevents further partitioning when the number of data in a node is less than a certain number.
       </div>
       <div class="textL20M20">
         8) In the entire decision tree, small trees (subtrees) with the same shape can appear in multiple nodes,
            which can complicate the decision tree.
       </div>
       <div class="textL20M20">
         9) A decision tree examines only the conditions for one variable in one node. Therefore, the classification rule 
         of the decision tree partitions the entire decision space into straight lines parallel to the coordinate axes (variables)
        (&lt;Figure 6.2.10&gt;).
       </div>
    <p>
       <img class="figure50" src="./Figure/Fig060210.png">
       <div class="figText">&lt;Figure 6.2.10&gt; Split two dimension decision space by a decision tree</div>
    <p>
       <div class="textL20">
         However, the example data in &lt;Figure 6.2.11&gt; is not easy to split by a decision tree.
         The test condition at the node can be transformed into one for more than one variable to solve the problem,. 
         For example, a diagonal shape like &lt;Figure 6.2.11&gt; can be considered as a test condition for two variables.
         It would be good to create a test condition for more than two variables, but the calculation is complicated 
         and another problem of ‘how to create the optimal test condition?’ arises.
       </div>
    <p>
       <img class="figure50" src="./Figure/Fig060211.png">
       <div class="figText">&lt;Figure 6.2.11&gt; Example data which is not easy to split by a decision tree</div>
    <p>
       <div class="textL20M20">
         10) The choice of uncertainty measures such as entropy or Gini coefficient does not have a significant effect 
         on the performance of the decision tree. This is because the measures have similar characteristics.
         What affects the performance of the decision tree is the choice of the tree pruning method rather than 
         the choice of measure.
       </div>
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060205">6.2.5 R practice</h4>
  <p>
  <div class="mainTable">
       Let us practice R commands using the data saved at C:\Rwork\PurchaseByCredit20.csv. 
       The file format is a comma separated value (csv) type. You can find this file
       from 『eStat』 system. Click Ex > DataScience and then click the data 'PurchaseByCredit20.csv'.
       After this file is loaded to 『eStat』, save it using 'csv Save' button. It will be
       saved at the Download folder on your PC. Copy this file to C:\Rwork\ folder.
       In order to practice the decision tree using this data, you need to change first the working directory of R 
       as follows.
    <p>
       File > Change Directory > C: > Rwork
    <br><br>
       If you read the data file in R, it looks like as follows.
    <p>
    <table style="width:800px;">
         <tr>
           <td># read the data file</td>
         </tr>
         <tr>
           <td style="width:650px;"><span style="color:red;">> card <- read.csv("PurchaseByCredit20.csv", header=T, as.is=FALSE)</span></td>
           <td class="tdCenter" style="width:150px;"><button onclick="rFunction(0)">copy r command</button></td>
         </tr>
         <tr>
           <td>
             <span style="color:red;">> card</span>
<pre>
      id Gender Age Income Credit Purchase
1      1   male 20s LT2000   Fair      Yes
2      2 female 30s GE2000   Good       No
3      3 female 20s GE2000   Fair       No
4      4 female 20s GE2000   Fair      Yes
5      5 female 20s LT2000    Bad       No
6      6 female 30s GE2000   Fair       No
7      7 female 30s GE2000   Good      Yes
8      8   male 20s LT2000   Fair       No
9      9 female 20s GE2000   Good       No
10    10   male 30s GE2000   Fair      Yes
11    11 female 30s GE2000   Good      Yes
12    12 female 20s LT2000   Fair       No
13    13   male 30s GE2000   Fair       No
14    14   male 30s LT2000   Fair      Yes
15    15 female 30s GE2000   Good      Yes
16    16 female 30s GE2000   Fair       No
17    17 female 20s GE2000    Bad       No
18    18   male 20s GE2000    Bad       No
19    19   male 30s GE2000   Good      Yes
20    20   male 20s LT2000   Fair       No
</pre>
           </td>
         </tr>
         <tr>
           <td><span style="color:red;">> attach(card)</span></td>
           <td class="tdCenter"><button onclick="rFunction(2)">copy r command</button></td>          
         </tr>
    </table>
  </div>
  <p>
  
  <div class="mainTable">
       To analyze decision trees using R, you need to install a package called <b>rpart</b>. From the main menu of R,
       select ‘Package’ => ‘Install package(s)’, and a window called ‘CRAN mirror’ will appear. Here, 
       select ‘0-Cloud [https]’ and click ‘OK’. Then, when the window called ‘Packages’ appears, select 
       ‘rpart’ and click ‘OK’. 
       'rpart' is a package for modeling of Recursive Partitioning and Regression Trees and general usage and
       key arguments of the function are described in the following table. 
    <p>
       <table style="width:800px;">
         <tr>
           <th class="tdLeft" colspan="2">Fit a Recursive Partitioning and Regression Trees<br>
           </th>
         </tr>
         <tr>
           <th class="tdLeft" colspan="2">
             rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)
           </th>
         </tr>
         <tr>
           <td>formula</td>
           <td>a formula, with a response but no interaction terms. If this a a data frame, that is taken as the model frame (see model.frame).</td>
         </tr>
         <tr>
           <td>data</td>
           <td>an optional data frame in which to interpret the variables named in the formula.</td>
         </tr>
         <tr>
           <td>method</td>
           <td>one of "anova", "poisson", "class" or "exp". If method is missing then the routine tries to make an intelligent guess. If y is a survival object, then method = "exp" is assumed, if y has 2 columns then method = "poisson" is assumed, if y is a factor then method = "class" is assumed, otherwise method = "anova" is assumed. It is wisest to specify the method directly, especially as more criteria may added to the function in future.</td>
         </tr>
         <tr>
           <td>parms</td>
           <td>optional parameters for the splitting function.<br>
               Anova splitting has no parameters.<br>
               Poisson splitting has a single parameter, the coefficient of variation of the prior distribution on the rates. The default value is 1.<br>
               Exponential splitting has the same parameter as Poisson.<br>
               For classification splitting, the list can contain any of: the vector of prior probabilities (component prior), the loss matrix (component loss) or the splitting index (component split). The priors must be positive and sum to 1. The loss matrix must have zeros on the diagonal and positive off-diagonal elements. The splitting index can be gini or information. The default priors are proportional to the data counts, the losses default to 1, and the split defaults to gini.<br>
           </td>
         </tr>
         <tr>
           <td>control</td>
           <td>a list of options that control details of the rpart algorithm. See rpart.control.</td>
         </tr>
         <tr>
           <th class="tdLeft" colspan="2">rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, 
              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,
              surrogatestyle = 0, maxdepth = 30, ...)
           </th>
         </tr>
         <tr>
           <td>minsplit</td>
           <td>the minimum number of observations that must exist in a node in order for a split to be attempted.</td>
         </tr>
         <tr>
           <td>minbucket</td>
           <td>the minimum number of observations in any terminal <leaf> node. If only one of minbucket or minsplit is specified, the code either sets minsplit to minbucket*3 or minbucket to minsplit/3, as appropriate.</td>
         </tr>
         <tr>
           <td>cp</td>
           <td>complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. For instance, with anova splitting, this means that the overall R-squared must increase by cp at each step. The main role of this parameter is to save computing time by pruning off splits that are obviously not worthwhile. Essentially,the user informs the program that any split which does not improve the fit by cp will likely be pruned off by cross-validation, and that hence the program need not pursue it.</td>
         </tr>
         <tr>
           <td>maxdepth</td>
           <td>Set the maximum depth of any node of the final tree, with the root node counted as depth 0. Values greater than 30 rpart will give nonsense results on 32-bit machines.
           </td>
         </tr>
       </table>
    <p>
       An example of R commands for a decision tree using the dataset card is as follows.
       The results of practicing a decision tree in R with purchase as the dependent variable of card data 
       and other variables as independent variables are as follows. In Example 6.2.5, the information gain 
       of credit status was the largest, so this variable was the root node. However, since some of the number of data  
       belonging to credit status variable value was small (‘bad’ = 3, ‘good’ = 6, ), the next largest information gain, 
       which is age, was selected as the root node in R. 
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:600px;"><span style="color:red;">> install.packages('rpart')</span></td>
           <td class="tdCenter"><button onclick="rFunction(48)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> library(rpart)</span></td>
           <td class="tdCenter"><button onclick="rFunction(49)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> fit <- rpart(Purchase ~ Gender + Age + Income + Credit, data = card)</span></td>
           <td class="tdCenter"><button onclick="rFunction(50)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> fit</span>
<pre>
n= 20 
node), split, n, loss, yval, (yprob)
      * denotes terminal node
1) root 20 8 No (0.6000000 0.4000000)  
  2) Age=20s 10 2 No (0.8000000 0.2000000) *
  3) Age=30s 10 4 Yes (0.4000000 0.6000000) 
</pre> 
           </td>
           <td class="tdCenter"><button onclick="rFunction(51)">copy r command</button></td>
         </tr>
       </table>
    <br>
       However, the analysis result is simple because the minimum number of pruning data of the rpart function is 20 
       or more by default. Let's change this default setting and prune it. 
       The results of practicing decision trees in R with purchase as the dependent variable of card data 
       and other variables as independent variables are as follows. 
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:600px;"><span style="color:red;">> fit2 <- rpart(Purchase ~ Gender + Age + Income + Credit, data = card, control = rpart.control(minsplit = 6))</span></td>
           <td class="tdCenter"><button onclick="rFunction(52)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> fit2</span>
<pre>
n= 20 
node), split, n, loss, yval, (yprob)
      * denotes terminal node
1) root 20 8 No (0.6000000 0.4000000)  
  2) Age=20s 10 2 No (0.8000000 0.2000000) *
  3) Age=30s 10 4 Yes (0.4000000 0.6000000)  
    6) Credit=Fair 5 2 No (0.6000000 0.4000000) *
    7) Credit=Good 5 1 Yes (0.2000000 0.8000000) *
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(53)">copy r command</button></td>
         </tr>
       </table>
       To draw a decision tree as a graph, use the plot and text commands below.
       'plot' draws a tree diagram and, if the option 'compress' is T, the vertical width is narrowed, 
       and if 'uniform' is T, the horizontal width is narrowed. 'margin' sets the margin. 
       If the margin is 0, the label may be cut off, so set it little by little. 
       'text' labels the tree, and 'use.n' displays something like 0/4. The decision tree drawn with the above command is as follows.
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:600px;"><span style="color:red;">> plot(fit2,compress=T,uniform=T,margin=0.1)</span></td>
           <td class="tdCenter"><button onclick="rFunction(54)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> text(fit2,use.n=T,col='blue')</span><br>
             <img class="figure50" src="./Figure/Fig060212.png">
             <div class="figText">&lt;Figure 6.2.12&gt; Decision tree using R</div>
           </td>
           <td class="tdCenter"><button onclick="rFunction(55)">copy r command</button></td>
         </tr>
       </table>
       In the decision tree diagram above, age=a is the condition of the left branch, and 'a' means the first variable value '20s',
       credit=b is also the condition of the left branch, and 'b' means 'fair' 
       (in this case, it seems to have been merged because 'bad' is missing).
       For more information, please refer to the help. In packages such as R, when the number of data corresponding to each variable value
       is too small, pruning is determined by merging with adjacent variable values.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h3 id="0603">6.3 Naive Bayes classification model</h3>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060301">6.3.1 Bayes classification</h4>
  <p>
  <div class="mainTable">
       When the <b>prior probability</b> of being classified into each group and the <b>likelihood probability</b>
       of each group are known, the <b>Bayes classification model</b> is a method of classifying data into groups 
       with high probability by calculating the posterior probability using Bayes theorem, which was introduced
       in section 4.1.2. Let us look at the Bayes classification model for a single variable case using the following example.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.3.1</b> (Classification by prior probability)
    <br>
       When 20 customers who visited a computer store were surveyed, 8 customers purchased a computer, and 12 customers
       did not purchase a computer. Based on this information, classify a customer who visited this store on a day 
       whether he would purchase a computer or not.
    <p>
       <b>Answer</b>
    <p>
       Among the 20 customers who visited the store, only 8 (40%) purchased a computer, and 12 (60%) did not purchase a computer.
       This information, the probability of the purchasing group is 40% and the probability of the non-purchasing 
       group is 60% by surveying past data, are called <b>prior probabilities</b>. If a decision is made based on 
       these prior probabilities, since the probability of the non-purchasing group is higher than the purchasing group, 
       it is reasonable to classify a customer who visited on a day as into the non-purchasing group.
  </div>
  <p>
  <div class="mainTable">
       As in the example above, classifying data whose group membership is unknown  
       into a group with the highest prior probability is called a <b>classification by prior probability.</b>
       If the purchasing group of the computer is \(G_{1}\) and the non-purchasing group is \(G_{2}\), and the 
       prior probability of each group is \(P(G_{1})\) and \(P(G_{2})\), the classification rule by the prior probability is as follows;
  </div>
  <p>
  <div class="mainTableYellow">
       <b>Classification rule by prior probability</b>
    <p>
      \(\qquad\)‘If \(P(G_{1})\) ≥ \(P(G_{2})\), classify the data into group \(G_{1}\), otherwise classify it into \(G_{2}\)’ 
  </div>
  <p>
  <div class="mainTable">
       If we can obtain the distribution of the age of the purchasing group and the non-purchasing group, it can be 
       useful information for judging whether to purchase or not. This is called the <b>likelihood probability distribution</b>
       or <b>group probability distribution</b>. In this case, we can calculate the <b>posterior probability</b> of each group
       using the Bayes theorem, and the classification using this posterior probability is called <b>Bayes classification</b>. 
       Let us look at the following example.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.3.2</b> (Classification by posterior probability)
    <br>
       In Example 6.3.1, the classification decision was made using simple prior information such as the probability of 
       the purchasing group and the non-purchasing group. One of the additional information that can be obtained is 
       a customer's age. Suppose there are 10 customers in their age 20's 
       among 20 customers and 10 customers in their age 30's. Among the 8 purchasing groups, 2 customers in their age 20's 
       and 6 are in their age 30's. If a customer who visited the store on a day is in his age 20's, classify the customer 
       whether he purchases the computer or not by calculating the posterior probability. 
    <p>
       <b>Answer</b>
    <p>
       The customer's age by the purchasing group (\(\small G_{1}\)) and the non-purchasing group (\(\small G_{2}\))
       are summarized in the following table. 
    <p>
       <table style="width:600px"> 
        <tr>
          <th colspan="4">Table 6.3.1 crosstable on Age by Purchasing status</th>
        </tr>
        <tr> 
          <th style="width=100px;">Age</th>
          <th style="width:170px">Purchasing group<br>\(\small G_{1}\)</th>
          <th style="width:170px">Non-purchasing group<br>\(\small G_{2}\)</th>
          <th style="width:130px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">20's</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr>
          <td class="tdCenter">30's</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
       </table>
    <p>
       Looking at this table, we can see that the purchasing group (\(\small G_{1}\)) has a higher proportion of customers 
       in age 30's, and the non-purchasing group (\(\small G_{2}\)) has a higher proportion of customers in their age 20's. 
       The age distribution of each group is called the <b>likelihood probability distribution</b>. 
       When the age of 20's is represented as \(\small X\), the probability of age 20s in the purchasing group is 2/8, 
       which is denoted as the conditional probability \(\small P( X | G_{1} ) \) and the probability of age 20's 
       in the non-purchasing group is 8/12, which is denoted as \(\small P( X | G_{2} ) \).
       If a customer who visited on a day was in his age 20's, the probability that this customer would purchase the computer
       is called the <b>posterior probability</b> of the purchasing group and is denoted as \(\small P( G_{1} | X ) \). 
       This posterior probability can be obtained as follows using Bayes' theorem. 
       $$ \small
       \begin{align}
         P( G_{1} | X ) &= \frac {P( G_{1} ) \times P(X|G_{1}) }{P( G_{1} ) \times P(X|G_{1}) + P( G_{2} ) \times P(X|G_{2})} \\
         &= \frac{ \frac{8}{20} \times \frac{2}{8} }{ \frac{8}{20} \times \frac{2}{8} +  \frac{12}{20} \times \frac{8}{12} } = 0.2
       \end{align}
       $$
       Here, the denominator is the probability of all age 20's, \(\small P( X ) \) = 10/20, and the numerator means 
       the proportion of age 20's who purchased the computer, 2/20. In the same way, the posterior probability of 
       non-purchasing goup among age 20's is denoted as \(\small P( G_{2} | X ) \), and is calculated as follows using Bayes' theorem. 
       $$ \small
       \begin{align}
         P( G_{2} | X ) &= \frac {P( G_{2} ) \times P(X|G_{2}) }{P( G_{1} ) \times P(X|G_{1}) + P( G_{2} ) \times P(X|G_{2})} \\
         &= \frac{ \frac{12}{20} \times \frac{8}{12} }{ \frac{8}{20} \times \frac{2}{8} +  \frac{12}{20} \times \frac{8}{12} } = 0.8
       \end{align}
       $$
       Therefore, since the posterior probability that a customer in age 20's belongs to the non-purchasing group is 0.8, 
       which is higher than the posterior probability of belonging to the purchasing group of 0.2, this customer is 
       classified as a non-purchasing group. As a result, when we obtain additional information that the visitor is in age 20's,
       we can see that the probability that this person belongs to the purchasing group of 0.2 is lower than the 
       prior probability of 0.4.
  </div>
  <p>
  <div class="mainTable">
       When we know that a customer who visited is age 20's (\(\small X\)), we calculate the posterior probability 
       \(\small P( G_{1} | X ) \) that he belongs to the purchasing group (\(\small G_{1}\)) and the posterior probability
       \(\small P( G_{2} | X ) \) that he belongs to the non-purchasing group (\(\small G_{2}\)) and classify the person
       into the group with the higher posterior probability. This is called <b>Bayes classification</b> and the rule  
       can be summarized as follows. 
  </div>
  <p>
  <div class="mainTableYellow">
       <b>Bayes classification rule by posterior probability</b>
    <p>
       \(\qquad\) ‘If \(\small P( G_{1} | X ) \) ≥ \(\small P( G_{2} | X ) \), classify data as \(\small G_{1}\), otherwise classify as \(\small G_{2}\)’ 
    <p>
       Here, \(\small P( G_{1} | X ) \) and \(\small P( G_{2} | X ) \) have the same denominator in the calculation 
       of the posterior probability, so the classification rule can be written as follows. 
    <p>
       \(\qquad\) ‘If \(\small \frac{P( X | G_{1} )}{P( X | G_{2} )} \) ≥ \(\small \frac{P(G_{2})}{P(G_{1})} \), classify data as \(\small G_{1}\), otherwise classify as \(\small G_{2}\)’ 
  </div>
  <p>
  <div class="mainTable">
       The Bayes classification model for one variable can be easily extended to the case of \(m\) random variables
       \(\small \boldsymbol X = (X_{1}, X_{2}, ... , X_{m}) \). Let the prior probabilities of \(k\) number of  groups,
       \(\small G_{1}, G_{2}, ... , G_{k}\), be \(\small P(G_{1}), P(G_{2}), ... , P(G_{k})\), and let the likelihood 
       probability distribution function for each group be 
       \(\small P( \boldsymbol X | G_{1}), P( \boldsymbol X | G_{2}), ... , P( \boldsymbol X | G_{k})\).
       Given the observation data \(\small \boldsymbol x\) for classification, the posterior probability 
       \(\small P(G_{i} | \boldsymbol x)\) that this data comes from the group \(\small G_{i}\) is as follows.
       $$ \small
         P( G_{i} | \boldsymbol x ) = \frac {P( G_{i} ) \times P(\boldsymbol x | G_{i})}{P(G_{1}) \times P(\boldsymbol x |G_{1}) + P(G_{2}) \times P(\boldsymbol x | G_{2}) + \cdots + P(G_{k}) \times P(\boldsymbol x | G_{k}) } 
       $$
       The Bayes classification rule using the posterior probability is as follows. 
  </div>
  <p>
  <div class="mainTableYellow">
       <b>Bayes Classification</b> - multiple groups
    <br>
       Suppose that prior probabilities of \(k\) number of  groups, \(\small G_{1}, G_{2}, ... , G_{k}\), are 
       \(\small P(G_{1}), P(G_{2}), ... , P(G_{k})\), and likelihood probability distribution functions  
       for each group are \(\small P( \boldsymbol X | G_{1}), P( \boldsymbol X | G_{2}), ... , P( \boldsymbol X | G_{k})\). 
       Given the observation data \(\small \boldsymbol x\) for classification, let the posterior probabilities  
       that \(\small \boldsymbol x\) comes from each group be 
       \(\small P(G_{1} | \boldsymbol x), P(G_{2} | \boldsymbol x), ... , P(G_{k} | \boldsymbol x)\). 
       The Bayes classification rule is as follows. 
    <p>
       \(\qquad\) 'Classify \(\boldsymbol x\) into a group with the highest posterior probability'
    <p>
       If we denote the likelihood probability functions as 
       \(\small f_{1}(\boldsymbol x), f_{2}(\boldsymbol x), ... , f_{k}(\boldsymbol x)\),
       since the denominators in the calculation of posterior probabilities are the same, the Bayes classification rule
       can be written as follows.
    <p> 
       \(\qquad\) 'If \(\small P(G_{k}) f_{k}(\boldsymbol x) ≥ P(G_{i}) f_{i}(\boldsymbol x) \) for all \(k\) ≠ \(i\),
       classify \(\boldsymbol x\) into group \(\small G_{k}\)'
    <p>
       If there are only two groups \(\small G_{1}\)and \(\small G_{2}\), the Bayes classification rule is expressed as follows. 
    <p>
       \(\qquad\) 'if \( \frac{f_{1}(\boldsymbol x)}{f_{2}(\boldsymbol x)} ≥ \frac{P(G_{2})}{P(G_{1})} \),
       classify \(\boldsymbol x\) into group \(\small G_{1}\), else into group \(\small G_{2}\)'
  </div>
  <p>
  <div class="mainTable">
       When there are sample data, if the likelihood probability distribution \(f_{1}(\boldsymbol x)\) can be estimated
       from the sample, the Bayes classification rule can also be estimated using the likelihood probability distribution. 
       Therefore, the Bayes classification rule can appear in many variations depending on the estimation method of 
       the likelihood probability distribution. Estimation of the likelihood probability distribution using samples 
       can be done using either a parametric method, such as maximum likelihood estimation, or a nonparametric method. 
       In the case of categorical data, a multidimensional distribution estimated from the sample is often used, 
       and in the case of continuous data, a multivariate normal distribution is often used.
       For more information, please refer to the related references.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060302">6.3.2 Naive Bayes classification model for categorical data</h4>
  <p>
  <div class="mainTable">
       When there are two or more categorical variables for classification, if the variables can be assumed independent, 
       the Bayes classification is called a <b>naive Bayes classification</b>. In the case of categorical data, 
       since there are many cases in which variables can be assumed independent in real applications,
       the naive Bayes classification is used frequently. Let us consider an example of the naive Bayes classification.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.3.3</b>
       Consider a survey of 20 customers at a computer store on age (\(\small X_{1}\)), monthly income (\(\small X_{2}\)), 
       credit status (\(\small X_{3}\)) and their purchasing status as shown in Table 6.3.2. 
       Note that the age transformed into '20s' and '30s' as categorical, income into 'LT2000' and 'GE2000',
       and credit status into 'Bad', 'Fair' and 'Good'. 
    <p>
      <table style="width:740px"> 
        <tr>
          <th colspan="6">Table 6.3.2  Survey of customers on age, income, credit status and purchasing status</th>
        </tr>
        <tr> 
          <th style="width:120px">Number</th>
          <th style="width:120px">Age</th>
          <th style="width:140px">Income<br>(unit USD)</th>
          <th style="width:120px">Credit</th>
          <th style="width:120px">Purchase</th>
        </tr>
        <tr><td class="tdCenter">1</td>  <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">2</td>  <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">3</td>  <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">4</td>  <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">5</td>  <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">6</td>  <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">7</td>  <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">8</td>  <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">9</td>  <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">10</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">11</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">12</td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">13</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">14</td> <td class="tdCenter">30s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">15</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">16</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">17</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">18</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> </tr>
        <tr><td class="tdCenter">19</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> </tr>
        <tr><td class="tdCenter">20</td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> </tr>
      </table>
    <p>
       If a customer who visited this store one day is 33 years old, has a monthly income of 1900 USD, and has a good credit status,
       classify him using the posterior probability of whether he will buy a computer or not.
    <p>
       <b>Answer</b>
    <p>
       The one-dimensional likelihood probability distributions, 
       \(\small P(X_{1} | G_{i}),\; P(X_{2} | G_{i}),\; P(X_{3} | G_{i})\), of each variable by purchasing group 
       (\(\small G_{1}\)) and non-purchasing group (\(\small G_{1}\)) are summarized as in Table 6.3.3,
       and the multidimensional likelihood probability distribution of three variables, 
       \(\small P((X_{1}, X_{2}, X_{3}) | G_{i})\), is summarized as in Table 6.3.4.
    <p>
       <table style="width:630px"> 
        <tr>
          <th colspan="4">Table 6.3.3 One-dimensional likelihood probability distributions on Age, Income and Credit</th>
        </tr>
        <tr> 
          <th style="width=100px;">Age</th>
          <th style="width:170px">Purchasing group<br>\(\small G_{1}\)</th>
          <th style="width:170px">Non-purchasing group<br>\(\small G_{2}\)</th>
          <th style="width:130px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">20's</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr>
          <td class="tdCenter">30's</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">10</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
        <tr><td> </td></tr>
        <tr> 
          <th style="width=100px;">Income</th>
          <th style="width:170px">Purchasing group<br>\(\small G_{1}\)</th>
          <th style="width:170px">Non-purchasing group<br>\(\small G_{2}\)</th>
          <th style="width:130px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">LT2000</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">6</td>
        </tr>
        <tr>
          <td class="tdCenter">GE2000</td>
          <td class="tdCenter">6</td>
          <td class="tdCenter">8</td>
          <td class="tdCenter">14</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
        <tr><td> </td></tr>
        <tr> 
          <th style="width=100px;">Credit</th>
          <th style="width:170px">Purchasing group<br>\(\small G_{1}\)</th>
          <th style="width:170px">Non-purchasing group<br>\(\small G_{2}\)</th>
          <th style="width:130px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter">0</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">3</td>
        </tr>
        <tr>
          <td class="tdCenter">Fair</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">7</td>
          <td class="tdCenter">11</td>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">6</td>
        </tr>
        <tr>
          <th>Total</th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
       </table>
    <p>
       <table style="width:700px"> 
        <tr>
          <th colspan="6">Table 6.3.4 Multi-dimensional likelihood probability distributions on Age, Income and Credit</th>
        </tr>
        <tr> 
          <th style="width=100px;">Age</th>
          <th style="width=100px;">Income</th>
          <th style="width=100px;">Credit</th>
          <th style="width:170px">Purchasing group<br>\(\small G_{1}\)</th>
          <th style="width:170px">Non-purchasing group<br>\(\small G_{2}\)</th>
          <th style="width:130px">Total</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="6">20's</td>
          <td class="tdCenter" rowspan="3">LT2000</td>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter"></td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">1</td>
        </tr>
        <tr>
          <td class="tdCenter">Fair</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">4</td>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="3">GE2000</td>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter"></td>
          <td class="tdCenter">2</td>
          <td class="tdCenter">2</td>
        </tr>
        <tr>
          <td class="tdCenter">Fair</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">2</td>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter"></td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">1</td>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="6">30's</td>
          <td class="tdCenter" rowspan="3">LT2000</td>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
        </tr>
        <tr>
          <td class="tdCenter">Fair</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter"></td>
          <td class="tdCenter">1</td>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="3">GE2000</td>
          <td class="tdCenter">Bad</td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
          <td class="tdCenter"></td>
        </tr>
        <tr>
          <td class="tdCenter">Fair</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">3</td>
          <td class="tdCenter">4</td>
        </tr>
        <tr>
          <td class="tdCenter">Good</td>
          <td class="tdCenter">4</td>
          <td class="tdCenter">1</td>
          <td class="tdCenter">5</td>
        </tr>
        <tr>
          <th>Total</th>
          <th></th>
          <th></th>
          <th>8</th>
          <th>12</th>
          <th>20</th>
        </tr>
       </table>
    <p>
       If a customer who visited the computer store is represented as \( \boldsymbol x\) = 
       (\(\small x_{1}, x_{2}, x_{3}\)) = (30s, LT2000, Fair), the posterior probability 
       that this customer belongs to the purchasing group \(\small G_{1}\) is \(\small P(G_{1} | \boldsymbol x )\)
       and the posterior probability that this customer belongs to the non-purchasing group \(\small G_{2}\) is 
       \(\small P(G_{2} | \boldsymbol x )\). However, in the multidimensional likelihood distribution for 
       the three variables in Table 6.3.4, the probability of a customer being in their 30s, with an income of LT2000, 
       and with fair credit is \(\small P(G_{1} | \boldsymbol x )\) = 1/8 and \(\small P(G_{2} | \boldsymbol x )\) = 0. 
       If the number of samples is insufficient, it is difficult to correctly estimate the likelihood probability distribution.
       In this case, if variables, age, income, and credit status, can be assumed to be independent, 
       the one-dimensional likelihood probability distribution of each variable is used approximately to estimate 
       the multidimensional likelihood probability distribution as follows. 
    <p>
       \( \qquad \small 
          P(\boldsymbol X = (X_{1}, X_{2}, X_{3})\; |\; G_{i}) ≈ P(X_{1} | G_{i}) \; P(X_{2} | G_{i}) \; P(X_{3} | G_{i}) 
       \)
    <p>
       For this problem, the approximate likelihood of customer \(\boldsymbol x\) = (30s, LT2000, Fair) is as follows.
    <p>
       \( \qquad \small 
          P(\boldsymbol x = (30s, LT2000, Fair) \;|\; G_{1}) ≈ \frac{6}{8} \times \frac{2}{8} \times \frac{4}{8} = 0.0938 
       \)
    <p>
       \( \qquad \small 
          P(\boldsymbol x = (30s, LT2000, Fair) \;|\; G_{2}) ≈ \frac{4}{12} \times \frac{4}{12} \times \frac{7}{12} = 0.0648 
       \)
    <p>
       Therefore, the posterior probability for each group is as follows. 
    <p>
       \( \qquad \small
       \begin{align}
         P( G_{1} | \boldsymbol x ) &= \frac {P( G_{1} ) \times P(\boldsymbol x | G_{1}) }{P( G_{1} ) \times P(\boldsymbol x | G_{1}) + P( G_{2} ) \times P(\boldsymbol x | G_{2})} \\
         &= \frac{ 0.4 \times 0.0938 }{ 0.4 \times 0.0938 +  0.6 \times 0.0648 } = 0.4911
       \end{align}
       \)
    <p>
       \( \qquad \small
       \begin{align}
         P( G_{2} | \boldsymbol x ) &= \frac {P( G_{2} ) \times P(\boldsymbol x | G_{2}) }{P( G_{1} ) \times P(\boldsymbol x | G_{1}) + P( G_{2} ) \times P(\boldsymbol x | G_{2})} \\
         &= \frac{ 0.6 \times 0.0648 }{ 0.4 \times 0.0938 +  0.6 \times 0.0648 } = 0.5089
       \end{align}
       \)
    <p>
       Since the posterior probability of belonging to the non-purchasing group is 0.5089, which is greater than 
       the probability of belonging to the purchasing group, which is 0.4911, the customer is classified as 
       the non-purchasing group. Lift chart, confusion matrix and ROC graph are to evaluate the classification 
       model, and they will be explained in the next section.
    <p>
       <!---   ************ html for naive Bayes classification ************  ---->
       <b>[<span data-msgid="NaiveBayes"></span>]</b>
    <p>
       <iframe class="example140" src="./example/060301.html"> </iframe>
    <p>
  </div>
  <p>
  <div class="mainTable">
       As in the example above, assuming that variables for classification are independent, 
       obtaining an approximate likelihood probability distribution by group, obtaining the posterior probability, 
       and then classifying is called a <b>naive Bayes classification</b>. The naive Bayes classification 
       is less realistic because it assumes that all variables are independent of each other, but it is often used 
       as an approximate classification method. However, you should know that it can show inaccurate classification results 
       when variables are related to each other. 
    <p>
       To compensate for the problem of assuming that all variables are independent, 
       subsets of variables can be assumed to be independent, and the likelihood probability 
       distribution can be obtained. This classification model is called a <b>Bayes belief network</b>. 
       In this method, it is necessary to first investigate which variables are related to each other and 
       which variables are independent. For more information, please refer to the references.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060303">6.3.3 Stepwise variable selection</h4>
  <p>
  <div class="mainTable">
       Bayes classification can be applied to whether the variables are continuous or discrete, and an appropriate likelihood
       probability distribution can be estimated. If continuous and discrete variables are mixed, the naive Bayes classification 
       can be applied by categorizing the continuous variables, as we discussed in section 6.2.3. 
    <p>
       If there are many variables in the data, in general, we use variables that can best explain group variables, 
       that is, variables with high discriminatory power between groups.
       A stepwise variable selection can be helpful in classifying data to increase accuracy. 
       There are two methods to select variables: forward selection and
       backward elimination. The <b>forward selection</b> of variables is similar to the variable selection in 
       a decision tree, which selects a variable with the highest information gain using the uncertainty measures
       or chi-square test. After selecting a variable for classification, add another variable with the next highest
       information gain in the selection step-by-step until finding a set of variables with the highest classification accuracy.
    <p> 
       The <b>backward elimination</b> initially includes all variables for classification and selects a variable to remove 
       from the set of all variables, which can improve classification accuracy. Continue removing a variable
       from the set of variables until there is no improvement in classification accuracy. The chi-square test,
       which we discussed in section 6.2.2 is often used for this backward selection of variables in naive Bayes classification.
       A stepwise method also selects variables using the forward selection method while examining 
       whether the variables already selected can be removed. However, it is not easy to verify whether the ‘optimal’ 
       variable selection was made regardless of the method used. For more information, please refer to the references.
  </div>
  <p>

  <!------------------------------------------------------------->
  <h5>Characteristics of Bayes classification model</h5>
  <p>
  <div class="mainTable">
       The characteristics of Bayes classification are summarized as follows.
    <p>
       <div class="textL20M20">
         1) Since the Bayes classification model classifies using the posterior probability, which is calculated by 
            the prior probability and the likelihood probability distribution of each group, the risk of overfitting  
            the model is low but robust.
       </div>
       <div class="textL20M20">
         2) Bayes classification model can perform stable classification even when there are incomplete data, 
            outliers, and missing values.
       </div>
  </div>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060304">6.3.4 R practice - Naive Bayes classification</h4>
  <p>
  <div class="mainTable">
       To analyze naive Bayes classification using R, you need to install a package called <b>naivebayes</b>. 
       From the main menu of R,
       select ‘Package’ => ‘Install package(s)’, and a window called ‘CRAN mirror’ will appear. Here, 
       select ‘0-Cloud [https]’ and click ‘OK’. Then, when the window called ‘Packages’ appears, select 
       ‘naivebayes’ and click ‘OK’. 
       'naivebayes' is a package for modeling of Naive Bayes model in which predictors are assumed to be 
       independent within each class label. General usage and
       key arguments of the function are described in the following table. 
    <p>
       <table style="width:800px;">
         <tr>
           <th class="tdLeft" colspan="2">Fit naive Bayes model in which predictors are assumed to be independent within each class label.<br>
           </th>
         </tr>
         <tr>
           <th class="tdLeft" colspan="2">
             ## Default S3 method:<br>
             naive_bayes(x, y, prior = NULL, laplace = 0, usekernel = FALSE, usepoisson = FALSE, ...)<br>
             ## S3 method for class 'formula'<br>
             naive_bayes(formula, data, prior = NULL, laplace = 0, usekernel = FALSE, usepoisson = FALSE, subset, na.action = stats::na.pass, ...)
           </th>
         </tr>
         <tr>
           <td>x</td>
           <td>matrix or dataframe with categorical (character/factor/logical) or metric (numeric) predictors.</td>
         </tr>
         <tr>
           <td>y</td>
           <td>class vector (character/factor/logical)</td>
         </tr>
         <tr>
           <td>formula</td>
           <td>an object of class "formula" (or one that can be coerced to "formula") of the form: class ~ predictors (class has to be a factor/character/logical).</td>
         </tr>
         <tr>
           <td>data</td>
           <td>matrix or dataframe with categorical (character/factor/logical) or metric (numeric) predictors.</td>
         </tr>
         <tr>
           <td>prior</td>
           <td>vector with prior probabilities of the classes. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.</td>
         </tr>
         <tr>
           <td>laplace</td>
           <td>value used for Laplace smoothing (additive smoothing). Defaults to 0 (no Laplace smoothing).</td>
         </tr>
         <tr>
           <td>usekernel</td>
           <td>logical; if TRUE, density is used to estimate the class conditional densities of metric predictors. This applies to vectors with class "numeric". For further details on interaction between usekernel and usepoisson parameters please see Note below.</td>
         </tr>
         <tr>
           <td>usepoisson</td>
           <td>logical; if TRUE, Poisson distribution is used to estimate the class conditional PMFs of integer predictors (vectors with class "integer").</td>
         </tr>
         <tr>
           <td>subset</td>
           <td>an optional vector specifying a subset of observations to be used in the fitting process.</td>
         </tr>
         <tr>
           <td>na.actioncp</td>
           <td>a function which indicates what should happen when the data contain NAs. By default (na.pass), missing values are not removed from the data and are then omited while constructing tables. Alternatively, na.omit can be used to exclude rows with at least one missing value before constructing tables.</td>
         </tr>
       </table>
    <p>
       An example of R commands for a naive Bayes classification in R with purchase as the dependent variable of card data 
       and other variables as independent variables is as follows. 
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:600px;"><span style="color:red;">> install.packages('naivebayes')</span></td>
           <td class="tdCenter"><button onclick="rFunction(60)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> library(naivebayes)</span></td>
           <td class="tdCenter"><button onclick="rFunction(61)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> nbfit <- naive_bayes(Purchase ~ Gender + Age + Income + Credit, data = card)</span></td>
           <td class="tdCenter"><button onclick="rFunction(62)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> nbfit</span>
<pre>
Call: 
naive_bayes.formula(formula = Purchase ~ Gender + Age + Income + Credit, data = card) 
--------------------------------------------------------  
Laplace smoothing: 0 
--------------------------------------------------------
A priori probabilities:  
 No Yes  
0.6 0.4  
-------------------------------------------------------- 
Tables:  
-------------------------------------------------------- 
:: Gender (Bernoulli)  
-------------------------------------------------------- 
Gender          No       Yes 
  Female 0.6666667 0.5000000 
  Male   0.3333333 0.5000000
-------------------------------------------------------- 
:: Age (Bernoulli)  
-------------------------------------------------------- 
Age          No       Yes 
  20s 0.6666667 0.2500000 
  30s 0.3333333 0.7500000 
-------------------------------------------------------- 
:: Income (Bernoulli)  
-------------------------------------------------------- 
Income          No       Yes 
  GE2000 0.6666667 0.7500000 
  LT2000 0.3333333 0.2500000 
-------------------------------------------------------- 
:: Credit (Categorical)  
-------------------------------------------------------- 
Credit        No       Yes 
  Bad  0.2500000 0.0000000 
  Fair 0.5833333 0.5000000 
  Good 0.1666667 0.5000000 
-------------------------------------------------------- 
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(63)">copy r command</button></td>
         </tr>
       </table>
       If you want to classify the group of the card data using this naive Bayes model with posterior probabilities,
       R commands are as follows.  
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:600px;"><span style="color:red;">> pred <- predict(nbfit, card, type = 'prob')"</span></td>
           <td class="tdCenter"><button onclick="rFunction(64)">copy r command</button></td>
         </tr>
         <tr>
           <td><span style="color:red;">> pred</span>
<pre>
            No          Yes 
 [1,] 0.8057554 0.1942446043 
 [2,] 0.2084691 0.7915309446 
 [3,] 0.8468809 0.1531190926 
 [4,] 0.8468809 0.1531190926 
 [5,] 0.9994378 0.0005621838 
 [6,] 0.4796574 0.5203426124 
 [7,] 0.2084691 0.7915309446 
 [8,] 0.8057554 0.1942446043 
 [9,] 0.6124402 0.3875598086 
[10,] 0.3154930 0.6845070423 
[11,] 0.2084691 0.7915309446 
[12,] 0.8924303 0.1075697211 
[13,] 0.3154930 0.6845070423 
[14,] 0.4087591 0.5912408759 
[15,] 0.2084691 0.7915309446 
[16,] 0.4796574 0.5203426124 
[17,] 0.9991570 0.0008430387 
[18,] 0.9983153 0.0016846571 
[19,] 0.1163636 0.8836363636 
[20,] 0.8057554 0.1942446043 
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(65)">copy r command</button></td>
         </tr>
       </table>
       To make a classification crosstable, you can create a vector of prediction and 
       use table command as below. Using this classification table, accuracy of the model is calculated as 0.7
       which is (8+6) / (8+4+2+6).
    <p>
       <table style="width:800px;">
         <tr>
           <td style="width:600px;"><span style="color:red;">> pred2 <- predict(nbfit, card)</span></td>
           <td class="tdCenter"><button onclick="rFunction(66)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> pred2</span>
<pre>
 [1] No  Yes No  No  No  Yes Yes No  No  Yes Yes No  Yes Yes Yes Yes No  No  Yes 
[20] No 
Levels: No Yes 
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(67)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> classtable <- table(Purchase, pred2)</span></td>
           <td class="tdCenter"><button onclick="rFunction(68)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> classtable</span>
<pre>
        pred2 
Purchase No Yes 
     No   8   4 
     Yes  2   6 
</pre>
           </td>
           <td class="tdCenter"><button onclick="rFunction(69)">copy r command</button></td>
         </tr>
         <tr>
           <td style="width:600px;"><span style="color:red;">> sum(diag(classtable)) / sum(classtable)</span>
[1] 0.7
           </td>
           <td class="tdCenter"><button onclick="rFunction(70)">copy r command</button></td>
         </tr>
       </table>

  </div>
  <p>



  <!------------------------------------------------------------->
  <h3 id="0604">6.4 Evaluation and comparison of a classification model</h3>
  <p>

  <!------------------------------------------------------------->
  <h4 id="060401">6.4.1  Evaluation of a classification model</h4>
  <p>
  <div class="mainTable">
       Suppose there are two groups \(\small G_1 , G_2\), and there are \(n\) number of data whose group affiliation is known.  
       As we discussed in section 6.1.1, if a classification model is used to classify each data, 
       the actual group of data and the group classified by the model can be compared and summarized in Table 6.4.1.
    <p>
      <table style="width:600px"> 
        <tr>
          <th colspan="5">Table 6.4.1  Table for the test results of the actual group and the classified group</th>
        </tr>
        <tr> 
          <th></th>
          <th></th>
          <th colspan="2">Classified group</th>
          <th></th>
        <tr> 
          <th></th>
          <th></th>
          <th>\(\small G_1\)</th>
          <th>\(\small G_2\)</th>
          <th>Total</th>
        </tr>
        <tr>
          <td class="tdCenter" rowspan="2">Actual group</td>
          <td class="tdCenter">\(\small G_1\)</td>
          <td class="tdCenter">\(\small f_{11}\)</td>
          <td class="tdCenter">\(\small f_{12}\)</td>
          <td class="tdCenter">\(\small f_{11} + f_{12}\)</td>
        </tr>
        <tr>
          <td class="tdCenter">\(\small G_2\)</td>
          <td class="tdCenter">\(\small f_{21}\)</td>
          <td class="tdCenter">\(\small f_{22}\)</td>
          <td class="tdCenter">\(\small f_{21} + f_{22}\)</td>
        </tr>
        <tr>
          <td></td>
          <td class="tdCenter">Total</td>
          <td></td>
          <td></td>
          <td class="tdCenter">\(n\)</td>
        </tr>
      </table>
    <br>
       Here, \(f_{ij}\) means the number of data of the group \(G_i\) classified into the group \(G_j\).
       The number of data correctly classified out of the total data is \(f_{11} + f_{22}\), and the number of data 
       incorrectly classified is \(f_{12} + f_{21}\). As we defined in the section 6.1.1, 
       the <b>accuracy</b> of the classification model is the ratio of the number of correctly classified data 
       out of the total number of data, and the <b>error rate</b> is the ratio of the number of incorrectly classified data 
       out of the total number of data.
       $$ 
       \begin{align}
         \text{Accuracy} &=  \frac{f_{11} + f_{22}}{n} \\
         \text{Error rate} &=  \frac{f_{12} + f_{21}}{n}
       \end{align}
       $$
       The accuracy and error rate can be considered reasonable measures if the risk of misclassification
       of the group \(G_{1}\), \(f_{12}\), and the risk of misclassification of the group \(G_{2}\), \(f_{21}\), 
       are the same. However, in real problems, the risk of misclassification may be different for each group. 
       For example, consider two types of misclassification that a doctor misclassifies a cancer patient
       as a healthy person and a doctor misclassifies a healthy person as a cancer patient.
       The former case has a greater risk of misclassification than the latter because it carries the risk of shortening life.
       When the risk of misclassification is different, the following measures called <b>sensitivity</b>, 
       <b>specificity</b>, and <b>precision</b> are used. 
       $$ 
       \begin{align}
         \text{Sensitivity} &=  \frac{f_{11}}{f_{11} + f_{12}} \\
         \text{Specificity} &=  \frac{f_{22}}{f_{21} + f_{22}} \\
         \text{Precision}   &=  \frac{f_{11}}{f_{11} + f_{21}}
       \end{align}
       $$
       In the example above, sensitivity is the rate at which actual cancer patients are classified as cancer patients, 
       specificity is the rate at which healthy people are classified as healthy people, and precision is the rate 
       at which actual cancer patients are classified among classified as cancer patients. Accuracy can be expressed 
       as the weighted sum of sensitivity and specificity. 
       $$ 
         \text{Accuracy} = \frac{f_{11} + f_{12}}{n} \text{(Sensitivity)} + \frac{f_{21} + f_{22}}{n} \text{(Specificity)} 
       $$
    <p>
       Lift chart, confusion matrix, expected profit and ROC curve are graphs that evaluate classification models 
       using sensitivity and specificity.
  </div>
  <p>

  <!------------------------------------------------------------------------------------->
  <h5>Lift Chart</h5>
  <p>
  <div class="mainTable">
       Assume that the results of a classification model are expressed as continuous values, such as the posterior probability
       of a Bayes classification model, and that a large posterior probability means a high probability of being classified 
       as group 1. Suppose we arrange all data in descending order of posterior probability and observe the top 10% of 
       the data. In that case, the rate of classifying the actual group 1, as group 1 will be very high if the classification model is good. 
       The rate of group 1 among the entire data is called the <b>baseline response</b> (%), and the sensitivity of classifying
       actual group 1 as group 1 for the top 10% data is called the upper 10% response. The response rate of the top 10% data
       compared to the baseline response rate is called the <b>lift</b> or <b>improvement</b> of the top 10%. 
       $$\it
         \text{Top 10% lift} = \frac{\text{Response rate of top 10%}}{\text{Baseline response rate}}
       $$
       As a measure of the classification model, the lift is to compare how much the response rate of the top p% 
       of data has improved to the response rate of the entire data.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.4.1</b>
       Consider the naive Bayes classification model for the survey data in Example 6.3.3. The survey data 
       and their classification results with the posterior proability of each group are summarized in Table 6.4.2. 
       Find the lift table and draw lift chart.
    <p>
      <table style="width:800px"> 
        <tr>
          <th colspan="8">Table 6.4.2  Survey data and classification results with posterior probability of each group</th>
        </tr>
        <tr> 
          <th style="width:60px">Number</th>
          <th style="width:100px">Age</th>
          <th style="width:100px">Income<br>(unit USD)</th>
          <th style="width:100px">Credit</th>
          <th style="width:100px">Purchase</th>
          <th style="width:100px">Classification</th>
          <th style="width:100px">Posterior<br>Group 1: No</th>
          <th style="width:100px">Posterior<br>Group 2: Yes</th>
        </tr>
        <tr><td class="tdCenter">1</td>  <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter">0.138</td> </tr>
        <tr><td class="tdCenter">2</td>  <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">No </td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">0.835</td> </tr>
        <tr><td class="tdCenter">3</td>  <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">0.806</td> <td class="tdCenter">0.194</td> </tr>
        <tr><td class="tdCenter">4</td>  <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> <td class="tdCenter">No </td> <td class="tdCenter">0.806</td> <td class="tdCenter">0.194</td> </tr>
        <tr><td class="tdCenter">5</td>  <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">1.000</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">6</td>  <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.409</td> <td class="tdCenter">0.591</td> </tr>
        <tr><td class="tdCenter">7</td>  <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">0.835</td> </tr>
        <tr><td class="tdCenter">8</td>  <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter">0.138</td> </tr>
        <tr><td class="tdCenter">9</td>  <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">0.542</td> <td class="tdCenter">0.458</td> </tr>
        <tr><td class="tdCenter">10</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.409</td> <td class="tdCenter">0.591</td> </tr>
        <tr><td class="tdCenter">11</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">0.835</td> </tr>
        <tr><td class="tdCenter">12</td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter">0.138</td> </tr>
        <tr><td class="tdCenter">13</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.409</td> <td class="tdCenter">0.591</td> </tr>
        <tr><td class="tdCenter">14</td> <td class="tdCenter">30s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">Yes</td> <td class="tdCenter">No </td> <td class="tdCenter">0.509</td> <td class="tdCenter">0.491</td> </tr>
        <tr><td class="tdCenter">15</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">0.835</td> </tr>
        <tr><td class="tdCenter">16</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.409</td> <td class="tdCenter">0.591</td> </tr>
        <tr><td class="tdCenter">17</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">1.000</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">18</td> <td class="tdCenter">20s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Bad </td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">1.000</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">19</td> <td class="tdCenter">30s</td> <td class="tdCenter">GE2000</td> <td class="tdCenter">Good</td> <td class="tdCenter">Yes</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">0.835</td> </tr>
        <tr><td class="tdCenter">20</td> <td class="tdCenter">20s</td> <td class="tdCenter">LT2000</td> <td class="tdCenter">Fair</td> <td class="tdCenter">No </td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter">0.138</td> </tr>
      </table>
    <p>
       <b>Answer</b>
    <p>
       In order to make the lift table, we need first to arrange all data in descending order of the 1st group posterior probability,
       and then organize them into 10 data categories (2 data each category is 10%). Since there are 12 data in group 1 
       and 8 data in group 2 out of the total 20 data, the baseline response rate of this data is 
       \(\frac{12}{20}\) which is 60%. In each data category, the number of the 1st group data, the cumulated
       number of data, and the cumulated number of the 1st group data are counted to calculate the response rate,
       cumulated response rate, and lift of each category. The captured column is the ratio of the number of 
       the 1st group out of the number of data in each category. The % response and Lift of the 1st category 
       which is the upper 10% of data are calculated as follows.
       $$ \small
       \begin{align}
         \text{upper 10% Response} &= \frac{\text{Number of 1st group classified as group 1 in upper 10%}}{\text{Number of data in upper 10%}} \times 100 \\
         \text{upper 10% Lift}     &= \frac{\text{upper 10% Response}}{\text{Baseline response}} \times 100 \\
       \end{align}
       $$
       Table 6.4.3 is the lift table and, and the lift chart is as in &lt;Figure 6.4.1&gt;. 
    <p>
      <table style="width:800px"> 
        <tr>
          <th colspan="9">Table 6.4.3  Lift table for training data using the data in Table 6.4.2</th>
        </tr>
        <tr> 
          <th style="width:60px">Category<br>upper %</th>
          <th style="width:100px">Number of data</th>
          <th style="width:100px">Number of 1st group</th>
          <th style="width:100px">Cumulated num. of data</th>
          <th style="width:100px">Cumulated num. of 1st group</th>
          <th style="width:100px">Captured</th>
          <th style="width:100px">Response</th>
          <th style="width:100px">Cumulated response</th>
          <th style="width:100px">Lift</th>
        </tr>
        <tr><td class="tdCenter">upper (0,10%]</td>  <td class="tdCenter">2</td> <td class="tdCenter">2</td> <td class="tdCenter">2</td>  <td class="tdCenter">2</td>  <td class="tdCenter">1.0</td> <td class="tdCenter">1.00</td> <td class="tdCenter">1.00</td> <td class="tdCenter">1.67</td> </tr>
        <tr><td class="tdCenter">(10,20%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">2</td> <td class="tdCenter">4</td>  <td class="tdCenter">4</td>  <td class="tdCenter">1.0</td> <td class="tdCenter">1.00</td> <td class="tdCenter">1.00</td> <td class="tdCenter">1.67</td> </tr>
        <tr><td class="tdCenter">(20,30%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">1</td> <td class="tdCenter">6</td>  <td class="tdCenter">5</td>  <td class="tdCenter">0.5</td> <td class="tdCenter">0.50</td> <td class="tdCenter">0.83</td> <td class="tdCenter">0.83</td> </tr>
        <tr><td class="tdCenter">(30,40%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">2</td> <td class="tdCenter">8</td>  <td class="tdCenter">7</td>  <td class="tdCenter">1.0</td> <td class="tdCenter">1.00</td> <td class="tdCenter">0.88</td> <td class="tdCenter">1.67</td> </tr>
        <tr><td class="tdCenter">(40,50%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">1</td> <td class="tdCenter">10</td> <td class="tdCenter">8</td>  <td class="tdCenter">0.5</td> <td class="tdCenter">0.50</td> <td class="tdCenter">0.80</td> <td class="tdCenter">0.83</td> </tr>
        <tr><td class="tdCenter">(50,60%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">1</td> <td class="tdCenter">12</td> <td class="tdCenter">9</td>  <td class="tdCenter">0.5</td> <td class="tdCenter">0.50</td> <td class="tdCenter">0.75</td> <td class="tdCenter">0.83</td> </tr>
        <tr><td class="tdCenter">(60,70%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">2</td> <td class="tdCenter">14</td> <td class="tdCenter">11</td> <td class="tdCenter">1.0</td> <td class="tdCenter">1.00</td> <td class="tdCenter">0.79</td> <td class="tdCenter">1.67</td> </tr>
        <tr><td class="tdCenter">(70,80%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">0</td> <td class="tdCenter">16</td> <td class="tdCenter">11</td> <td class="tdCenter">0.0</td> <td class="tdCenter">0.00</td> <td class="tdCenter">0.69</td> <td class="tdCenter">0.00</td> </tr>
        <tr><td class="tdCenter">(80,90%]</td>       <td class="tdCenter">2</td> <td class="tdCenter">1</td> <td class="tdCenter">18</td> <td class="tdCenter">12</td> <td class="tdCenter">0.5</td> <td class="tdCenter">0.50</td> <td class="tdCenter">0.67</td> <td class="tdCenter">0.83</td> </tr>
        <tr><td class="tdCenter">(90,100%]</td>      <td class="tdCenter">2</td> <td class="tdCenter">0</td> <td class="tdCenter">20</td> <td class="tdCenter">12</td> <td class="tdCenter">0.0</td> <td class="tdCenter">0.00</td> <td class="tdCenter">0.60</td> <td class="tdCenter">0.00</td> </tr>
      </table>
    <p>
       &lt;Figure 6.4.1&gt; is a chart using the lift table in Table 6.4.3. The upper percentile of 
       of the data is on the x-axis, and the response rate, which is the rate of actually group 1 when the data 
       corresponding to the upper percentile category are considered as group 1, is plotted on the y-axis. 
    <p>
       <img class="figure70" src="./Figure/Fig060401.png">
       <div class="figText">&lt;Figure 6.4.1&gt; Lift chart for training data</div>
    <p>
       Since there is a small number of data in this example, we cannot see a general pattern of response rate.
       In general, response rates show a decreasing pattern by upper % categories and a response rate becomes 
       below of the baseline response at one upper % category. We can also observe a category which the lift
       becomes below 1. This upper % category can be used which value of the posterior probability will be 
       the boundary to decide a group. A lift chart is also drawn for both training and test data, which is called 
       a cross-lift chart. If it is a stable classification model, the lift charts of the training and test data
       should not be significantly different.
  </div>
  <p>
  <div class="mainTable">
       We can use a lift chart to compare two classification models as shown in &lt;Figure 6.4.2&gt;. 
       Model \(\small M_1\) classifies group 1 more accurately than model \(\small M_2\) at each percentile of data, 
       so model \(\small M_1\) can be said to be better than \(\small M_2\).
    <p>
       <img class="figure50" src="./Figure/Fig060402.png">
       <div class="figText">&lt;Figure 6.4.2&gt; An example of a lift chart for comparing two classification models</div>
  </div>
  <p>

  <!------------------------------------------------------------------------------------->
  <h5>Confusion matrix</h5>
  <p>
  <div class="mainTable">
       The lift table sorts the entire data in descending order of posterior probability, divides them into 
       several categories with similar numbers of data, and then shows the response rate, lift, of each category of data. 
       The <b>confusion matrix</b> divides the posterior probability values ​​into several cut-off values, 
       and then shows the correct classification, misclassification, accuracy, sensitivity, and specificity 
       of the entire data for each cut-off value. The confusion matrix is ​​often used to determine 
       the cut-off value of the posterior probability to determine the group. Generally, the cut-off value 
       for determining the group is mainly accuracy, but sensitivity and specificity should also be carefully examined.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.4.2</b>
       Consider the naive Bayes classification model for the survey data in Example 6.3.3. The survey data 
       and their classification results with the posterior proability of each group are summarized in Table 6.4.2. 
       Find the confusion matrix and draw a graph of sensitivity, specificity and accuracy for each category of
       posterior probability.
    <p>
       <b>Answer</b>
    <p>
       Consider a classification table created by dividing the posterior probability value into 0.1 units by 
       the cut-off value. For each cut-off value, data smaller than this value are classified into group 2, 
       and data larger than this value are classified into group 1. Table 6.4.4 is the confusion matrix
       which shows the correct classification, misclassification, accuracy, sensitivity, and specificity of 
       the entire data for each cut-off value. &lt;Figure 6.4.3&gt; is the confusion matrix graph.
    <p>
      <table style="width:800px"> 
        <tr>
          <th colspan="8">Table 6.4.4 Confusion matrix</th>
        </tr>
        <tr> 
          <th style="width:60px">Number</th>
          <th style="width:100px">Posterior probability</th>
          <th style="width:80px">Number of data</th>
          <th style="width:80px">f<sub>11</sub></th>
          <th style="width:80px">f<sub>12</sub></th>
          <th style="width:80px">f<sub>21</sub></th>
          <th style="width:80px">f<sub>22</sub></th>
          <th style="width:100px">Accuracy</th>
          <th style="width:100px">Sensitivity</th>
          <th style="width:100px">Specificity</th>
        </tr>
        <tr><td class="tdCenter">1</td>  <td class="tdCenter">0.00</td> <td class="tdCenter">20</td> <td class="tdCenter">12</td> <td class="tdCenter">0</td>  <td class="tdCenter">8</td> <td class="tdCenter">0</td> <td class="tdCenter">0.600</td> <td class="tdCenter">1.000</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">2</td>  <td class="tdCenter">0.10</td> <td class="tdCenter">20</td> <td class="tdCenter">12</td> <td class="tdCenter">0</td>  <td class="tdCenter">8</td> <td class="tdCenter">0</td> <td class="tdCenter">0.600</td> <td class="tdCenter">1.000</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">3</td>  <td class="tdCenter">0.20</td> <td class="tdCenter">20</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">4</td> <td class="tdCenter">4</td> <td class="tdCenter">0.750</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.500</td> </tr>
        <tr><td class="tdCenter">4</td>  <td class="tdCenter">0.30</td> <td class="tdCenter">20</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">4</td> <td class="tdCenter">4</td> <td class="tdCenter">0.750</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.500</td> </tr>
        <tr><td class="tdCenter">5</td>  <td class="tdCenter">0.40</td> <td class="tdCenter">20</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">4</td> <td class="tdCenter">4</td> <td class="tdCenter">0.750</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.500</td> </tr>
        <tr><td class="tdCenter">6</td>  <td class="tdCenter">0.50</td> <td class="tdCenter">20</td> <td class="tdCenter">8</td>  <td class="tdCenter">4</td>  <td class="tdCenter">3</td> <td class="tdCenter">5</td> <td class="tdCenter">0.650</td> <td class="tdCenter">0.667</td> <td class="tdCenter">0.625</td> </tr>
        <tr><td class="tdCenter">7</td>  <td class="tdCenter">0.60</td> <td class="tdCenter">20</td> <td class="tdCenter">7</td>  <td class="tdCenter">5</td>  <td class="tdCenter">2</td> <td class="tdCenter">6</td> <td class="tdCenter">0.650</td> <td class="tdCenter">0.583</td> <td class="tdCenter">0.750</td> </tr>
        <tr><td class="tdCenter">8</td>  <td class="tdCenter">0.70</td> <td class="tdCenter">20</td> <td class="tdCenter">7</td>  <td class="tdCenter">5</td>  <td class="tdCenter">2</td> <td class="tdCenter">6</td> <td class="tdCenter">0.650</td> <td class="tdCenter">0.583</td> <td class="tdCenter">0.750</td> </tr>
        <tr><td class="tdCenter">9</td>  <td class="tdCenter">0.80</td> <td class="tdCenter">20</td> <td class="tdCenter">7</td>  <td class="tdCenter">5</td>  <td class="tdCenter">2</td> <td class="tdCenter">6</td> <td class="tdCenter">0.650</td> <td class="tdCenter">0.583</td> <td class="tdCenter">0.750</td> </tr>
        <tr><td class="tdCenter">10</td> <td class="tdCenter">0.90</td> <td class="tdCenter">20</td> <td class="tdCenter">3</td>  <td class="tdCenter">9</td>  <td class="tdCenter">0</td> <td class="tdCenter">8</td> <td class="tdCenter">0.550</td> <td class="tdCenter">0.250</td> <td class="tdCenter">1.000</td> </tr>
        <tr><td class="tdCenter">11</td> <td class="tdCenter">1.00</td> <td class="tdCenter">20</td> <td class="tdCenter">0</td>  <td class="tdCenter">12</td> <td class="tdCenter">0</td> <td class="tdCenter">8</td> <td class="tdCenter">0.400</td> <td class="tdCenter">0.000</td> <td class="tdCenter">1.000</td> </tr>
      </table>
    <p>
       <img class="figure70" src="./Figure/Fig060403.png">
       <div class="figText">&lt;Figure 6.4.3&gt; Confusion matrix graph</div>
  </div>
  <p>

  <!------------------------------------------------------------------------------------->
  <h5>ROC graph</h5>
  <p>
  <div class="mainTable">
       When the result of a classification model can be expressed as a continuous value, such as the 
       posterior probability of a Bayes classification model or a logistic regression model, a <b>receiver operating 
       characteristic (ROC) graph</b> can be drawn. The ROC graph is a graph with (1 - specificity) of 
       a classification model on the x-axis and sensitivity on the y-axis. It can be said that 
       the sensitivity is the true positive rate (TPR) and (1 - specificity) is the false positive rate (FPR) 
       which is the rate of misclassifying data from group 2 into group 1. A point on the ROC graph represents 
       the result of a classification model when a critical value of the posterior probability is used.
       If we change the critical value, the classification result will be changed and a new point on the ROC graph
       is created. In other words, the ROC graph examines the changes in FPR and TPR when the critical value of 
       the posterior probability is changged.
     <p>
       &lt;Figure 6.4.4&gt; is a ROC graph of two classification models, \(\small M_{1}\) and \(\small M_{2}\). 
       In the ROC graph, the point (FPR=0, TPR=0) represents a classification model that classifies all points 
       into group 2, the point (FPR=1, TPR=1) classifies all data into group 1, and the point (FPR=0, TPR=1) 
       represents an ideal model that does not misclassify group 2 data into group 1 and correctly classifies
       all group 1 data. Therefore, a good classification model should have the classification results 
       located in the upper left corner of the ROC graph. The diagonal line in the figure shows an exceptional model
       in which both the TPR and FPR ratios are the same, that means a special case in which data are randomly 
       classified into groups 1 and 2 with a fixed probability. In this case, group 1 data is classified into group 1 
       with probability \(p\) (TPR = \(p\)), and group 2 data is also classified into group 1 with probability 
       \(p\) (FPR = \(p\)). The ROC graph is helpful in comparing the performance of several classification models. 
       In &lt;Figure 6.4.4&gt;, the ROC graph of classification model \(\small M_{1}\) is located upper 
       on the left than that of classification model \(\small M_{2}\). This means that \(\small M_{1}\) has
       the correct classification rate TPR is always better than the misclassification rate FPR for group 1, 
       so model \(\small M_{1}\) can be said to be better than model \(\small M_{2}\). In this case, 
       one classification model is always better than another classification model, but there are also cases 
       where the ROC graphs of the two models intersect so that neither model can always be said to be better.
    <p>
       <img class="figure70" src="./Figure/Fig060404.png">
       <div class="figText">&lt;Figure 6.4.4&gt; ROC graph for two classification models \(\small M_{1}\) and \(\small M_{2}\)</div>
    <br>
       The area under the ROC graph is also called the <b>c-statistic</b>, and it can be used to compare 
       how good the performance of the model is on average. In the case of an ideal model (FPR=0, TPR=1),
       the area is 1, and in the case of a random classification where the classification result of 
       the model is located on the diagonal, the area is \(\frac{1}{2}\). If the area under the ROC graph
       of one model is larger than that of another model, it can be said to be a better model on average.
    <p>
       To draw a ROC graph, first, sort in ascending order of the posterior probability of group 1 and 
       classify the entire data using each posterior probability value as the cut-off value for classifying 
       the two groups, calculate the sensitivity and specificity, and then draw a connecting line 
       using each sensitivity as the y-axis and (1 - specificity) as the x-axis. If you draw a ROC graph 
       in this way, the area under the curve, c-statistic, can be easily obtained. Let's look at the following 
       example to see how to draw a ROC graph.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.4.3</b>
       Consider the naive Bayes classification model for the survey data in Example 6.3.3. The survey data 
       and their classification results with the posterior proability of each group are summarized in Table 6.4.2. 
       The results are sorted in ascending order of the posterior probability of group 1.  
       Calculate TPR and FPR for ROC graph and draw the ROC graph of this classification model.
    <p>
       <b>Answer</b>
    <p>
       To draw a ROC graph, sort in ascending order of the posterior probability of group 1.
       First, if we classify all data into group 1, the number of group 1 data classified into group 1 is 
       \(\small f_{11}\) = 12, and the number of group 2 data classified into group 1 is \(\small f_{21}\) = 8.
       Next, if we classify the first data into group 2 and the second data and above into group 1, 
       we get \(\small f_{11}\) = 11, \(\small f_{12}\) = 1,\(\small f_{21}\) = 8. Next, if we classify 
       the first and second data into group 2 and the third data and above into group 1, we get 
        \(\small f_{11}\) = 11, \(\small f_{12}\) = 1,\(\small f_{21}\) = 7, \(\small f_{22}\) = 1.
       If we classify in a similar way and obtain TPR and FPR, they are as shown in Table 6.4.5. 
       The rightmost column in the table is the case where all data is classified into group 2. 
       If you draw an ROC graph using the TPR and FPR in this table, it will look like &lt;Figure 6.4.5&gt;.
    <p>
      <table style="width:800px"> 
        <tr>
          <th colspan="9">Table 6.4.5 Calculation of TPR and FPR for ROC graph</th>
        </tr>
        <tr> 
          <th style="width:60px">Number</th>
          <th style="width:80px">Group</th>
          <th style="width:80px">Posterior probability</th>
          <th style="width:80px">f<sub>11</sub></th>
          <th style="width:80px">f<sub>12</sub></th>
          <th style="width:80px">f<sub>21</sub></th>
          <th style="width:80px">f<sub>22</sub></th>
          <th style="width:80px">TPR</th>
          <th style="width:80px">FPR</th>
        </tr>
        <tr><td class="tdCenter">0</td>  <td class="tdCenter">   </td> <td class="tdCenter">     </td> <td class="tdCenter">12</td> <td class="tdCenter">0</td>  <td class="tdCenter">8</td> <td class="tdCenter">0</td> <td class="tdCenter">1.000</td> <td class="tdCenter">1.000</td> </tr>
        <tr><td class="tdCenter">1</td>  <td class="tdCenter">No </td> <td class="tdCenter">0.165</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">8</td> <td class="tdCenter">0</td> <td class="tdCenter">0.917</td> <td class="tdCenter">1.000</td> </tr>
        <tr><td class="tdCenter">2</td>  <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">7</td> <td class="tdCenter">1</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.875</td> </tr>
        <tr><td class="tdCenter">3</td>  <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">6</td> <td class="tdCenter">2</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.750</td> </tr>
        <tr><td class="tdCenter">4</td>  <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">5</td> <td class="tdCenter">3</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.625</td> </tr>
        <tr><td class="tdCenter">5</td>  <td class="tdCenter">Yes</td> <td class="tdCenter">0.165</td> <td class="tdCenter">11</td> <td class="tdCenter">1</td>  <td class="tdCenter">4</td> <td class="tdCenter">4</td> <td class="tdCenter">0.917</td> <td class="tdCenter">0.500</td> </tr>
        <tr><td class="tdCenter">6</td>  <td class="tdCenter">No </td> <td class="tdCenter">0.409</td> <td class="tdCenter">10</td> <td class="tdCenter">2</td>  <td class="tdCenter">4</td> <td class="tdCenter">4</td> <td class="tdCenter">0.833</td> <td class="tdCenter">0.500</td> </tr>
        <tr><td class="tdCenter">7</td>  <td class="tdCenter">Yes</td> <td class="tdCenter">0.409</td> <td class="tdCenter">10</td> <td class="tdCenter">2</td>  <td class="tdCenter">3</td> <td class="tdCenter">5</td> <td class="tdCenter">0.833</td> <td class="tdCenter">0.375</td> </tr>
        <tr><td class="tdCenter">8</td>  <td class="tdCenter">No </td> <td class="tdCenter">0.409</td> <td class="tdCenter"> 9</td> <td class="tdCenter">3</td>  <td class="tdCenter">3</td> <td class="tdCenter">5</td> <td class="tdCenter">0.750</td> <td class="tdCenter">0.375</td> </tr>
        <tr><td class="tdCenter">9</td>  <td class="tdCenter">No </td> <td class="tdCenter">0.409</td> <td class="tdCenter"> 8</td> <td class="tdCenter">4</td>  <td class="tdCenter">3</td> <td class="tdCenter">5</td> <td class="tdCenter">0.667</td> <td class="tdCenter">0.375</td> </tr>
        <tr><td class="tdCenter">10</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.509</td> <td class="tdCenter"> 8</td> <td class="tdCenter">4</td>  <td class="tdCenter">2</td> <td class="tdCenter">6</td> <td class="tdCenter">0.667</td> <td class="tdCenter">0.250</td> </tr>
        <tr><td class="tdCenter">11</td> <td class="tdCenter">No </td> <td class="tdCenter">0.542</td> <td class="tdCenter"> 7</td> <td class="tdCenter">5</td>  <td class="tdCenter">2</td> <td class="tdCenter">6</td> <td class="tdCenter">0.583</td> <td class="tdCenter">0.250</td> </tr>
        <tr><td class="tdCenter">12</td> <td class="tdCenter">No </td> <td class="tdCenter">0.806</td> <td class="tdCenter"> 6</td> <td class="tdCenter">6</td>  <td class="tdCenter">2</td> <td class="tdCenter">6</td> <td class="tdCenter">0.500</td> <td class="tdCenter">0.250</td> </tr>
        <tr><td class="tdCenter">13</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.806</td> <td class="tdCenter"> 6</td> <td class="tdCenter">6</td>  <td class="tdCenter">1</td> <td class="tdCenter">7</td> <td class="tdCenter">0.500</td> <td class="tdCenter">0.125</td> </tr>
        <tr><td class="tdCenter">14</td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter"> 5</td> <td class="tdCenter">7</td>  <td class="tdCenter">1</td> <td class="tdCenter">7</td> <td class="tdCenter">0.417</td> <td class="tdCenter">0.125</td> </tr>
        <tr><td class="tdCenter">15</td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter"> 4</td> <td class="tdCenter">8</td>  <td class="tdCenter">1</td> <td class="tdCenter">7</td> <td class="tdCenter">0.333</td> <td class="tdCenter">0.125</td> </tr>
        <tr><td class="tdCenter">16</td> <td class="tdCenter">Yes</td> <td class="tdCenter">0.862</td> <td class="tdCenter"> 4</td> <td class="tdCenter">8</td>  <td class="tdCenter">0</td> <td class="tdCenter">8</td> <td class="tdCenter">0.333</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">17</td> <td class="tdCenter">No </td> <td class="tdCenter">0.862</td> <td class="tdCenter"> 3</td> <td class="tdCenter">9</td>  <td class="tdCenter">0</td> <td class="tdCenter">8</td> <td class="tdCenter">0.250</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">18</td> <td class="tdCenter">No </td> <td class="tdCenter">1.000</td> <td class="tdCenter"> 2</td> <td class="tdCenter">10</td> <td class="tdCenter">0</td> <td class="tdCenter">8</td> <td class="tdCenter">0.167</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">19</td> <td class="tdCenter">No </td> <td class="tdCenter">1.000</td> <td class="tdCenter"> 1</td> <td class="tdCenter">11</td> <td class="tdCenter">0</td> <td class="tdCenter">8</td> <td class="tdCenter">0.083</td> <td class="tdCenter">0.000</td> </tr>
        <tr><td class="tdCenter">20</td> <td class="tdCenter">No </td> <td class="tdCenter">1.000</td> <td class="tdCenter"> 0</td> <td class="tdCenter">12</td> <td class="tdCenter">0</td> <td class="tdCenter">3</td> <td class="tdCenter">0.000</td> <td class="tdCenter">0.000</td> </tr>
      </table>
    <p>
       <img class="figure70" src="./Figure/Fig060405.png">
       <div class="figText">&lt;Figure 6.4.5&gt; ROC graph</div>
  </div>
  <p>

  <!------------------------------------------------------------------------------------->
  <h4 id="060402">6.4.2  Comparison of classification models</h4>
  <p>
  <div class="mainTable">
       In order to classify data, rather than applying one classification model, several models are tried and the most appropriate model
       for the given data is selected. The comparison of models is made using a comparison of accuracy, but the difference 
       in accuracy between the two models may not be statistically significant. In this section, we will look at the 
       confidence interval estimation for the accuracy of a classification model and the statistical method for 
       comparing the accuracy of two models.
    <p>
       In addition to accuracy, the comparison of two classification models should also consider the speed of processing 
       the algorithm on a computer, robustness for evaluating the impact of noisy data or missing values, scalability 
       for efficiently building the model even when a large amount of data is given, and interpretability of the model results.
  </div>
  <p>

  <h5>Confidence interval for accuracy</h5>
  <p>
  <div class="mainTable">
       Let \(p\) be the actual accuracy of the model and \(n\) be the number of test data set. If we let the random variable
       \(\small X\) be the number of data correctly classified by the classification model, \(\small X\) is a binomial random 
       variable with parameters \(n\) and \(p\). In this case, the accuracy, \(\hat p = \frac{X}{n}\), of the classification experiment
       has a mean of \(p\) and a variance of \(\frac{p(1-p)}{n}\). If \(n\) is sufficiently large, 
       the binomial distribution can be approximated by the normal distribution, so the 100(1-\(\alpha\))% confidence interval
       for the actual accuracy \(p\) can be obtainable using the following probability statement. 
       $$
         P ( - Z_{\frac{\alpha}{2}} < \frac{\hat p - p}{\sqrt{p(1-p)}} < Z_{\frac{\alpha}{2}} ) = 1 - \alpha
       $$
       Here, \( Z_{\frac{\alpha}{2}}\) means the right \(\frac{\alpha}{2}\) * 100% quantile of the standard normal distribution, 
       and when the confidence level is 95%, \( Z_{\frac{\alpha}{2}}\) = 1.96. If we rearrange this equation, 
       the confidence interval for the accuracy \(p\) can be shown as follows. 
       $$
         \frac { ( 2 n \times \hat p + Z_{\frac{\alpha}{2}}^2 ) \; ± \; Z_{\frac{\alpha}{2}} \sqrt{ Z_{\frac{\alpha}{2}}^2 + 4 n \times \hat p - 4n \times {\hat p}^2 }} {2 n + 2 Z_{\frac{\alpha}{2}}^2}
       $$
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.4.4</b>
       When a classification model was applied to 100 test data, it had an accuracy of \(\hat p\) = 80%. 
       Estimate the confidence interval of actual accuracy with a 95% confidence level.
    <p>
       <b>Answer</b>
    <p>
       When the confidence level is 95%, \(\alpha\) = 0.05, so Z_{\frac{0.05}{2}} = 1.96. 
       The accuracy of the experiment \(\hat p\) = 0.8, and the number of data \(n\) = 100, 
       so by substituting the confidence interval formula, we get the following. 
       $$
         \frac { ( 2 \times 100 \times 0.8 + {1.96}^2 ) \; ± \; 1.96 \sqrt{ {1.96}^2 + 4 \times 100 \times 0.8 - 4 \times 100 \times {0.8}^2 }  }{2 \times 100  + 2 \times {1.96}^2}
       $$
       That is, the 95% confidence interval of the actual accuracy is (71.1%, 86.7%).
  </div>
  <p>

  <h5>Comparison of accuracy of two models</h5>
  <p>
  <div class="mainTable">
       Suppose that two classification models \( M_{1}\) and \( M_{1}\) are applied to two independent test data sets
       of which their numbers of data are \(n_{1}\) and \(n_{2}\), and the accuracies \(\hat p_{1}\) and \(\hat p_{2}\) are measured
       respectively. Let us find out how to test whether the accuracies \(\hat p_{1}\) and \(\hat p_{2}\) are statistically significant. 
       If \(n_{1}\) and \(n_{2}\) are sufficiently large, the accuracy \(\hat p_{1}\) and \(\hat p_{2}\) will approximately follow 
       normal distributions, and the difference in accuracy \(\hat p_{1} - \hat p_{2}\) will also approximately follow 
       a normal distribution with mean \( p_{1} - p_{2}\) and variance as follows.
       $$
         \frac{\hat p_{1} (1-\hat p_{1})}{n_1} + \frac{\hat p_{2} (1-\hat p_{2})}{n_2}
       $$
       Therefore, the (\(1 - \alpha\))% confidence interval of true accuracy difference, \(p_{1} - p_{2}\), is as follows.
       $$
         (\hat p_{1} - \hat p_{2}) \; ± \; Z_{\frac{\alpha}{2}} \; \sqrt{\frac{\hat p_{1} (1-\hat p_{1})}{n_1} + \frac{\hat p_{2} (1-\hat p_{2})}{n_2}}
       $$
       If this confidence interval includes 0, we can conclude that the accuracies of the two models are statistically the same.
  </div>
  <p>
  <div class="mainTableGrey">
       <b>Example 6.4.5</b>
       If a classification model \(\small M_{1}\) shows 85% accuracy on 50 test data and classification model \(\small M_{2}\)
       shows 75% accuracy on 500 test data, can we conclude that classification model \(\small M_{1}\) is a better model than \(\small M_{2}\)?
    <p>
       <b>Answer</b>
    <p>
       The number of data applied to model \(\small M_{1}\) is \(\small n_{1}\) = 50 and the accuracy is \(\small \hat p_{1}\) = 0.85, 
       the number of data applied to model \(\small M_{2}\) is \(\small n_{2}\) = 500 and the accuracy is \(\small \hat p_{2}\) = 0.75, 
       Therefore, the 95% confidence interval is as follows.
    <p>
       \( \qquad
         (0.85 - 0.75) \; ± \; 1.96 \sqrt{ \frac{ 0.85 (1-0.85)}{50} + \frac{ 0.75 (1-0.75)}{500}}
       \)
    <p>
       \( \qquad
         0.10 ± 1.96×0.0541
       \)
    <p>
       Therefore, the 95% confidence interval is (-0.2060, 0.0060). This confidence interval includes 0, 
       so the difference in observed accuracy is not statistically significant. In other words, the true accuracies of model 
       \(\small M_{1}\) and model \(\small M_{2}\) are not statistically significant, so we cannot say 
       which classification model is better.
  </div>
  <p>

  <h5>Generalized error considering model overfitting</h5>
  <p>
  <div class="mainTable">
       A good classification model is a model that really classifies well when applied to actual data. However, a model 
       that shows satisfactory accuracy in test data may have poor classification accuracy when applied to actual data. 
       This case is called a <b>model overfitting</b>, and it often occurs in decision trees or neural network models.
       There are various causes of overfitting, but it can occur when there is noisy data in the data or when the number of nodes 
       in the decision tree model is set too high. It can also occur when the training data, which is a part of the entire data set, 
       does not sufficiently represent the entire set. In order to prevent overfitting, the <b>generalized error</b>, 
       which is the sum of the experimental error of the model and the penalty term for the complexity of the model, 
       is used to compare and select a model among several models. For example, suppose that a decision tree \(T\) with \(k\) leaves 
       has an error rate at node \(t_{i}\), \(e(t_{i})\), when classifying \(n(t_{i})\) data. The generalized error
       \(e_{g}(T)\) of this decision tree \(T\) can be defined as follows. 
       $$
         \begin{align}
         e_{g}(T) &= \frac{\sum_{i=1}^{k} [ e(t_{i}) + \Omega(t_{i}) ] }{\sum_{i=1}^{k} n(t_{i})} \\
                  &= \frac{e(T) + \Omega(T)}{\sum_{i=1}^{k} n(t_{i})}
         \end{align}
       $$
       Here, \(\Omega(t_{i})\) is the penalty term in each node \(t_i\), \(e(T)\) is the overall error rate, 
       and \(\Omega(T)\) is the overall penalty term. If the penalty term of each node is \(\Omega(t_{i})\) = 0.2, 
       \(\Omega(T)\) becomes (the number of nodes) × 0.2, so the generalized error increases as the number of nodes increases. 
       Therefore, if there are two decision tree models with similar error rates, the generalization error of the model 
       with the smaller number of nodes is smaller.
  </div>
  <p>

  <!--------------------------------------------->
  <h3 id="0605">6.5 Exercise</h3>
  <p>
  <div class="mainTableExercise">
      <p class="textL30M30">6.1  Ten data of two groups (+ group or - group) for two binary variables (A and B) are as follows.
      <table style="width:300px"> 
         <tr> 
           <th>A</th>
           <th>B</th>
           <th>Group</th>
         </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">F</td> <td class="tdCenter">+</td> </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">T</td> <td class="tdCenter">+</td> </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">T</td> <td class="tdCenter">+</td> </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">F</td> <td class="tdCenter">-</td> </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">T</td> <td class="tdCenter">+</td> </tr>
         <tr> <td class="tdCenter">F</td> <td class="tdCenter">F</td> <td class="tdCenter">-</td> </tr>
         <tr> <td class="tdCenter">F</td> <td class="tdCenter">F</td> <td class="tdCenter">+</td> </tr>
         <tr> <td class="tdCenter">F</td> <td class="tdCenter">F</td> <td class="tdCenter">-</td> </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">T</td> <td class="tdCenter">+</td> </tr>
         <tr> <td class="tdCenter">T</td> <td class="tdCenter">F</td> <td class="tdCenter">-</td> </tr>
      </table>
      <div class="textL30M20">
        1) Calculate the information gain using the Gini coefficient for all ten data. 
           Calculate the information gain using the Gini coefficient for variable A and B.
           Which variable's branching is better using these results in the decision tree?
      </div>
      <div class="textL30M20">
        2) Calculate the information gain using the entropy coefficient for all ten data. 
           Calculate the information gain using the entropy coefficient for variable A and B.
           Which variable's branching is better using these results in the decision tree?
           Compare the result with 1)
      </div>
      <div class="textL30M20">
        3) Find a decision tree using the entropy coefficient to classify the group.
           When the values ​​of variables A and B of a test data are T and F, respectively, classify the data using the decision tree.
      </div>
      <div class="textL30M20">
        4) Let the prior probability of the + group be 0.6 and the prior probability of the - group be 0.4.
           Find a naive Bayes classification rule to classify the group.
           When the values ​​of variables A and B of a test data are T and F, respectively, classify the data using 
           the naive Bayes classification.
      </div>
      <div class="textL30M20">
        5) Find the lift chart, confusion matrix, and ROC curve in 4)
      </div>

      <p class="textL30M30">6.2  When we surveyed 20 people who visited a particular department store, 10 people made purchases, 
         and 10 people did not make purchases. The survey results of these 20 people's gender (M: male, F: female,
         car ownership (S: small, M: medium, L: large), house ownership (Y: yes, N: no), and purchases (Y: yes, N: no) were as follows.
        <p>
        <table style="width:300px"> 
          <tr> <th>Gender</th> <th>Car</th> <th>House</th> <th>Purchase</th> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">M</td> <td class="tdCenter">Y</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">L</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">L</td> <td class="tdCenter">Y</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">L</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">M</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">L</td> <td class="tdCenter">Y</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">L</td> <td class="tdCenter">N</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">M</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">M</td> <td class="tdCenter">Y</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">S</td> <td class="tdCenter">Y</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">S</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">S</td> <td class="tdCenter">N</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">M</td> <td class="tdCenter">N</td> <td class="tdCenter">Y</td> </tr>
          <tr> <td class="tdCenter">M</td> <td class="tdCenter">S</td> <td class="tdCenter">N</td> <td class="tdCenter">N</td> </tr>
          <tr> <td class="tdCenter">F</td> <td class="tdCenter">L</td> <td class="tdCenter">Y</td> <td class="tdCenter">Y</td> </tr>
        </table>
      <div class="textL30M20">
        1) Find a decision tree using the entropy coefficient to classify the group.
           When a customer is female, a large car owner, and has a house, classify the customer using the decision tree.
      </div>
      <div class="textL30M20">
        2) Let the prior probability of the purchasing group be 0.6 and the non-purchasing group be 0.4.
           Find a naive Bayes classification rule to classify the group.
           When a customer is female, a large car owner, and has a house, classify the customer using 
           the naive Bayes classification.
      </div>
      <div class="textL30M20">
        3) Find the lift chart, confusion matrix, and ROC curve in 2)
      </div>


</div>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="https://maxcdn.bootstrapcdn.com/js/ie10-viewport-bug-workaround.js"></script>
    <script>
        //document.getElementById('sidebar').getElementsByTagName('ul')[0].className += "nav nav-sidebar";
        
        /* ajust the height when click the toc
           the code is from https://github.com/twbs/bootstrap/issues/1768
        */
        var shiftWindow = function() { scrollBy(0, -50) };
        window.addEventListener("hashchange", shiftWindow);
        function load() { if (window.location.hash) shiftWindow(); }
        
        /*add Bootstrap styles to tables*/
        var tables = document.getElementsByTagName("table");
        for(var i = 0; i < tables.length; ++i){
            tables[i].className += "table table-bordered table-hover";
        }
    </script>

</body>
</html>

